{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import heapq, numpy as np\n",
    "import random\n",
    "#!pip3 install gensim\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "dir_path = \"awards_2002/\"\n",
    "root_dir = os.fsencode(dir_path)\n",
    "for directory in os.listdir(root_dir):\n",
    "    sub_directory = os.fsdecode(directory)\n",
    "    current_path = dir_path + sub_directory + \"/\"\n",
    "    \n",
    "    for file in os.listdir(dir_path + sub_directory):\n",
    "        with open(current_path + file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            documents.append(f.read())\n",
    "            \n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_names(vectorizer, matrix):\n",
    "    features = tfidf_vectorizer.get_feature_names()\n",
    "    for doc_i in range(5):\n",
    "        print(\"\\nDocument %d, top terms by TF-IDF\" % doc_i)\n",
    "        for term, score in sorted(list(zip(features,matrix.toarray()[doc_i])), key=lambda x:-x[1])[:5]:\n",
    "            print(\"%.2f\\t%s\" % (score, term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clusters(matrix, clusters, n_keywords=10):\n",
    "    max_cluster = 10\n",
    "    for cluster in range(min(clusters), max_cluster):\n",
    "        cluster_docs = [i for i, c in enumerate(clusters) if c == cluster]\n",
    "        print(\"Cluster: %d (%d docs)\" % (cluster, len(cluster_docs)))\n",
    "        \n",
    "        # Keep scores for top n terms\n",
    "        new_matrix = np.zeros((len(cluster_docs), matrix.shape[1]))\n",
    "        for cluster_i, doc_vec in enumerate(matrix[cluster_docs].toarray()):\n",
    "            for idx, score in heapq.nlargest(n_keywords, enumerate(doc_vec), key=lambda x:x[1]):\n",
    "                new_matrix[cluster_i][idx] = score\n",
    "\n",
    "        # Aggregate scores for kept top terms\n",
    "        keywords = heapq.nlargest(n_keywords, zip(new_matrix.sum(axis=0), features))\n",
    "        print(', '.join([w for s,w in keywords]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a Experiment with KMeans and hierarchial clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 0 (44 docs)\n",
      "jack, babcock, comparative, divisors, frp, clare, bimetallic, granularities, dipolar, graduating\n",
      "\n",
      "Cluster: 1 (65 docs)\n",
      "flatau, applies, paving, barbados, alkynyl, mails, hts, plurality, adel, kinds\n",
      "\n",
      "Cluster: 2 (35 docs)\n",
      "homozygous, mts, generalizable, dinosaurs, hyperbolicity, hoof, hood, additive, buphy, honing\n",
      "\n",
      "Cluster: 3 (22 docs)\n",
      "883, bolometer, lynn, lectures, natl, beatrice, france, biocontrol, nobel, 797\n",
      "\n",
      "Cluster: 4 (55 docs)\n",
      "diffusion, accepts, jimmy, darrell, lamps, guard, marguerite, diffusionless, coprocessor, autonoma\n",
      "\n",
      "Cluster: 5 (36 docs)\n",
      "689, divisors, hadjicostis, benjamin, cropping, interbasin, divorce, coakley, euler, incubators\n",
      "\n",
      "Cluster: 6 (17 docs)\n",
      "chou, icdp, darrell, 42600, darren, middleton, 1041, 200590001, interdisciplinary, 63132\n",
      "\n",
      "Cluster: 7 (31 docs)\n",
      "gibbs, daytona, fm, indians, karen_fischer, 6276, logger, constitution, fllwshp, bivalves\n",
      "\n",
      "Cluster: 8 (22 docs)\n",
      "generosity, plantp, exploratory, mil, appalachians, 987, gaim, carlo, contradict, paleogmagnetic\n",
      "\n",
      "Cluster: 9 (48 docs)\n",
      "generalizable, birth, blackout, granitic, insertional, fluidized, lilly, epitaxially, 900324221, granitoid\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=3, use_idf=True, sublinear_tf=True, max_df=0.1, max_features=100000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "matrix_sample = tfidf_matrix[:1000]\n",
    "km = KMeans(n_clusters=30, random_state=42, verbose=0)\n",
    "km.fit(matrix_sample)\n",
    "print_clusters(matrix_sample, km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 1 (4 docs)\n",
      "blackout, portfolios, blackouts, cascading, corrective, criticality, finance, budgeting, preventive, disruptions\n",
      "\n",
      "Cluster: 2 (18 docs)\n",
      "hybrid, embedded, vlsi, compiler, stanford, scheduling, qca, secure, quantum, computation\n",
      "\n",
      "Cluster: 3 (6 docs)\n",
      "mantle, galapagos, antarctic, hotspot, floreana, seismic, broadband, tethyan, plume, indian\n",
      "\n",
      "Cluster: 4 (4 docs)\n",
      "paleointensity, deposits, pilcomayo, peopling, forams, foram, alkenone, cores, mesozoic, rio\n",
      "\n",
      "Cluster: 5 (5 docs)\n",
      "rivers, hyporheic, braided, hydrologic, anabranching, burges, bedload, flume, jointed, bedrock\n",
      "\n",
      "Cluster: 6 (12 docs)\n",
      "rifting, detachment, cordillera, uplift, gps, faulting, basin, continental, slip, paleomagnetic\n",
      "\n",
      "Cluster: 7 (8 docs)\n",
      "dm, estimators, random, saddlepoint, mutual, wehrly, tcs, wavelet, descriptive, advisors\n",
      "\n",
      "Cluster: 8 (14 docs)\n",
      "algebraic, commutative, homotopy, algebra, geometry, eisenbud, geometric, varieties, cohomology, manifolds\n",
      "\n",
      "Cluster: 9 (18 docs)\n",
      "probability, ergodic, harmonic, singularities, hamiltonian, protter, stochastic, oscillatory, almost, waves\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=2, use_idf=True,max_df=0.1, max_features=100000, sublinear_tf=True)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "matrix_sample = tfidf_matrix[:1000]\n",
    "z = linkage(matrix_sample.todense(), metric=\"cosine\", method=\"complete\")\n",
    "clusters = fcluster(z, t=0.99, criterion=\"distance\")\n",
    "print_clusters(matrix_sample, clusters)\n",
    "# seems a lot better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a results\n",
    "\n",
    "#### Fcluster\n",
    "\n",
    "* With a min_df of 1 a lot of numbers started popping up and multiple clusters with the same terms\n",
    "* Higher min_df doesn't do much more than potentially hide \"high value\" terms\n",
    "* Mostly good terms with a decent setup\n",
    "* Small cluster size (# of docs) - related to the t in fcluster\n",
    "* Method to euclidian instead of complete didn't give much benefit\n",
    "\n",
    "#### KMeans\n",
    "\n",
    "* Large clusters\n",
    "* More numbers in the clusters (Potentially useless, potentially good ie. genes)\n",
    "* Seems dependant on the random_state\n",
    "* Higher than 2 min_df just leads to clusters that are too broad\n",
    "\n",
    "--\n",
    "\n",
    "In my experimentation I feel like the end-result that was best was the most recent hierarchial clustering. For one, none of the clusters had numbers which I atleast saw as a larger negative.\n",
    "\n",
    "That said it has it's pros and cons as well. The clusters are considerably smaller in size compared to the KMeans clusters, where these are about 10 or so docs in size, the KMeans clusters seem to be around 25 or so. This is both good and bad in the sense that a smaller cluster most likely means that it's more specific, but it might also mean that it just made multiple clusters that are very similar.\n",
    "\n",
    "As such I'll go with the fcluster that I have above. It uses\n",
    "\n",
    "\n",
    "linkage(metric=\"cosine\", method=\"complete\")\n",
    "\n",
    "fcluster(t=0.99, criterion=\"distance\")\n",
    "\n",
    "Changing the method only gave very similar or sparse clusters. The t value just made the clusters even smaller, to the point where a doc was basically its own cluster. The min_df and max_df seemed to be pretty optimal at these values, as changing them too much just made clusters too broad or made them have too many \"bad\" terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b label the clusters\n",
    "\n",
    "Copypaste the cluster just in case since i shuffle the docs at the start of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cluster: 1 (7 docs) - **Electrical engineering**\n",
    "\n",
    "multimedia, compiler, smt, hmd, asic, processors, ieee, multiuser, adaptable, fpga\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 2 (8 docs) - **Software verification**\n",
    "\n",
    "hybrid, verification, embedded, software, qos, certification, stanford, rtl, checking, device\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 3 (7 docs) - **Continental drifting / Seafloor geography**\n",
    "\n",
    "continental, rift, rifting, spreading, seafloor, extension, pilcomayo, gulf, deposits, rio\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 4 (9 docs) - **Geography statistics**\n",
    "\n",
    "mantle, antarctic, seismic, gps, geodetic, stations, fault, puget, permanent, recoverable\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 5 (10 docs) - **Seismic activity?**\n",
    "\n",
    "detachment, uplift, floreana, magmatic, tectonic, cordillera, arc, strike, mafic, plateau\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 6 (15 docs) - **Thermodynamics**\n",
    "\n",
    "equations, ergodic, differential, probability, volterra, singularities, hyperbolic, oscillations, boundary, partial\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 7 (4 docs) - **Linear algebra**\n",
    "\n",
    "spaces, operators, teichmueller, functions, operator, metric, hankel, toeplitz, green, holomorphic\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 8 (13 docs) - **Algebraic topology**\n",
    "\n",
    "manifolds, homotopy, dm, geometric, compact, algebras, surfaces, variables, ring, operators\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 9 (4 docs) - **Deforestation & poor countries**\n",
    "\n",
    "migrants, semantic, tenure, real, compositionality, semantics, migration, syntactic, web, deforestation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c pick out 2 good and 2 bad clusters\n",
    "\n",
    "Clusters 7 & 8 are both good in my opinion.\n",
    "\n",
    "7 is a little small in size, however the terms are almost all related and for example the 3 names all correspond to functions related to algebra, and obviously functions are also in the picture.\n",
    "\n",
    "8 is also grouped in a similar way, where the terms can all be related back to topology, where for example homotpoty and manifolds are both main branches of topology.\n",
    "\n",
    "As for bad clusters, from these 10 I'd say it would be cluster 9 and cluster 3. (5 by extension)\n",
    "\n",
    "Cluster 9 is simply too hard to interpret. It has a mix of very different terms that are hard to group together. It could be correlated to the Amazon rainforest and the deforestation there but where do semantics come into the picture there.\n",
    "\n",
    "Cluster 3 in turn isn't that bad, however I feel like its too similar to that of cluster 5. THey're both related to seismic activity, and it's essentially just one being the seafloor, the other being mountains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d LDA modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf2_vectorizer = TfidfVectorizer()\n",
    "word_tokenizer = tfidf2_vectorizer.build_tokenizer()\n",
    "tokenized_text = [word_tokenizer(doc) for doc in documents]\n",
    "\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "lda_corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "lda_model = models.LdaModel(lda_corpus, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "0.0085\tresearch\n",
      "0.0063\tAward\n",
      "0.0057\tProgram\n",
      "0.0056\tUS\n",
      "0.0051\tInvestigator\n",
      "0.0048\tEstimated\n",
      "0.0045\tNSF\n",
      "0.0044\tResearch\n",
      "0.0043\tINT\n",
      "0.0041\tThis\n",
      "\n",
      "Topic 1\n",
      "0.0092\tAward\n",
      "0.0073\tInvestigator\n",
      "0.0063\tProgram\n",
      "0.0060\tNSF\n",
      "0.0058\tEstimated\n",
      "0.0057\tDate\n",
      "0.0048\tcurrent\n",
      "0.0046\t2002\n",
      "0.0045\tPrincipal\n",
      "0.0042\tresearch\n",
      "\n",
      "Topic 2\n",
      "0.0067\tAward\n",
      "0.0056\tAST\n",
      "0.0055\tDate\n",
      "0.0049\tProgram\n",
      "0.0049\tNSF\n",
      "0.0047\tEstimated\n",
      "0.0046\tInvestigator\n",
      "0.0036\tThis\n",
      "0.0035\t2002\n",
      "0.0033\tcurrent\n",
      "\n",
      "Topic 3\n",
      "0.0078\tAward\n",
      "0.0063\tInvestigator\n",
      "0.0052\tNSF\n",
      "0.0052\tEstimated\n",
      "0.0051\tcurrent\n",
      "0.0050\tProgram\n",
      "0.0049\tDate\n",
      "0.0043\tPrincipal\n",
      "0.0037\t2002\n",
      "0.0031\tThis\n",
      "\n",
      "Topic 4\n",
      "0.0089\tInvestigator\n",
      "0.0082\tAward\n",
      "0.0064\tProgram\n",
      "0.0064\tcurrent\n",
      "0.0062\tNSF\n",
      "0.0060\tstudents\n",
      "0.0060\tPrincipal\n",
      "0.0056\tDate\n",
      "0.0055\tEstimated\n",
      "0.0052\tresearch\n",
      "\n",
      "Topic 5\n",
      "0.0108\tInvestigator\n",
      "0.0101\tAward\n",
      "0.0092\tresearch\n",
      "0.0084\tNSF\n",
      "0.0083\tUniversity\n",
      "0.0076\tPrincipal\n",
      "0.0073\tcurrent\n",
      "0.0073\tProgram\n",
      "0.0071\t2002\n",
      "0.0063\tEstimated\n",
      "\n",
      "Topic 6\n",
      "0.0076\tInvestigator\n",
      "0.0058\tcurrent\n",
      "0.0058\tNSF\n",
      "0.0056\tAward\n",
      "0.0052\tEstimated\n",
      "0.0044\tresearch\n",
      "0.0042\tPrincipal\n",
      "0.0041\tDate\n",
      "0.0040\tProgram\n",
      "0.0039\tdata\n",
      "\n",
      "Topic 7\n",
      "0.0199\tAward\n",
      "0.0155\tInvestigator\n",
      "0.0141\tProgram\n",
      "0.0138\tNSF\n",
      "0.0134\tDate\n",
      "0.0133\tEstimated\n",
      "0.0128\t2002\n",
      "0.0092\tcurrent\n",
      "0.0090\tPrincipal\n",
      "0.0073\tOF\n",
      "\n",
      "Topic 8\n",
      "0.0066\tAward\n",
      "0.0058\tInvestigator\n",
      "0.0049\tNSF\n",
      "0.0048\tDate\n",
      "0.0047\tProgram\n",
      "0.0045\tEstimated\n",
      "0.0041\tresearch\n",
      "0.0034\tcurrent\n",
      "0.0034\tPrincipal\n",
      "0.0033\tThis\n",
      "\n",
      "Topic 9\n",
      "0.0096\tPhase\n",
      "0.0087\tAward\n",
      "0.0073\tDMI\n",
      "0.0053\tDate\n",
      "0.0052\tSBIR\n",
      "0.0052\tProgram\n",
      "0.0051\tInvestigator\n",
      "0.0049\tproject\n",
      "0.0048\tThis\n",
      "0.0046\tNSF\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect topics\n",
    "for i, topic in lda_model.show_topics(num_words=50, formatted=False):\n",
    "    print(\"Topic\", i)\n",
    "    printed_terms = 0\n",
    "    for term, score in topic:\n",
    "        if printed_terms >= 10:\n",
    "            break\n",
    "        elif term in \"this will that the of and to for in or The is be may an a with at are on by as from can\".split():\n",
    "            continue\n",
    "        printed_terms += 1\n",
    "        print(\"%.4f\\t%s\" % (score,term))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case a lot of the topics seem to be very similar if not almost identical, however because of how LDA is intended to work this does make some sense."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

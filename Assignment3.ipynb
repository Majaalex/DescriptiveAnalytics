{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import heapq, numpy as np\n",
    "import random\n",
    "#!pip3 install gensim\n",
    "from gensim import corpora, models\n",
    "import logging\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "dir_path = \"awards_2002/\"\n",
    "root_dir = os.fsencode(dir_path)\n",
    "for directory in os.listdir(root_dir):\n",
    "    sub_directory = os.fsdecode(directory)\n",
    "    current_path = dir_path + sub_directory + \"/\"\n",
    "    \n",
    "    for file in os.listdir(dir_path + sub_directory):\n",
    "        with open(current_path + file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            documents.append(f.read())\n",
    "            \n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_names(vectorizer, matrix):\n",
    "    features = tfidf_vectorizer.get_feature_names()\n",
    "    for doc_i in range(5):\n",
    "        print(\"\\nDocument %d, top terms by TF-IDF\" % doc_i)\n",
    "        for term, score in sorted(list(zip(features,matrix.toarray()[doc_i])), key=lambda x:-x[1])[:5]:\n",
    "            print(\"%.2f\\t%s\" % (score, term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clusters(matrix, clusters, n_keywords=10):\n",
    "    max_cluster = 10\n",
    "    for cluster in range(min(clusters), max_cluster):\n",
    "        cluster_docs = [i for i, c in enumerate(clusters) if c == cluster]\n",
    "        print(\"Cluster: %d (%d docs)\" % (cluster, len(cluster_docs)))\n",
    "        \n",
    "        # Keep scores for top n terms\n",
    "        new_matrix = np.zeros((len(cluster_docs), matrix.shape[1]))\n",
    "        for cluster_i, doc_vec in enumerate(matrix[cluster_docs].toarray()):\n",
    "            for idx, score in heapq.nlargest(n_keywords, enumerate(doc_vec), key=lambda x:x[1]):\n",
    "                new_matrix[cluster_i][idx] = score\n",
    "\n",
    "        # Aggregate scores for kept top terms\n",
    "        keywords = heapq.nlargest(n_keywords, zip(new_matrix.sum(axis=0), features))\n",
    "        print(', '.join([w for s,w in keywords]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a Experiment with KMeans and hierarchial clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=30, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=3, use_idf=True, sublinear_tf=True, max_df=0.1, max_features=100000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "matrix_sample = tfidf_matrix[:1000]\n",
    "km = KMeans(n_clusters=30, random_state=42, verbose=0)\n",
    "km.fit(matrix_sample)\n",
    "print_clusters(matrix_sample, km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=2, use_idf=True,max_df=0.1, max_features=100000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "matrix_sample = tfidf_matrix[:1000]\n",
    "z = linkage(matrix_sample.todense(), metric=\"cosine\", method=\"complete\")\n",
    "clusters = fcluster(z, t=0.99, criterion=\"distance\")\n",
    "print_clusters(matrix_sample, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a results\n",
    "\n",
    "#### Fcluster\n",
    "\n",
    "* With a min_df of 1 a lot of numbers started popping up and multiple clusters with the same terms\n",
    "* Higher min_df doesn't do much more than potentially hide \"high value\" terms\n",
    "* Mostly good terms with a decent setup\n",
    "* Small cluster size (# of docs) - related to the t in fcluster\n",
    "* Method to euclidian instead of complete didn't give much benefit\n",
    "\n",
    "#### KMeans\n",
    "\n",
    "* Large clusters\n",
    "* More numbers in the clusters (Potentially useless, potentially good ie. genes)\n",
    "* Seems dependant on the random_state\n",
    "* Higher than 2 min_df just leads to clusters that are too broad\n",
    "\n",
    "--\n",
    "\n",
    "In my experimentation I feel like the end-result that was best was the most recent hierarchial clustering. For one, none of the clusters had numbers which I atleast saw as a larger negative.\n",
    "\n",
    "That said it has it's pros and cons as well. The clusters are considerably smaller in size compared to the KMeans clusters, where these are about 10 or so docs in size, the KMeans clusters seem to be around 25 or so. This is both good and bad in the sense that a smaller cluster most likely means that it's more specific, but it might also mean that it just made multiple clusters that are very similar.\n",
    "\n",
    "As such I'll go with the fcluster that I have above. It uses\n",
    "\n",
    "\n",
    "linkage(metric=\"cosine\", method=\"complete\")\n",
    "\n",
    "fcluster(t=0.99, criterion=\"distance\")\n",
    "\n",
    "Changing the method only gave very similar or sparse clusters. The t value just made the clusters even smaller, to the point where a doc was basically its own cluster. The min_df and max_df seemed to be pretty optimal at these values, as changing them too much just made clusters too broad or made them have too many \"bad\" terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b label the clusters\n",
    "\n",
    "Copypaste the cluster just in case since i shuffle the docs at the start of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cluster: 1 (7 docs) - **Electrical engineering**\n",
    "\n",
    "multimedia, compiler, smt, hmd, asic, processors, ieee, multiuser, adaptable, fpga\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 2 (8 docs) - **Software verification**\n",
    "\n",
    "hybrid, verification, embedded, software, qos, certification, stanford, rtl, checking, device\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 3 (7 docs) - **Continental drifting / Seafloor geography**\n",
    "\n",
    "continental, rift, rifting, spreading, seafloor, extension, pilcomayo, gulf, deposits, rio\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 4 (9 docs) - **Geography statistics**\n",
    "\n",
    "mantle, antarctic, seismic, gps, geodetic, stations, fault, puget, permanent, recoverable\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 5 (10 docs) - **Seismic activity?**\n",
    "\n",
    "detachment, uplift, floreana, magmatic, tectonic, cordillera, arc, strike, mafic, plateau\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 6 (15 docs) - **Thermodynamics**\n",
    "\n",
    "equations, ergodic, differential, probability, volterra, singularities, hyperbolic, oscillations, boundary, partial\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 7 (4 docs) - **Linear algebra**\n",
    "\n",
    "spaces, operators, teichmueller, functions, operator, metric, hankel, toeplitz, green, holomorphic\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 8 (13 docs) - **Algebraic topology**\n",
    "\n",
    "manifolds, homotopy, dm, geometric, compact, algebras, surfaces, variables, ring, operators\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 9 (4 docs) - **Deforestation & poor countries**\n",
    "\n",
    "migrants, semantic, tenure, real, compositionality, semantics, migration, syntactic, web, deforestation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c pick out 2 good and 2 bad clusters\n",
    "\n",
    "Clusters 7 & 8 are both good in my opinion.\n",
    "\n",
    "7 is a little small in size, however the terms are almost all related and for example the 3 names all correspond to functions related to algebra, and obviously functions are also in the picture.\n",
    "\n",
    "8 is also grouped in a similar way, where the terms can all be related back to topology, where for example homotpoty and manifolds are both main branches of topology.\n",
    "\n",
    "As for bad clusters, from these 10 I'd say it would be cluster 9 and cluster 3. (5 by extension)\n",
    "\n",
    "Cluster 9 is simply too hard to interpret. It has a mix of very different terms that are hard to group together. It could be correlated to the Amazon rainforest and the deforestation there but where do semantics come into the picture there.\n",
    "\n",
    "Cluster 3 in turn isn't that bad, however I feel like its too similar to that of cluster 5. THey're both related to seismic activity, and it's essentially just one being the seafloor, the other being mountains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d LDA modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf2_vectorizer = TfidfVectorizer()\n",
    "word_tokenizer = tfidf2_vectorizer.build_tokenizer()\n",
    "tokenized_text = [word_tokenizer(doc) for doc in documents]\n",
    "\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "lda_corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "lda_model = models.LdaModel(lda_corpus, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "0.0031\tGLOBEC\n",
      "0.0028\tplume\n",
      "0.0024\tEstimated\n",
      "0.0022\tNSF\n",
      "0.0021\tbiological\n",
      "0.0021\tDate\n",
      "0.0020\tPrincipal\n",
      "0.0020\tOCE\n",
      "0.0020\tProgram\n",
      "0.0019\tcurrent\n",
      "\n",
      "Topic 1\n",
      "0.0129\tProgram\n",
      "0.0125\tstudents\n",
      "0.0114\tNSF\n",
      "0.0108\tEstimated\n",
      "0.0103\tDate\n",
      "0.0078\tprogram\n",
      "0.0075\tPrincipal\n",
      "0.0075\tcurrent\n",
      "0.0068\t2002\n",
      "0.0066\tEDUCATION\n",
      "\n",
      "Topic 2\n",
      "0.0073\tcurrent\n",
      "0.0070\tPrincipal\n",
      "0.0068\tNSF\n",
      "0.0063\tProgram\n",
      "0.0059\tDate\n",
      "0.0058\tEstimated\n",
      "0.0055\t2002\n",
      "0.0041\tCo\n",
      "0.0040\tproject\n",
      "0.0031\tType\n",
      "\n",
      "Topic 3\n",
      "0.0064\tProgram\n",
      "0.0062\tEstimated\n",
      "0.0057\tNSF\n",
      "0.0056\tDate\n",
      "0.0055\tcurrent\n",
      "0.0053\tmaterials\n",
      "0.0052\t2002\n",
      "0.0050\tPrincipal\n",
      "0.0038\tResearch\n",
      "0.0033\tExpires\n",
      "\n",
      "Topic 4\n",
      "0.0021\tDate\n",
      "0.0018\tNSF\n",
      "0.0017\tEstimated\n",
      "0.0015\tProgram\n",
      "0.0014\tcurrent\n",
      "0.0013\t2002\n",
      "0.0011\tphage\n",
      "0.0011\tPrincipal\n",
      "0.0009\tManager\n",
      "0.0009\tLatest\n",
      "\n",
      "Topic 5\n",
      "0.0056\tProgram\n",
      "0.0055\tEstimated\n",
      "0.0054\tDate\n",
      "0.0052\tNSF\n",
      "0.0046\tcurrent\n",
      "0.0042\t2002\n",
      "0.0039\tPrincipal\n",
      "0.0033\tsystems\n",
      "0.0032\tOrg\n",
      "0.0032\tdata\n",
      "\n",
      "Topic 6\n",
      "0.0063\ttheory\n",
      "0.0063\tDMS\n",
      "0.0048\tMATHEMATICAL\n",
      "0.0047\tEstimated\n",
      "0.0044\tProgram\n",
      "0.0043\tDate\n",
      "0.0042\tNSF\n",
      "0.0040\tproblems\n",
      "0.0039\twhich\n",
      "0.0034\tproject\n",
      "\n",
      "Topic 7\n",
      "0.0066\tspin\n",
      "0.0058\tdevices\n",
      "0.0047\tECS\n",
      "0.0038\toptical\n",
      "0.0033\tmaterials\n",
      "0.0033\tsemiconductor\n",
      "0.0032\tdevice\n",
      "0.0031\tNSF\n",
      "0.0027\tEstimated\n",
      "0.0027\t1517\n",
      "\n",
      "Topic 8\n",
      "0.0045\tNSF\n",
      "0.0044\tDate\n",
      "0.0042\tEstimated\n",
      "0.0042\tProgram\n",
      "0.0032\thave\n",
      "0.0031\t2002\n",
      "0.0031\tcurrent\n",
      "0.0029\tPrincipal\n",
      "0.0028\tstudy\n",
      "0.0025\tSCIENCES\n",
      "\n",
      "Topic 9\n",
      "0.0066\tNSF\n",
      "0.0066\tProgram\n",
      "0.0059\tDate\n",
      "0.0057\tEstimated\n",
      "0.0046\tcurrent\n",
      "0.0043\t2002\n",
      "0.0040\tPrincipal\n",
      "0.0034\tUniversity\n",
      "0.0031\tproject\n",
      "0.0031\tPrgm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect topics\n",
    "for i, topic in lda_model.show_topics(num_words=50, formatted=False):\n",
    "    print(\"Topic\", i)\n",
    "    printed_terms = 0\n",
    "    for term, score in topic:\n",
    "        if printed_terms >= 10:\n",
    "            break\n",
    "        elif term in \"Award Investigator research this these will that the This of OF and to for in or The is be may an a with at are on by as from can\".split():\n",
    "            continue\n",
    "        printed_terms += 1\n",
    "        print(\"%.4f\\t%s\" % (score,term))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case a lot of the topics seem to be very similar if not almost identical, however because of how LDA is intended to work this does make some sense. Since this modelling is designed so that a document can fall under multiple topics.\n",
    "\n",
    "After removing some stopwords and also removing some terms that occured in every listed topic, you can see that there are some differences between the topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_words = [\"mathematics\", \"console\", \"spring\", \"technology\", \"communication\"]\n",
    "tfidf2_vectorizer = TfidfVectorizer()\n",
    "word_tokenizer = tfidf2_vectorizer.build_tokenizer()\n",
    "tokenized_text = [word_tokenizer(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base\n",
    "\n",
    "Nothing unusual here, I'm not surprised to see that the base settings are decent. Interestingly technology is only 0.77 similar to technologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 18:49:20,322 : INFO : collecting all words and their counts\n",
      "2020-03-12 18:49:20,323 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 18:49:20,857 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 18:49:20,858 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 18:49:21,140 : INFO : effective_min_count=5 retains 27041 unique words (23% of original 113911, drops 86870)\n",
      "2020-03-12 18:49:21,141 : INFO : effective_min_count=5 leaves 3552589 word corpus (96% of original 3681650, drops 129061)\n",
      "2020-03-12 18:49:21,205 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 18:49:21,208 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 18:49:21,209 : INFO : downsampling leaves estimated 2916612 word corpus (82.1% of prior 3552589)\n",
      "2020-03-12 18:49:21,274 : INFO : estimated required memory for 27041 words and 100 dimensions: 35153300 bytes\n",
      "2020-03-12 18:49:21,274 : INFO : resetting layer weights\n",
      "2020-03-12 18:49:25,237 : INFO : training model with 3 workers on 27041 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 18:49:26,261 : INFO : EPOCH 1 - PROGRESS: at 44.26% examples, 1271867 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 18:49:27,263 : INFO : EPOCH 1 - PROGRESS: at 82.74% examples, 1194191 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:49:27,703 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:49:27,704 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:49:27,706 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:49:27,706 : INFO : EPOCH - 1 : training on 3681650 raw words (2916678 effective words) took 2.5s, 1184159 effective words/s\n",
      "2020-03-12 18:49:28,714 : INFO : EPOCH 2 - PROGRESS: at 46.09% examples, 1342338 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:49:29,716 : INFO : EPOCH 2 - PROGRESS: at 85.17% examples, 1236975 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:49:30,084 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:49:30,086 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:49:30,088 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:49:30,088 : INFO : EPOCH - 2 : training on 3681650 raw words (2916613 effective words) took 2.4s, 1226344 effective words/s\n",
      "2020-03-12 18:49:31,107 : INFO : EPOCH 3 - PROGRESS: at 40.34% examples, 1161442 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:49:32,112 : INFO : EPOCH 3 - PROGRESS: at 80.83% examples, 1167406 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 18:49:32,578 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:49:32,579 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:49:32,581 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:49:32,582 : INFO : EPOCH - 3 : training on 3681650 raw words (2916403 effective words) took 2.5s, 1171602 effective words/s\n",
      "2020-03-12 18:49:33,595 : INFO : EPOCH 4 - PROGRESS: at 41.64% examples, 1213525 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:49:34,599 : INFO : EPOCH 4 - PROGRESS: at 80.83% examples, 1174469 words/s, in_qsize 5, out_qsize 1\n",
      "2020-03-12 18:49:35,067 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:49:35,069 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:49:35,078 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:49:35,078 : INFO : EPOCH - 4 : training on 3681650 raw words (2916277 effective words) took 2.5s, 1173276 effective words/s\n",
      "2020-03-12 18:49:36,089 : INFO : EPOCH 5 - PROGRESS: at 39.04% examples, 1132171 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 18:49:37,105 : INFO : EPOCH 5 - PROGRESS: at 78.46% examples, 1131460 words/s, in_qsize 5, out_qsize 2\n",
      "2020-03-12 18:49:37,637 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:49:37,640 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:49:37,649 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:49:37,650 : INFO : EPOCH - 5 : training on 3681650 raw words (2917133 effective words) took 2.6s, 1136199 effective words/s\n",
      "2020-03-12 18:49:37,650 : INFO : training on a 18408250 raw words (14583104 effective words) took 12.4s, 1174897 effective words/s\n",
      "2020-03-12 18:49:37,660 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  mathematics\n",
      "[('discipline', 0.813413679599762), ('science', 0.8133169412612915), ('engineering', 0.8105632066726685), ('literacy', 0.750910758972168), ('majors', 0.7439529895782471), ('physics', 0.741843581199646), ('practice', 0.7403053045272827), ('sciences', 0.7306692004203796), ('concepts', 0.7280646562576294), ('careers', 0.705974817276001)]\n",
      "Most similar to:  console\n",
      "[('fluorometer', 0.8892923593521118), ('shake', 0.8729320168495178), ('compass', 0.8702945709228516), ('calorimeter', 0.8649951815605164), ('chirped', 0.851511538028717), ('interferometer', 0.8476803302764893), ('cabinetry', 0.8462677597999573), ('spotting', 0.8419115543365479), ('inkjet', 0.8396214842796326), ('sizing', 0.8387095928192139)]\n",
      "Most similar to:  spring\n",
      "[('late', 0.8034616708755493), ('Miocene', 0.7378062605857849), ('1996', 0.7290747165679932), ('fall', 0.7276666760444641), ('1995', 0.6936283707618713), ('Triassic', 0.6931883096694946), ('Sydney', 0.6879227757453918), ('Sept', 0.6851096749305725), ('meridional', 0.6832845211029053), ('period', 0.6817299127578735)]\n",
      "Most similar to:  technology\n",
      "[('technologies', 0.7731820940971375), ('innovation', 0.709852933883667), ('industry', 0.7097426056861877), ('nanotechnology', 0.7043181657791138), ('emerging', 0.6882134675979614), ('manufacturing', 0.6735219359397888), ('technological', 0.6725765466690063), ('biotechnology', 0.6636173725128174), ('standards', 0.6596079468727112), ('vision', 0.6583538055419922)]\n",
      "Most similar to:  communication\n",
      "[('networking', 0.8730631470680237), ('wireless', 0.8683319091796875), ('security', 0.8467535376548767), ('communications', 0.8462536931037903), ('computing', 0.8292272090911865), ('transportation', 0.821768581867218), ('architectural', 0.8175112009048462), ('networks', 0.8083754777908325), ('routing', 0.8032872080802917), ('network', 0.7976449728012085)]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# test with size, window, min_count, iter, sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text)\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size = 5, Size = 200\n",
    "\n",
    "A low size is a huge detriment too the quality of the model. It underfits the model and it then thinks everything is very similar to the given word.\n",
    "\n",
    "A larger value than the default 100 doesn't necessarily change much. At least in this case. The order of words and such that are similar does change a little, but it does not seem to give any proper quantitative value. Maybe with a larger corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 18:50:06,005 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2020-03-12 18:50:06,006 : INFO : collecting all words and their counts\n",
      "2020-03-12 18:50:06,007 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 18:50:06,535 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 18:50:06,536 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 18:50:06,862 : INFO : effective_min_count=5 retains 27041 unique words (23% of original 113911, drops 86870)\n",
      "2020-03-12 18:50:06,863 : INFO : effective_min_count=5 leaves 3552589 word corpus (96% of original 3681650, drops 129061)\n",
      "2020-03-12 18:50:06,933 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 18:50:06,935 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 18:50:06,936 : INFO : downsampling leaves estimated 2916612 word corpus (82.1% of prior 3552589)\n",
      "2020-03-12 18:50:06,996 : INFO : estimated required memory for 27041 words and 5 dimensions: 14602140 bytes\n",
      "2020-03-12 18:50:06,997 : INFO : resetting layer weights\n",
      "2020-03-12 18:50:10,934 : INFO : training model with 3 workers on 27041 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 18:50:11,940 : INFO : EPOCH 1 - PROGRESS: at 62.65% examples, 1828242 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 18:50:12,584 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:50:12,585 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:50:12,586 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:50:12,587 : INFO : EPOCH - 1 : training on 3681650 raw words (2916839 effective words) took 1.6s, 1770394 effective words/s\n",
      "2020-03-12 18:50:13,592 : INFO : EPOCH 2 - PROGRESS: at 54.98% examples, 1603355 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 18:50:14,378 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:50:14,379 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:50:14,381 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:50:14,382 : INFO : EPOCH - 2 : training on 3681650 raw words (2916926 effective words) took 1.8s, 1628627 effective words/s\n",
      "2020-03-12 18:50:15,389 : INFO : EPOCH 3 - PROGRESS: at 56.01% examples, 1630880 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:50:16,158 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:50:16,160 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:50:16,162 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:50:16,162 : INFO : EPOCH - 3 : training on 3681650 raw words (2916216 effective words) took 1.8s, 1641536 effective words/s\n",
      "2020-03-12 18:50:17,169 : INFO : EPOCH 4 - PROGRESS: at 77.91% examples, 2266979 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:50:17,445 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:50:17,448 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:50:17,450 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:50:17,450 : INFO : EPOCH - 4 : training on 3681650 raw words (2916819 effective words) took 1.3s, 2271875 effective words/s\n",
      "2020-03-12 18:50:18,458 : INFO : EPOCH 5 - PROGRESS: at 54.98% examples, 1598731 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 18:50:19,123 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:50:19,125 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:50:19,128 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:50:19,128 : INFO : EPOCH - 5 : training on 3681650 raw words (2916855 effective words) took 1.7s, 1742578 effective words/s\n",
      "2020-03-12 18:50:19,128 : INFO : training on a 18408250 raw words (14583655 effective words) took 8.2s, 1779809 effective words/s\n",
      "2020-03-12 18:50:19,138 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  mathematics\n",
      "[('science', 0.9933551549911499), ('industrial', 0.9932775497436523), ('enhancing', 0.9929811954498291), ('focusing', 0.9906290173530579), ('economics', 0.9903695583343506), ('promoting', 0.9901710152626038), ('interested', 0.9900872707366943), ('engineering', 0.9888055324554443), ('disciplines', 0.9886555671691895), ('awareness', 0.9861686825752258)]\n",
      "Most similar to:  console\n",
      "[('interfering', 0.9994806051254272), ('MSCs', 0.9993323087692261), ('insecticides', 0.9991728067398071), ('gravitationally', 0.9990473985671997), ('Pseudocalanus', 0.9988341331481934), ('disequilibria', 0.998599648475647), ('somatic', 0.9985452890396118), ('3C', 0.9980460405349731), ('dip', 0.9979593753814697), ('Chusang', 0.9974350929260254)]\n",
      "Most similar to:  spring\n",
      "[('meridional', 0.9877832531929016), ('Melville', 0.9772300124168396), ('GPa', 0.9688113331794739), ('cam', 0.9660539627075195), ('Luke', 0.964349627494812), ('606042218', 0.9621831178665161), ('Kimball', 0.9603199362754822), ('subtropical', 0.9574132561683655), ('meters', 0.9566030502319336), ('Bill', 0.9551271200180054)]\n",
      "Most similar to:  technology\n",
      "[('resources', 0.9986404776573181), ('sharing', 0.9957191944122314), ('collection', 0.9932020902633667), ('pedagogical', 0.993184506893158), ('advanced', 0.9924819469451904), ('skills', 0.9907841682434082), ('creating', 0.9897846579551697), ('building', 0.9896368384361267), ('networking', 0.9893449544906616), ('share', 0.9862260222434998)]\n",
      "Most similar to:  communication\n",
      "[('manufacturing', 0.9994798898696899), ('generation', 0.9929558634757996), ('communications', 0.9919814467430115), ('computer', 0.989551305770874), ('vehicle', 0.9883595705032349), ('traditional', 0.9880405068397522), ('networks', 0.9873889088630676), ('delivery', 0.9854302406311035), ('multi', 0.9853246212005615), ('fabrication', 0.9843739867210388)]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# test with size (int), window(int), min_count(int), iter(int), sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text, size=5)\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 19:04:25,184 : INFO : collecting all words and their counts\n",
      "2020-03-12 19:04:25,185 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 19:04:25,718 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 19:04:25,719 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 19:04:25,794 : INFO : effective_min_count=5 retains 27041 unique words (23% of original 113911, drops 86870)\n",
      "2020-03-12 19:04:25,795 : INFO : effective_min_count=5 leaves 3552589 word corpus (96% of original 3681650, drops 129061)\n",
      "2020-03-12 19:04:25,861 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 19:04:25,863 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 19:04:25,863 : INFO : downsampling leaves estimated 2916612 word corpus (82.1% of prior 3552589)\n",
      "2020-03-12 19:04:25,917 : INFO : estimated required memory for 27041 words and 200 dimensions: 56786100 bytes\n",
      "2020-03-12 19:04:25,918 : INFO : resetting layer weights\n",
      "2020-03-12 19:04:30,040 : INFO : training model with 3 workers on 27041 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 19:04:31,056 : INFO : EPOCH 1 - PROGRESS: at 39.83% examples, 1148267 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:04:32,067 : INFO : EPOCH 1 - PROGRESS: at 71.45% examples, 1030777 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:04:32,974 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:04:32,976 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:04:32,978 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:04:32,978 : INFO : EPOCH - 1 : training on 3681650 raw words (2916637 effective words) took 2.9s, 993752 effective words/s\n",
      "2020-03-12 19:04:33,998 : INFO : EPOCH 2 - PROGRESS: at 39.04% examples, 1121172 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:04:35,020 : INFO : EPOCH 2 - PROGRESS: at 70.93% examples, 1016011 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:04:35,677 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:04:35,678 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:04:35,686 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:04:35,686 : INFO : EPOCH - 2 : training on 3681650 raw words (2916208 effective words) took 2.7s, 1078418 effective words/s\n",
      "2020-03-12 19:04:36,698 : INFO : EPOCH 3 - PROGRESS: at 41.64% examples, 1210976 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:04:37,713 : INFO : EPOCH 3 - PROGRESS: at 77.11% examples, 1113568 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:04:38,423 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:04:38,426 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:04:38,426 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:04:38,427 : INFO : EPOCH - 3 : training on 3681650 raw words (2916568 effective words) took 2.7s, 1067056 effective words/s\n",
      "2020-03-12 19:04:39,432 : INFO : EPOCH 4 - PROGRESS: at 30.95% examples, 903744 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:04:40,445 : INFO : EPOCH 4 - PROGRESS: at 62.10% examples, 900952 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:04:41,447 : INFO : EPOCH 4 - PROGRESS: at 93.44% examples, 902555 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:04:41,638 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:04:41,650 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:04:41,653 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:04:41,654 : INFO : EPOCH - 4 : training on 3681650 raw words (2916570 effective words) took 3.2s, 904900 effective words/s\n",
      "2020-03-12 19:04:42,667 : INFO : EPOCH 5 - PROGRESS: at 30.95% examples, 897447 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:04:43,669 : INFO : EPOCH 5 - PROGRESS: at 61.83% examples, 898138 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:04:44,688 : INFO : EPOCH 5 - PROGRESS: at 92.66% examples, 890870 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:04:44,907 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:04:44,918 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:04:44,922 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:04:44,922 : INFO : EPOCH - 5 : training on 3681650 raw words (2916443 effective words) took 3.3s, 893519 effective words/s\n",
      "2020-03-12 19:04:44,923 : INFO : training on a 18408250 raw words (14582426 effective words) took 14.9s, 979865 effective words/s\n",
      "2020-03-12 19:04:44,930 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  mathematics\n",
      "[('science', 0.8161193132400513), ('engineering', 0.8135734796524048), ('physics', 0.776203989982605), ('literacy', 0.7544336318969727), ('discipline', 0.7528648376464844), ('sciences', 0.7514590620994568), ('nanotechnology', 0.7381583452224731), ('practice', 0.7265830636024475), ('concepts', 0.7165606021881104), ('careers', 0.7163411378860474)]\n",
      "Most similar to:  console\n",
      "[('API', 0.8792423009872437), ('OPL', 0.8758563995361328), ('chirped', 0.8735769391059875), ('inkjet', 0.8725205659866333), ('cabinetry', 0.8700251579284668), ('reader', 0.8603600263595581), ('ankle', 0.8571581840515137), ('Switch', 0.8512565493583679), ('compass', 0.8501532077789307), ('addressable', 0.8495756387710571)]\n",
      "Most similar to:  spring\n",
      "[('42', 0.7050052285194397), ('Spain', 0.6770830154418945), ('24', 0.6746290922164917), ('1991', 0.6729241013526917), ('1999', 0.6727629899978638), ('13', 0.6706552505493164), ('preceding', 0.6692010760307312), ('Classic', 0.6662352085113525), ('34', 0.6642528176307678), ('21', 0.661834716796875)]\n",
      "Most similar to:  technology\n",
      "[('technologies', 0.7756147384643555), ('manufacturing', 0.7102969884872437), ('nanotechnology', 0.705764651298523), ('vision', 0.7011598944664001), ('innovation', 0.687083899974823), ('emerging', 0.6868571043014526), ('standards', 0.6795938014984131), ('industry', 0.6790831089019775), ('technological', 0.6697720289230347), ('nanoscience', 0.6578951478004456)]\n",
      "Most similar to:  communication\n",
      "[('networking', 0.8508043885231018), ('wireless', 0.8458081483840942), ('security', 0.8342825770378113), ('communications', 0.8290494084358215), ('networks', 0.8020681142807007), ('computing', 0.7980635166168213), ('routing', 0.797751784324646), ('virtual', 0.7959601283073425), ('intelligence', 0.7956924438476562), ('architectural', 0.7934691905975342)]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# test with size (int), window(int), min_count(int), iter(int), sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text, size=200)\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min_count=2,10\n",
    "\n",
    "Going too high on min_count simply causes situations where there are no words that are similar.\n",
    "\n",
    "For words similar to mathematics it does not make a big difference, since they seem to occur so often in this scope of documents. For words similar to spring and console however, you quite quickly lose a lot of words from the list, and it just fills them with new ones. For whatever reason this does increase the similarity compared to base though.\n",
    "\n",
    "With a high count that is too high, in this case it just doesn't know some words. In my case console doesn't exist with a mincount that is 7 or higher. At this point console and similarities simply feels random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 18:54:36,744 : INFO : collecting all words and their counts\n",
      "2020-03-12 18:54:36,745 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 18:54:37,264 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 18:54:37,265 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 18:54:37,619 : INFO : effective_min_count=2 retains 53696 unique words (47% of original 113911, drops 60215)\n",
      "2020-03-12 18:54:37,620 : INFO : effective_min_count=2 leaves 3621435 word corpus (98% of original 3681650, drops 60215)\n",
      "2020-03-12 18:54:37,755 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 18:54:37,757 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 18:54:37,758 : INFO : downsampling leaves estimated 2993497 word corpus (82.7% of prior 3621435)\n",
      "2020-03-12 18:54:37,884 : INFO : estimated required memory for 53696 words and 100 dimensions: 69804800 bytes\n",
      "2020-03-12 18:54:37,885 : INFO : resetting layer weights\n",
      "2020-03-12 18:54:45,904 : INFO : training model with 3 workers on 53696 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 18:54:46,912 : INFO : EPOCH 1 - PROGRESS: at 42.66% examples, 1277377 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:54:47,920 : INFO : EPOCH 1 - PROGRESS: at 82.74% examples, 1231322 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 18:54:48,350 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:54:48,356 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:54:48,363 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:54:48,363 : INFO : EPOCH - 1 : training on 3681650 raw words (2993800 effective words) took 2.5s, 1219885 effective words/s\n",
      "2020-03-12 18:54:49,370 : INFO : EPOCH 2 - PROGRESS: at 38.75% examples, 1158179 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 18:54:50,385 : INFO : EPOCH 2 - PROGRESS: at 78.18% examples, 1160231 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 18:54:50,924 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:54:50,931 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:54:50,936 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:54:50,937 : INFO : EPOCH - 2 : training on 3681650 raw words (2993486 effective words) took 2.6s, 1165230 effective words/s\n",
      "2020-03-12 18:54:51,948 : INFO : EPOCH 3 - PROGRESS: at 54.14% examples, 1612841 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 18:54:52,951 : INFO : EPOCH 3 - PROGRESS: at 99.41% examples, 1481416 words/s, in_qsize 3, out_qsize 0\n",
      "2020-03-12 18:54:52,954 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:54:52,961 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:54:52,966 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:54:52,967 : INFO : EPOCH - 3 : training on 3681650 raw words (2994110 effective words) took 2.0s, 1478294 effective words/s\n",
      "2020-03-12 18:54:53,971 : INFO : EPOCH 4 - PROGRESS: at 39.29% examples, 1176627 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:54:54,974 : INFO : EPOCH 4 - PROGRESS: at 80.57% examples, 1203989 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 18:54:55,318 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:54:55,319 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:54:55,322 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:54:55,323 : INFO : EPOCH - 4 : training on 3681650 raw words (2993104 effective words) took 2.4s, 1272634 effective words/s\n",
      "2020-03-12 18:54:56,333 : INFO : EPOCH 5 - PROGRESS: at 38.75% examples, 1159535 words/s, in_qsize 5, out_qsize 1\n",
      "2020-03-12 18:54:57,345 : INFO : EPOCH 5 - PROGRESS: at 78.46% examples, 1166110 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:54:57,878 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:54:57,879 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:54:57,888 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:54:57,888 : INFO : EPOCH - 5 : training on 3681650 raw words (2994146 effective words) took 2.6s, 1170804 effective words/s\n",
      "2020-03-12 18:54:57,889 : INFO : training on a 18408250 raw words (14968646 effective words) took 12.0s, 1249050 effective words/s\n",
      "2020-03-12 18:54:57,897 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  mathematics\n",
      "[('science', 0.8420685529708862), ('engineering', 0.8404231071472168), ('sciences', 0.7676929235458374), ('discipline', 0.763458788394928), ('physics', 0.758654773235321), ('practice', 0.7490870356559753), ('concepts', 0.7400397062301636), ('professionals', 0.7394565939903259), ('education', 0.7319020628929138), ('nanoscience', 0.7282558679580688)]\n",
      "Most similar to:  console\n",
      "[('peroxide', 0.9277517199516296), ('Immuno', 0.9020063877105713), ('Gallium', 0.9012449979782104), ('bipolar', 0.8988353610038757), ('masonry', 0.896985650062561), ('attenuated', 0.8966995477676392), ('inhibitor', 0.895756185054779), ('CCVD', 0.8930081129074097), ('RNPR2', 0.8893278241157532), ('somites', 0.8876462578773499)]\n",
      "Most similar to:  spring\n",
      "[('winter', 0.7589244842529297), ('late', 0.7575393915176392), ('Miocene', 0.7560657262802124), ('Tertiary', 0.7387002110481262), ('northern', 0.7340983152389526), ('north', 0.7309749126434326), ('Holocene', 0.7283297777175903), ('fall', 0.7086670398712158), ('southern', 0.7020965814590454), ('Peru', 0.7014148831367493)]\n",
      "Most similar to:  technology\n",
      "[('technologies', 0.7725236415863037), ('innovation', 0.7199573516845703), ('manufacturing', 0.7167273759841919), ('industry', 0.7094303369522095), ('nanotechnology', 0.7082958817481995), ('company', 0.6979166269302368), ('infrastructure', 0.6951360702514648), ('vision', 0.6943759918212891), ('industries', 0.688491940498352), ('biotechnology', 0.6862728595733643)]\n",
      "Most similar to:  communication\n",
      "[('security', 0.8700631856918335), ('transportation', 0.8570749759674072), ('networking', 0.8511320352554321), ('virtual', 0.8445765376091003), ('wireless', 0.8411312699317932), ('communications', 0.828783392906189), ('computing', 0.8251001238822937), ('sharing', 0.8223384618759155), ('networks', 0.818806529045105), ('distributed', 0.8157088160514832)]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig()\n",
    "# test with size (int), window(int), min_count(int), iter(int), sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text, min_count=2)\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 19:38:14,801 : INFO : collecting all words and their counts\n",
      "2020-03-12 19:38:14,803 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 19:38:15,322 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 19:38:15,323 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 19:38:15,392 : INFO : effective_min_count=6 retains 24178 unique words (21% of original 113911, drops 89733)\n",
      "2020-03-12 19:38:15,393 : INFO : effective_min_count=6 leaves 3538274 word corpus (96% of original 3681650, drops 143376)\n",
      "2020-03-12 19:38:15,447 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 19:38:15,449 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 19:38:15,450 : INFO : downsampling leaves estimated 2900621 word corpus (82.0% of prior 3538274)\n",
      "2020-03-12 19:38:15,493 : INFO : estimated required memory for 24178 words and 100 dimensions: 31431400 bytes\n",
      "2020-03-12 19:38:15,494 : INFO : resetting layer weights\n",
      "2020-03-12 19:38:18,908 : INFO : training model with 3 workers on 24178 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 19:38:19,930 : INFO : EPOCH 1 - PROGRESS: at 40.34% examples, 1156857 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:38:20,932 : INFO : EPOCH 1 - PROGRESS: at 80.34% examples, 1155842 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:38:21,407 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:38:21,409 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:38:21,410 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:38:21,411 : INFO : EPOCH - 1 : training on 3681650 raw words (2900244 effective words) took 2.5s, 1163235 effective words/s\n",
      "2020-03-12 19:38:22,419 : INFO : EPOCH 2 - PROGRESS: at 39.54% examples, 1143699 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:38:23,424 : INFO : EPOCH 2 - PROGRESS: at 81.10% examples, 1170388 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:38:23,873 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:38:23,875 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:38:23,876 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:38:23,877 : INFO : EPOCH - 2 : training on 3681650 raw words (2900175 effective words) took 2.5s, 1178007 effective words/s\n",
      "2020-03-12 19:38:24,889 : INFO : EPOCH 3 - PROGRESS: at 40.84% examples, 1178704 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:38:25,892 : INFO : EPOCH 3 - PROGRESS: at 82.22% examples, 1185724 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:38:26,317 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:38:26,319 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:38:26,327 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:38:26,328 : INFO : EPOCH - 3 : training on 3681650 raw words (2900758 effective words) took 2.4s, 1185719 effective words/s\n",
      "2020-03-12 19:38:27,334 : INFO : EPOCH 4 - PROGRESS: at 40.09% examples, 1161312 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:38:28,341 : INFO : EPOCH 4 - PROGRESS: at 80.83% examples, 1166117 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:38:28,799 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:38:28,802 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:38:28,811 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:38:28,811 : INFO : EPOCH - 4 : training on 3681650 raw words (2900505 effective words) took 2.5s, 1169672 effective words/s\n",
      "2020-03-12 19:38:29,827 : INFO : EPOCH 5 - PROGRESS: at 40.55% examples, 1168048 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:38:30,830 : INFO : EPOCH 5 - PROGRESS: at 81.96% examples, 1180143 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:38:31,264 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:38:31,272 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:38:31,276 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:38:31,276 : INFO : EPOCH - 5 : training on 3681650 raw words (2901409 effective words) took 2.5s, 1179530 effective words/s\n",
      "2020-03-12 19:38:31,277 : INFO : training on a 18408250 raw words (14503091 effective words) took 12.4s, 1172632 effective words/s\n",
      "2020-03-12 19:38:31,278 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  mathematics\n",
      "[('science', 0.823967695236206), ('discipline', 0.8136472702026367), ('engineering', 0.7935560941696167), ('physics', 0.7585951089859009), ('sciences', 0.7480071187019348), ('majors', 0.740909993648529), ('literacy', 0.7239643335342407), ('humanities', 0.7178128957748413), ('practice', 0.7135639786720276), ('elementary', 0.7031224966049194)]\n",
      "Most similar to:  console\n",
      "[('Least', 0.9102983474731445), ('Squares', 0.9024231433868408), ('inhibitor', 0.9012001752853394), ('cryo', 0.9001132249832153), ('Telluride', 0.8987306356430054), ('osmium', 0.895600438117981), ('pumped', 0.8945826888084412), ('ellipticals', 0.8942266702651978), ('polystyrene', 0.8937935829162598), ('GPR', 0.892507791519165)]\n",
      "Most similar to:  spring\n",
      "[('Miocene', 0.7379252910614014), ('late', 0.7371035814285278), ('Tertiary', 0.7284600138664246), ('north', 0.7229325175285339), ('south', 0.7056753635406494), ('winter', 0.7026774287223816), ('Andes', 0.6979003548622131), ('northern', 0.6944183111190796), ('hemisphere', 0.6931707859039307), ('monsoon', 0.692557156085968)]\n",
      "Most similar to:  technology\n",
      "[('technologies', 0.7641334533691406), ('vision', 0.7057880759239197), ('nanotechnology', 0.6867215037345886), ('manufacturing', 0.6795425415039062), ('innovation', 0.6724838018417358), ('industry', 0.6699641942977905), ('biotechnology', 0.6661337018013), ('standards', 0.6645470857620239), ('electronics', 0.6623314619064331), ('industries', 0.6464810967445374)]\n",
      "Most similar to:  communication\n",
      "[('networking', 0.857134997844696), ('security', 0.8526128530502319), ('wireless', 0.8413337469100952), ('computing', 0.8212554454803467), ('transportation', 0.8203274011611938), ('communications', 0.8176514506340027), ('hardware', 0.8075908422470093), ('scheduling', 0.8008474111557007), ('multimedia', 0.7940057516098022), ('virtual', 0.7912511229515076)]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig()\n",
    "# test with size (int), window(int), min_count(int), iter(int), sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text, min_count=6)\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iter = 10\n",
    "\n",
    "Going higher on the iterations seem to make things more accurate, especially looking at mathematics and technology. The same word in different forms gets higher in similarity compared to the base, when looking at the order instead of the value. So essentially this tells us that the base values were underfitting our data. With too many iterations however you would potentially be looking at iterations, this didn't feel like it started occuring yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 18:57:31,284 : INFO : collecting all words and their counts\n",
      "2020-03-12 18:57:31,284 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 18:57:31,798 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 18:57:31,799 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 18:57:31,871 : INFO : effective_min_count=5 retains 27041 unique words (23% of original 113911, drops 86870)\n",
      "2020-03-12 18:57:31,872 : INFO : effective_min_count=5 leaves 3552589 word corpus (96% of original 3681650, drops 129061)\n",
      "2020-03-12 18:57:31,935 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 18:57:31,937 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 18:57:31,937 : INFO : downsampling leaves estimated 2916612 word corpus (82.1% of prior 3552589)\n",
      "2020-03-12 18:57:31,991 : INFO : estimated required memory for 27041 words and 100 dimensions: 35153300 bytes\n",
      "2020-03-12 18:57:31,992 : INFO : resetting layer weights\n",
      "2020-03-12 18:57:35,950 : INFO : training model with 3 workers on 27041 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 18:57:36,968 : INFO : EPOCH 1 - PROGRESS: at 41.64% examples, 1200328 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 18:57:37,976 : INFO : EPOCH 1 - PROGRESS: at 80.83% examples, 1165508 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 18:57:38,439 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:57:38,445 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:57:38,446 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:57:38,446 : INFO : EPOCH - 1 : training on 3681650 raw words (2916360 effective words) took 2.5s, 1170470 effective words/s\n",
      "2020-03-12 18:57:39,458 : INFO : EPOCH 2 - PROGRESS: at 39.54% examples, 1149857 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:57:40,474 : INFO : EPOCH 2 - PROGRESS: at 80.34% examples, 1159206 words/s, in_qsize 5, out_qsize 1\n",
      "2020-03-12 18:57:40,942 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:57:40,943 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:57:40,945 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:57:40,947 : INFO : EPOCH - 2 : training on 3681650 raw words (2917199 effective words) took 2.5s, 1169753 effective words/s\n",
      "2020-03-12 18:57:41,959 : INFO : EPOCH 3 - PROGRESS: at 39.83% examples, 1153623 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:57:42,973 : INFO : EPOCH 3 - PROGRESS: at 80.57% examples, 1162614 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:57:43,437 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:57:43,445 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:57:43,448 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:57:43,449 : INFO : EPOCH - 3 : training on 3681650 raw words (2916222 effective words) took 2.5s, 1167847 effective words/s\n",
      "2020-03-12 18:57:44,477 : INFO : EPOCH 4 - PROGRESS: at 41.10% examples, 1195311 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:57:45,478 : INFO : EPOCH 4 - PROGRESS: at 82.22% examples, 1194815 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:57:45,907 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:57:45,908 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:57:45,909 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:57:45,910 : INFO : EPOCH - 4 : training on 3681650 raw words (2916393 effective words) took 2.4s, 1196264 effective words/s\n",
      "2020-03-12 18:57:46,917 : INFO : EPOCH 5 - PROGRESS: at 47.72% examples, 1389415 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:57:47,926 : INFO : EPOCH 5 - PROGRESS: at 88.90% examples, 1287253 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:57:48,178 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:57:48,183 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:57:48,189 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:57:48,190 : INFO : EPOCH - 5 : training on 3681650 raw words (2916740 effective words) took 2.3s, 1281427 effective words/s\n",
      "2020-03-12 18:57:49,196 : INFO : EPOCH 6 - PROGRESS: at 40.55% examples, 1184015 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 18:57:50,203 : INFO : EPOCH 6 - PROGRESS: at 81.39% examples, 1181778 words/s, in_qsize 5, out_qsize 1\n",
      "2020-03-12 18:57:50,622 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:57:50,624 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:57:50,628 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:57:50,628 : INFO : EPOCH - 6 : training on 3681650 raw words (2917543 effective words) took 2.4s, 1198507 effective words/s\n",
      "2020-03-12 18:57:51,636 : INFO : EPOCH 7 - PROGRESS: at 40.55% examples, 1182026 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 18:57:52,639 : INFO : EPOCH 7 - PROGRESS: at 81.40% examples, 1182268 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:57:53,092 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:57:53,095 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:57:53,103 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:57:53,104 : INFO : EPOCH - 7 : training on 3681650 raw words (2916510 effective words) took 2.5s, 1180112 effective words/s\n",
      "2020-03-12 18:57:54,109 : INFO : EPOCH 8 - PROGRESS: at 41.10% examples, 1200279 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:57:55,117 : INFO : EPOCH 8 - PROGRESS: at 82.45% examples, 1196667 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 18:57:55,552 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:57:55,557 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:57:55,564 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:57:55,564 : INFO : EPOCH - 8 : training on 3681650 raw words (2917364 effective words) took 2.5s, 1187572 effective words/s\n",
      "2020-03-12 18:57:56,576 : INFO : EPOCH 9 - PROGRESS: at 54.14% examples, 1570590 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:57:57,474 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:57:57,478 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:57:57,481 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:57:57,482 : INFO : EPOCH - 9 : training on 3681650 raw words (2916140 effective words) took 1.9s, 1524933 effective words/s\n",
      "2020-03-12 18:57:58,489 : INFO : EPOCH 10 - PROGRESS: at 40.09% examples, 1168289 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 18:57:59,492 : INFO : EPOCH 10 - PROGRESS: at 79.80% examples, 1160150 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 18:57:59,990 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:58:00,000 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:58:00,002 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:58:00,002 : INFO : EPOCH - 10 : training on 3681650 raw words (2915896 effective words) took 2.5s, 1159524 effective words/s\n",
      "2020-03-12 18:58:00,003 : INFO : training on a 36816500 raw words (29166367 effective words) took 24.1s, 1212630 effective words/s\n",
      "2020-03-12 18:58:00,011 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  mathematics\n",
      "[('discipline', 0.6966089010238647), ('mathematical', 0.684969961643219), ('physics', 0.6776703596115112), ('statistics', 0.6553087830543518), ('concepts', 0.6546766757965088), ('majors', 0.6530417203903198), ('engineering', 0.6441001296043396), ('teachers', 0.6436055898666382), ('science', 0.6399111747741699), ('teaching', 0.6306414604187012)]\n",
      "Most similar to:  console\n",
      "[('collector', 0.7510401606559753), ('Berger', 0.7244255542755127), ('880', 0.7084409594535828), ('orchestrated', 0.6967594623565674), ('12CO2', 0.6941226720809937), ('chirped', 0.6934460997581482), ('Multibeam', 0.6846871376037598), ('Intercontinental', 0.6845357418060303), ('SOFDI', 0.6831621527671814), ('photodetector', 0.6816697120666504)]\n",
      "Most similar to:  spring\n",
      "[('late', 0.672702431678772), ('winter', 0.6662265658378601), ('Miocene', 0.6605401039123535), ('meridional', 0.6553417444229126), ('fall', 0.641124963760376), ('north', 0.6266238689422607), ('Rio', 0.6243113279342651), ('Tertiary', 0.6240402460098267), ('season', 0.6216842532157898), ('Ma', 0.6176620721817017)]\n",
      "Most similar to:  technology\n",
      "[('technologies', 0.7411321997642517), ('technological', 0.6322504281997681), ('nanotechnology', 0.622840166091919), ('industry', 0.6079474687576294), ('manufacturing', 0.6045987606048584), ('telecommunications', 0.5818339586257935), ('electronics', 0.5705662369728088), ('innovation', 0.5694010257720947), ('biotechnology', 0.5649293661117554), ('capability', 0.5649101734161377)]\n",
      "Most similar to:  communication\n",
      "[('communications', 0.7879689335823059), ('wireless', 0.7792861461639404), ('networking', 0.7647997140884399), ('distributed', 0.7032225131988525), ('security', 0.6849343776702881), ('secure', 0.6748926639556885), ('network', 0.6734974384307861), ('middleware', 0.6716087460517883), ('computing', 0.6687073707580566), ('mobile', 0.6662724018096924)]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# test with size (int), window(int), min_count(int), iter(int), sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text, iter=(10))\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 19:43:19,409 : INFO : collecting all words and their counts\n",
      "2020-03-12 19:43:19,412 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 19:43:19,914 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 19:43:19,915 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 19:43:19,984 : INFO : effective_min_count=5 retains 27041 unique words (23% of original 113911, drops 86870)\n",
      "2020-03-12 19:43:19,984 : INFO : effective_min_count=5 leaves 3552589 word corpus (96% of original 3681650, drops 129061)\n",
      "2020-03-12 19:43:20,047 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 19:43:20,049 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 19:43:20,050 : INFO : downsampling leaves estimated 2916612 word corpus (82.1% of prior 3552589)\n",
      "2020-03-12 19:43:20,104 : INFO : estimated required memory for 27041 words and 100 dimensions: 35153300 bytes\n",
      "2020-03-12 19:43:20,104 : INFO : resetting layer weights\n",
      "2020-03-12 19:43:24,031 : INFO : training model with 3 workers on 27041 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 19:43:25,041 : INFO : EPOCH 1 - PROGRESS: at 46.59% examples, 1357414 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:43:26,044 : INFO : EPOCH 1 - PROGRESS: at 86.53% examples, 1255561 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:43:26,373 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:43:26,375 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:43:26,377 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:43:26,377 : INFO : EPOCH - 1 : training on 3681650 raw words (2916677 effective words) took 2.3s, 1245990 effective words/s\n",
      "2020-03-12 19:43:27,383 : INFO : EPOCH 2 - PROGRESS: at 39.29% examples, 1144997 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:43:28,395 : INFO : EPOCH 2 - PROGRESS: at 78.73% examples, 1139822 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:43:28,923 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:43:28,925 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:43:28,925 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:43:28,926 : INFO : EPOCH - 2 : training on 3681650 raw words (2916547 effective words) took 2.5s, 1146098 effective words/s\n",
      "2020-03-12 19:43:29,939 : INFO : EPOCH 3 - PROGRESS: at 39.83% examples, 1152967 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:43:30,947 : INFO : EPOCH 3 - PROGRESS: at 79.80% examples, 1153033 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:43:31,435 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:43:31,444 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:43:31,447 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:43:31,448 : INFO : EPOCH - 3 : training on 3681650 raw words (2915619 effective words) took 2.5s, 1158353 effective words/s\n",
      "2020-03-12 19:43:32,456 : INFO : EPOCH 4 - PROGRESS: at 46.89% examples, 1366329 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:43:33,459 : INFO : EPOCH 4 - PROGRESS: at 87.09% examples, 1263422 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:43:33,771 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:43:33,780 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:43:33,783 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:43:33,784 : INFO : EPOCH - 4 : training on 3681650 raw words (2917678 effective words) took 2.3s, 1251312 effective words/s\n",
      "2020-03-12 19:43:34,799 : INFO : EPOCH 5 - PROGRESS: at 39.83% examples, 1150537 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:43:35,802 : INFO : EPOCH 5 - PROGRESS: at 79.80% examples, 1155601 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:43:36,292 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:43:36,300 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:43:36,303 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:43:36,304 : INFO : EPOCH - 5 : training on 3681650 raw words (2916518 effective words) took 2.5s, 1159914 effective words/s\n",
      "2020-03-12 19:43:37,308 : INFO : EPOCH 6 - PROGRESS: at 39.29% examples, 1145576 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:43:38,310 : INFO : EPOCH 6 - PROGRESS: at 79.53% examples, 1157667 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:43:38,812 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:43:38,821 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:43:38,823 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:43:38,824 : INFO : EPOCH - 6 : training on 3681650 raw words (2916707 effective words) took 2.5s, 1158924 effective words/s\n",
      "2020-03-12 19:43:39,838 : INFO : EPOCH 7 - PROGRESS: at 39.83% examples, 1157727 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:43:40,838 : INFO : EPOCH 7 - PROGRESS: at 80.08% examples, 1164235 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:43:41,322 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:43:41,331 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:43:41,334 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:43:41,334 : INFO : EPOCH - 7 : training on 3681650 raw words (2916857 effective words) took 2.5s, 1166168 effective words/s\n",
      "2020-03-12 19:43:42,339 : INFO : EPOCH 8 - PROGRESS: at 39.54% examples, 1154587 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:43:43,350 : INFO : EPOCH 8 - PROGRESS: at 79.80% examples, 1156378 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:43:43,833 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:43:43,841 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:43:43,845 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:43:43,846 : INFO : EPOCH - 8 : training on 3681650 raw words (2916676 effective words) took 2.5s, 1163298 effective words/s\n",
      "2020-03-12 19:43:44,858 : INFO : EPOCH 9 - PROGRESS: at 39.54% examples, 1145396 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:43:45,858 : INFO : EPOCH 9 - PROGRESS: at 79.80% examples, 1158545 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:43:46,358 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:43:46,360 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:43:46,361 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:43:46,361 : INFO : EPOCH - 9 : training on 3681650 raw words (2916600 effective words) took 2.5s, 1161181 effective words/s\n",
      "2020-03-12 19:43:47,375 : INFO : EPOCH 10 - PROGRESS: at 45.32% examples, 1312503 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:43:48,376 : INFO : EPOCH 10 - PROGRESS: at 85.98% examples, 1246040 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:43:48,717 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:43:48,725 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:43:48,728 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:43:48,729 : INFO : EPOCH - 10 : training on 3681650 raw words (2916785 effective words) took 2.4s, 1234184 effective words/s\n",
      "2020-03-12 19:43:49,745 : INFO : EPOCH 11 - PROGRESS: at 39.83% examples, 1148367 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:43:50,760 : INFO : EPOCH 11 - PROGRESS: at 80.57% examples, 1158809 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:43:51,226 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:43:51,234 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:43:51,237 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:43:51,238 : INFO : EPOCH - 11 : training on 3681650 raw words (2916761 effective words) took 2.5s, 1164165 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 19:43:52,248 : INFO : EPOCH 12 - PROGRESS: at 40.09% examples, 1168144 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:43:53,258 : INFO : EPOCH 12 - PROGRESS: at 80.83% examples, 1171156 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:43:53,714 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:43:53,717 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:43:53,726 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:43:53,727 : INFO : EPOCH - 12 : training on 3681650 raw words (2916083 effective words) took 2.5s, 1175605 effective words/s\n",
      "2020-03-12 19:43:54,739 : INFO : EPOCH 13 - PROGRESS: at 56.60% examples, 1637437 words/s, in_qsize 4, out_qsize 1\n",
      "2020-03-12 19:43:55,752 : INFO : EPOCH 13 - PROGRESS: at 97.34% examples, 1404680 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:43:55,800 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:43:55,807 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:43:55,811 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:43:55,811 : INFO : EPOCH - 13 : training on 3681650 raw words (2916807 effective words) took 2.1s, 1401742 effective words/s\n",
      "2020-03-12 19:43:56,830 : INFO : EPOCH 14 - PROGRESS: at 40.34% examples, 1161582 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:43:57,830 : INFO : EPOCH 14 - PROGRESS: at 94.98% examples, 1374821 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:43:57,907 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:43:57,908 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:43:57,914 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:43:57,915 : INFO : EPOCH - 14 : training on 3681650 raw words (2917365 effective words) took 2.1s, 1389773 effective words/s\n",
      "2020-03-12 19:43:58,920 : INFO : EPOCH 15 - PROGRESS: at 56.60% examples, 1647966 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:43:59,717 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:43:59,719 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:43:59,719 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:43:59,719 : INFO : EPOCH - 15 : training on 3681650 raw words (2916285 effective words) took 1.8s, 1619010 effective words/s\n",
      "2020-03-12 19:43:59,720 : INFO : training on a 55224750 raw words (43749965 effective words) took 35.7s, 1225887 effective words/s\n",
      "2020-03-12 19:43:59,727 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  mathematics\n",
      "[('science', 0.668274998664856), ('mathematical', 0.6588805913925171), ('discipline', 0.6571842432022095), ('majors', 0.6442996263504028), ('teachers', 0.6276435256004333), ('engineering', 0.6237860918045044), ('algebra', 0.6195244193077087), ('teaching', 0.6185547709465027), ('college', 0.6162368059158325), ('literacy', 0.6118821501731873)]\n",
      "Most similar to:  console\n",
      "[('airplane', 0.7388802766799927), ('NSA', 0.6568073034286499), ('imager', 0.6563106179237366), ('SSV1', 0.6425600051879883), ('SOFDI', 0.6360782384872437), ('interferometer', 0.6358640193939209), ('FE', 0.635016918182373), ('Intercontinental', 0.6320165395736694), ('ICPMS', 0.6242125034332275), ('Ratio', 0.6241852045059204)]\n",
      "Most similar to:  spring\n",
      "[('austral', 0.6717689037322998), ('late', 0.6683459281921387), ('1996', 0.6580739617347717), ('winter', 0.6544943451881409), ('fall', 0.6388822793960571), ('Miocene', 0.6140884160995483), ('meridional', 0.6122885942459106), ('continent', 0.6060815453529358), ('rainy', 0.6049214601516724), ('season', 0.5971851944923401)]\n",
      "Most similar to:  technology\n",
      "[('technologies', 0.7358746528625488), ('technological', 0.6218056082725525), ('nanotechnology', 0.5567646026611328), ('innovation', 0.5559595227241516), ('business', 0.5446015000343323), ('workforce', 0.5444252490997314), ('IT', 0.5442866683006287), ('biotechnology', 0.5419629216194153), ('manufacturing', 0.5333948731422424), ('electronics', 0.5297620296478271)]\n",
      "Most similar to:  communication\n",
      "[('communications', 0.7781584858894348), ('wireless', 0.7489259243011475), ('networking', 0.7146443128585815), ('wired', 0.6621236801147461), ('distributed', 0.6523131132125854), ('network', 0.612849235534668), ('neural', 0.6092813611030579), ('mobile', 0.6044656038284302), ('personal', 0.6038646697998047), ('middleware', 0.5979322791099548)]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# test with size (int), window(int), min_count(int), iter(int), sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text, iter=(15))\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sg = 1\n",
    "\n",
    "Changing to skip-gram definitely feels like it helped with regards to 3 of the words here, and their similar words. Console and spring seem to just be an extremely bad fit for this dataset, however for the other 3 words it was quick to decide on very similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 19:00:20,308 : INFO : collecting all words and their counts\n",
      "2020-03-12 19:00:20,309 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 19:00:20,823 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 19:00:20,824 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 19:00:20,899 : INFO : effective_min_count=5 retains 27041 unique words (23% of original 113911, drops 86870)\n",
      "2020-03-12 19:00:20,900 : INFO : effective_min_count=5 leaves 3552589 word corpus (96% of original 3681650, drops 129061)\n",
      "2020-03-12 19:00:20,965 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 19:00:20,967 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 19:00:20,968 : INFO : downsampling leaves estimated 2916612 word corpus (82.1% of prior 3552589)\n",
      "2020-03-12 19:00:21,023 : INFO : estimated required memory for 27041 words and 100 dimensions: 35153300 bytes\n",
      "2020-03-12 19:00:21,024 : INFO : resetting layer weights\n",
      "2020-03-12 19:00:25,068 : INFO : training model with 3 workers on 27041 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 19:00:26,115 : INFO : EPOCH 1 - PROGRESS: at 10.18% examples, 282102 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-12 19:00:27,139 : INFO : EPOCH 1 - PROGRESS: at 19.74% examples, 281265 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:00:28,167 : INFO : EPOCH 1 - PROGRESS: at 29.63% examples, 280220 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:00:29,184 : INFO : EPOCH 1 - PROGRESS: at 39.83% examples, 282892 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:00:30,225 : INFO : EPOCH 1 - PROGRESS: at 50.70% examples, 287489 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:00:31,232 : INFO : EPOCH 1 - PROGRESS: at 61.55% examples, 292048 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:00:32,241 : INFO : EPOCH 1 - PROGRESS: at 71.70% examples, 292168 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:00:33,245 : INFO : EPOCH 1 - PROGRESS: at 81.40% examples, 290413 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:00:34,268 : INFO : EPOCH 1 - PROGRESS: at 91.58% examples, 290208 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:00:35,095 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:00:35,101 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:00:35,120 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:00:35,120 : INFO : EPOCH - 1 : training on 3681650 raw words (2916833 effective words) took 10.0s, 290341 effective words/s\n",
      "2020-03-12 19:00:36,154 : INFO : EPOCH 2 - PROGRESS: at 14.38% examples, 405631 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:00:37,173 : INFO : EPOCH 2 - PROGRESS: at 26.19% examples, 374264 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:00:38,197 : INFO : EPOCH 2 - PROGRESS: at 37.41% examples, 355456 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:00:39,200 : INFO : EPOCH 2 - PROGRESS: at 48.51% examples, 347877 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:00:40,211 : INFO : EPOCH 2 - PROGRESS: at 59.80% examples, 342834 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:00:41,246 : INFO : EPOCH 2 - PROGRESS: at 71.20% examples, 339548 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:00:42,251 : INFO : EPOCH 2 - PROGRESS: at 82.45% examples, 337301 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:00:43,254 : INFO : EPOCH 2 - PROGRESS: at 93.71% examples, 335816 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:00:43,792 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:00:43,821 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:00:43,827 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:00:43,828 : INFO : EPOCH - 2 : training on 3681650 raw words (2916405 effective words) took 8.7s, 335110 effective words/s\n",
      "2020-03-12 19:00:44,875 : INFO : EPOCH 3 - PROGRESS: at 15.35% examples, 429680 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:00:45,888 : INFO : EPOCH 3 - PROGRESS: at 28.26% examples, 402639 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:00:46,901 : INFO : EPOCH 3 - PROGRESS: at 39.54% examples, 376332 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:00:47,909 : INFO : EPOCH 3 - PROGRESS: at 50.42% examples, 361275 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:00:48,932 : INFO : EPOCH 3 - PROGRESS: at 61.55% examples, 352691 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:00:49,937 : INFO : EPOCH 3 - PROGRESS: at 72.79% examples, 348092 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:00:50,977 : INFO : EPOCH 3 - PROGRESS: at 84.42% examples, 344051 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:00:51,982 : INFO : EPOCH 3 - PROGRESS: at 95.49% examples, 341657 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:00:52,329 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:00:52,372 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:00:52,375 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:00:52,375 : INFO : EPOCH - 3 : training on 3681650 raw words (2916844 effective words) took 8.5s, 341395 effective words/s\n",
      "2020-03-12 19:00:53,383 : INFO : EPOCH 4 - PROGRESS: at 12.80% examples, 369348 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:00:54,419 : INFO : EPOCH 4 - PROGRESS: at 24.04% examples, 345719 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:00:55,422 : INFO : EPOCH 4 - PROGRESS: at 35.27% examples, 338396 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:00:56,427 : INFO : EPOCH 4 - PROGRESS: at 46.59% examples, 336913 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:00:57,456 : INFO : EPOCH 4 - PROGRESS: at 57.94% examples, 332951 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:00:58,464 : INFO : EPOCH 4 - PROGRESS: at 69.38% examples, 332660 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:00:59,466 : INFO : EPOCH 4 - PROGRESS: at 80.34% examples, 330536 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:01:00,488 : INFO : EPOCH 4 - PROGRESS: at 91.58% examples, 329106 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:01:01,214 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:01:01,245 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:01:01,246 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:01:01,246 : INFO : EPOCH - 4 : training on 3681650 raw words (2917220 effective words) took 8.9s, 329018 effective words/s\n",
      "2020-03-12 19:01:02,257 : INFO : EPOCH 5 - PROGRESS: at 10.68% examples, 306624 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:01:03,265 : INFO : EPOCH 5 - PROGRESS: at 23.28% examples, 338333 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:01:04,302 : INFO : EPOCH 5 - PROGRESS: at 39.29% examples, 375777 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:01:05,325 : INFO : EPOCH 5 - PROGRESS: at 51.18% examples, 367100 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:01:06,340 : INFO : EPOCH 5 - PROGRESS: at 62.65% examples, 359448 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:01:07,370 : INFO : EPOCH 5 - PROGRESS: at 75.00% examples, 357277 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:01:08,377 : INFO : EPOCH 5 - PROGRESS: at 85.98% examples, 351383 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:01:09,398 : INFO : EPOCH 5 - PROGRESS: at 97.09% examples, 347413 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:01:09,609 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:01:09,626 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:01:09,644 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:01:09,645 : INFO : EPOCH - 5 : training on 3681650 raw words (2916081 effective words) took 8.4s, 347405 effective words/s\n",
      "2020-03-12 19:01:09,646 : INFO : training on a 18408250 raw words (14583383 effective words) took 44.6s, 327157 effective words/s\n",
      "2020-03-12 19:01:09,647 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  mathematics\n",
      "[('algebra', 0.7799115777015686), ('science', 0.7662696838378906), ('majoring', 0.7653417587280273), ('pedagogy', 0.7538089752197266), ('engineering', 0.7537627816200256), ('discipline', 0.752501904964447), ('preservice', 0.7514277696609497), ('humanities', 0.7514068484306335), ('fluency', 0.7431763410568237), ('introductory', 0.7357624173164368)]\n",
      "Most similar to:  console\n",
      "[('SOAs', 0.9342267513275146), ('susceptometer', 0.9162594676017761), ('containers', 0.914884626865387), ('reproducibly', 0.913713812828064), ('thicknesses', 0.912643313407898), ('emitter', 0.9124078154563904), ('micrographs', 0.9103856086730957), ('fluorine', 0.909922182559967), ('sends', 0.9097558259963989), ('chromatographs', 0.9091013073921204)]\n",
      "Most similar to:  spring\n",
      "[('austral', 0.8643998503684998), ('winter', 0.8076503872871399), ('interglacial', 0.7942143082618713), ('season', 0.7941508889198303), ('brief', 0.788597583770752), ('aboard', 0.7859319448471069), ('driest', 0.7847409248352051), ('downwelling', 0.7840861678123474), ('dextral', 0.7804068326950073), ('rainy', 0.7796751260757446)]\n",
      "Most similar to:  technology\n",
      "[('technologies', 0.7525999546051025), ('proffered', 0.7239270210266113), ('marketplace', 0.7194532155990601), ('nanosystems', 0.7189743518829346), ('infuse', 0.7125952243804932), ('nanomanufacturing', 0.7118242979049683), ('CAE', 0.710735559463501), ('proficient', 0.7097944021224976), ('entrepreneurship', 0.709140419960022), ('FFR', 0.7063742280006409)]\n",
      "Most similar to:  communication\n",
      "[('communications', 0.8293595314025879), ('wireless', 0.8002811074256897), ('interconnection', 0.7901073694229126), ('wired', 0.7701526880264282), ('asynchronous', 0.7518959045410156), ('networking', 0.7501370906829834), ('LANs', 0.7455559968948364), ('latencies', 0.7403062582015991), ('interpersonal', 0.7364535331726074), ('interdependencies', 0.7342554330825806)]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# test with size (int), window(int), min_count(int), iter(int), sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text, sg=1)\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### negative = 5, 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 19:01:09,689 : INFO : collecting all words and their counts\n",
      "2020-03-12 19:01:09,690 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 19:01:10,251 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 19:01:10,252 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 19:01:10,325 : INFO : effective_min_count=5 retains 27041 unique words (23% of original 113911, drops 86870)\n",
      "2020-03-12 19:01:10,326 : INFO : effective_min_count=5 leaves 3552589 word corpus (96% of original 3681650, drops 129061)\n",
      "2020-03-12 19:01:10,387 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 19:01:10,389 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 19:01:10,390 : INFO : downsampling leaves estimated 2916612 word corpus (82.1% of prior 3552589)\n",
      "2020-03-12 19:01:10,439 : INFO : estimated required memory for 27041 words and 100 dimensions: 35153300 bytes\n",
      "2020-03-12 19:01:10,440 : INFO : resetting layer weights\n",
      "2020-03-12 19:01:14,351 : INFO : training model with 3 workers on 27041 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 19:01:15,359 : INFO : EPOCH 1 - PROGRESS: at 39.54% examples, 1151023 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:01:16,361 : INFO : EPOCH 1 - PROGRESS: at 98.40% examples, 1430423 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:01:16,379 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:01:16,380 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:01:16,385 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:01:16,385 : INFO : EPOCH - 1 : training on 3681650 raw words (2916300 effective words) took 2.0s, 1436732 effective words/s\n",
      "2020-03-12 19:01:17,389 : INFO : EPOCH 2 - PROGRESS: at 42.13% examples, 1231578 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:01:18,399 : INFO : EPOCH 2 - PROGRESS: at 84.70% examples, 1226464 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:01:18,763 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:01:18,771 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:01:18,776 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:01:18,776 : INFO : EPOCH - 2 : training on 3681650 raw words (2916106 effective words) took 2.4s, 1221596 effective words/s\n",
      "2020-03-12 19:01:19,782 : INFO : EPOCH 3 - PROGRESS: at 47.72% examples, 1388869 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:01:20,782 : INFO : EPOCH 3 - PROGRESS: at 88.39% examples, 1284688 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:01:21,054 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:01:21,055 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:01:21,056 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:01:21,056 : INFO : EPOCH - 3 : training on 3681650 raw words (2917020 effective words) took 2.3s, 1280125 effective words/s\n",
      "2020-03-12 19:01:22,066 : INFO : EPOCH 4 - PROGRESS: at 40.84% examples, 1186512 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:01:23,080 : INFO : EPOCH 4 - PROGRESS: at 82.45% examples, 1190024 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:01:23,492 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:01:23,494 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:01:23,502 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:01:23,503 : INFO : EPOCH - 4 : training on 3681650 raw words (2916204 effective words) took 2.4s, 1193816 effective words/s\n",
      "2020-03-12 19:01:24,507 : INFO : EPOCH 5 - PROGRESS: at 40.84% examples, 1193715 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:01:25,511 : INFO : EPOCH 5 - PROGRESS: at 84.16% examples, 1222195 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:01:25,904 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:01:25,904 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:01:25,906 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:01:25,906 : INFO : EPOCH - 5 : training on 3681650 raw words (2916949 effective words) took 2.4s, 1215637 effective words/s\n",
      "2020-03-12 19:01:25,906 : INFO : training on a 18408250 raw words (14582579 effective words) took 11.6s, 1262013 effective words/s\n",
      "2020-03-12 19:01:25,914 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  mathematics\n",
      "[('science', 0.8211383819580078), ('engineering', 0.7983667850494385), ('discipline', 0.7597153186798096), ('sciences', 0.7477452754974365), ('humanities', 0.7420483827590942), ('profession', 0.7386701107025146), ('physics', 0.7385774254798889), ('majors', 0.7269306182861328), ('practice', 0.7155213356018066), ('mathematical', 0.7086160182952881)]\n",
      "Most similar to:  console\n",
      "[('chirped', 0.8900636434555054), ('spotting', 0.8849547505378723), ('inkjet', 0.8816371560096741), ('copolymerization', 0.8773975968360901), ('cabinetry', 0.8771728277206421), ('778404024', 0.8760333061218262), ('12CO2', 0.873612642288208), ('Oil', 0.8718723058700562), ('ACTION', 0.8714569211006165), ('carcinogen', 0.8702090978622437)]\n",
      "Most similar to:  spring\n",
      "[('north', 0.7399226427078247), ('1996', 0.7375333309173584), ('northern', 0.7340042591094971), ('late', 0.7328453660011292), ('Ma', 0.7225845456123352), ('Peru', 0.7179943323135376), ('13', 0.7164722681045532), ('west', 0.7158764600753784), ('eastern', 0.7135443091392517), ('Malawi', 0.703656017780304)]\n",
      "Most similar to:  technology\n",
      "[('technologies', 0.7841503024101257), ('innovation', 0.7041760087013245), ('manufacturing', 0.7036295533180237), ('emerging', 0.7030024528503418), ('technological', 0.7027777433395386), ('vision', 0.6971770524978638), ('nanotechnology', 0.6960403919219971), ('nanoscience', 0.6945284605026245), ('biotechnology', 0.6936560273170471), ('standards', 0.672986626625061)]\n",
      "Most similar to:  communication\n",
      "[('wireless', 0.8483846187591553), ('security', 0.8482265472412109), ('networking', 0.8425233364105225), ('communications', 0.8371071815490723), ('computing', 0.8205633163452148), ('network', 0.8100396394729614), ('transportation', 0.8075799345970154), ('distributed', 0.8001708984375), ('networks', 0.7953300476074219), ('virtual', 0.7856655120849609)]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# test with size (int), window(int), min_count(int), iter(int), sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text, negative=5)\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 19:12:17,238 : INFO : collecting all words and their counts\n",
      "2020-03-12 19:12:17,242 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 19:12:17,744 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 19:12:17,746 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 19:12:17,822 : INFO : effective_min_count=5 retains 27041 unique words (23% of original 113911, drops 86870)\n",
      "2020-03-12 19:12:17,823 : INFO : effective_min_count=5 leaves 3552589 word corpus (96% of original 3681650, drops 129061)\n",
      "2020-03-12 19:12:17,889 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 19:12:17,891 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 19:12:17,892 : INFO : downsampling leaves estimated 2916612 word corpus (82.1% of prior 3552589)\n",
      "2020-03-12 19:12:17,943 : INFO : estimated required memory for 27041 words and 100 dimensions: 35153300 bytes\n",
      "2020-03-12 19:12:17,944 : INFO : resetting layer weights\n",
      "2020-03-12 19:12:21,877 : INFO : training model with 3 workers on 27041 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=20 window=5\n",
      "2020-03-12 19:12:22,911 : INFO : EPOCH 1 - PROGRESS: at 16.41% examples, 466346 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:12:23,928 : INFO : EPOCH 1 - PROGRESS: at 33.40% examples, 477014 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:12:24,930 : INFO : EPOCH 1 - PROGRESS: at 50.12% examples, 480528 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:12:25,935 : INFO : EPOCH 1 - PROGRESS: at 67.32% examples, 484142 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:12:26,968 : INFO : EPOCH 1 - PROGRESS: at 84.70% examples, 484777 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:12:27,851 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:12:27,856 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:12:27,872 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:12:27,872 : INFO : EPOCH - 1 : training on 3681650 raw words (2915988 effective words) took 6.0s, 486834 effective words/s\n",
      "2020-03-12 19:12:28,915 : INFO : EPOCH 2 - PROGRESS: at 16.16% examples, 456219 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:12:29,918 : INFO : EPOCH 2 - PROGRESS: at 32.04% examples, 459988 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:12:30,924 : INFO : EPOCH 2 - PROGRESS: at 48.00% examples, 460907 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:12:31,953 : INFO : EPOCH 2 - PROGRESS: at 65.33% examples, 468456 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:12:32,962 : INFO : EPOCH 2 - PROGRESS: at 82.51% examples, 473127 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:12:33,977 : INFO : EPOCH 2 - PROGRESS: at 99.41% examples, 475683 words/s, in_qsize 3, out_qsize 0\n",
      "2020-03-12 19:12:33,985 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:12:33,986 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:12:33,995 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:12:33,995 : INFO : EPOCH - 2 : training on 3681650 raw words (2916388 effective words) took 6.1s, 477018 effective words/s\n",
      "2020-03-12 19:12:35,009 : INFO : EPOCH 3 - PROGRESS: at 15.61% examples, 451798 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:12:36,046 : INFO : EPOCH 3 - PROGRESS: at 31.52% examples, 450125 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:12:37,060 : INFO : EPOCH 3 - PROGRESS: at 47.12% examples, 450407 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:12:38,066 : INFO : EPOCH 3 - PROGRESS: at 69.14% examples, 495818 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:12:39,068 : INFO : EPOCH 3 - PROGRESS: at 85.98% examples, 494078 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:12:39,872 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:12:39,882 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:12:39,888 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:12:39,889 : INFO : EPOCH - 3 : training on 3681650 raw words (2916145 effective words) took 5.9s, 495201 effective words/s\n",
      "2020-03-12 19:12:40,895 : INFO : EPOCH 4 - PROGRESS: at 16.41% examples, 478796 words/s, in_qsize 5, out_qsize 1\n",
      "2020-03-12 19:12:41,915 : INFO : EPOCH 4 - PROGRESS: at 32.86% examples, 474781 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:12:42,918 : INFO : EPOCH 4 - PROGRESS: at 48.79% examples, 471285 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:12:43,942 : INFO : EPOCH 4 - PROGRESS: at 65.33% examples, 470949 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:12:44,950 : INFO : EPOCH 4 - PROGRESS: at 82.22% examples, 473810 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:12:45,952 : INFO : EPOCH 4 - PROGRESS: at 98.40% examples, 473361 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:12:46,012 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:12:46,044 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:12:46,052 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:12:46,052 : INFO : EPOCH - 4 : training on 3681650 raw words (2916047 effective words) took 6.2s, 473398 effective words/s\n",
      "2020-03-12 19:12:47,060 : INFO : EPOCH 5 - PROGRESS: at 16.42% examples, 477746 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:12:48,079 : INFO : EPOCH 5 - PROGRESS: at 33.40% examples, 482445 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:12:49,083 : INFO : EPOCH 5 - PROGRESS: at 50.42% examples, 486485 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:12:50,087 : INFO : EPOCH 5 - PROGRESS: at 66.46% examples, 480911 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:12:51,097 : INFO : EPOCH 5 - PROGRESS: at 82.45% examples, 476901 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:12:52,107 : INFO : EPOCH 5 - PROGRESS: at 98.40% examples, 474119 words/s, in_qsize 5, out_qsize 1\n",
      "2020-03-12 19:12:52,156 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:12:52,162 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:12:52,185 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:12:52,186 : INFO : EPOCH - 5 : training on 3681650 raw words (2916519 effective words) took 6.1s, 475821 effective words/s\n",
      "2020-03-12 19:12:52,186 : INFO : training on a 18408250 raw words (14581087 effective words) took 30.3s, 481091 effective words/s\n",
      "2020-03-12 19:12:52,194 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  mathematics\n",
      "[('discipline', 0.749215841293335), ('engineering', 0.7461268305778503), ('majors', 0.7420068383216858), ('physics', 0.7044917941093445), ('literacy', 0.6914047002792358), ('concepts', 0.6779178380966187), ('elementary', 0.6689209938049316), ('science', 0.6668782234191895), ('careers', 0.6648756265640259), ('teachers', 0.65972900390625)]\n",
      "Most similar to:  console\n",
      "[('chirped', 0.9434792399406433), ('LHP', 0.9293609857559204), ('Talking', 0.92826247215271), ('upwind', 0.9241377115249634), ('compass', 0.9227144122123718), ('SEC', 0.9227045774459839), ('wax', 0.9225384593009949), ('travels', 0.9219479560852051), ('compose', 0.9209423661231995), ('BICEP', 0.9177446365356445)]\n",
      "Most similar to:  spring\n",
      "[('late', 0.7833123207092285), ('winter', 0.781053900718689), ('Miocene', 0.7645460963249207), ('Tertiary', 0.7595193386077881), ('downwelling', 0.7564253807067871), ('northwest', 0.7498323321342468), ('rainy', 0.7495518922805786), ('monsoon', 0.7434316873550415), ('interglacial', 0.7413902282714844), ('inception', 0.7403826713562012)]\n",
      "Most similar to:  technology\n",
      "[('technologies', 0.7545039653778076), ('innovation', 0.6789952516555786), ('technological', 0.6739668250083923), ('manufacturing', 0.659937858581543), ('standards', 0.6512631177902222), ('nanotechnology', 0.6425842046737671), ('electronics', 0.6275079250335693), ('industry', 0.6228711605072021), ('communications', 0.6193678379058838), ('capability', 0.6172932982444763)]\n",
      "Most similar to:  communication\n",
      "[('networking', 0.8485509157180786), ('communications', 0.8269292116165161), ('wireless', 0.8262232542037964), ('security', 0.7879248857498169), ('transportation', 0.7866592407226562), ('computing', 0.7616659998893738), ('distributed', 0.7460471391677856), ('networks', 0.7456187009811401), ('network', 0.7452676296234131), ('mobile', 0.7429344654083252)]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# test with size (int), window(int), min_count(int), iter(int), sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text, negative=20)\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window = 3, 10\n",
    "\n",
    "On its own, windows doesn't seem more interesting than the others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 19:59:38,814 : INFO : collecting all words and their counts\n",
      "2020-03-12 19:59:38,816 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 19:59:39,325 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 19:59:39,326 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 19:59:39,754 : INFO : effective_min_count=5 retains 27041 unique words (23% of original 113911, drops 86870)\n",
      "2020-03-12 19:59:39,755 : INFO : effective_min_count=5 leaves 3552589 word corpus (96% of original 3681650, drops 129061)\n",
      "2020-03-12 19:59:39,819 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 19:59:39,822 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 19:59:39,822 : INFO : downsampling leaves estimated 2916612 word corpus (82.1% of prior 3552589)\n",
      "2020-03-12 19:59:39,872 : INFO : estimated required memory for 27041 words and 100 dimensions: 35153300 bytes\n",
      "2020-03-12 19:59:39,873 : INFO : resetting layer weights\n",
      "2020-03-12 19:59:43,764 : INFO : training model with 3 workers on 27041 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
      "2020-03-12 19:59:44,783 : INFO : EPOCH 1 - PROGRESS: at 42.92% examples, 1242611 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:59:45,786 : INFO : EPOCH 1 - PROGRESS: at 86.26% examples, 1247898 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:59:46,089 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:59:46,093 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:59:46,100 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:59:46,101 : INFO : EPOCH - 1 : training on 3681650 raw words (2915703 effective words) took 2.3s, 1252831 effective words/s\n",
      "2020-03-12 19:59:47,113 : INFO : EPOCH 2 - PROGRESS: at 39.83% examples, 1153477 words/s, in_qsize 5, out_qsize 1\n",
      "2020-03-12 19:59:48,122 : INFO : EPOCH 2 - PROGRESS: at 80.83% examples, 1168171 words/s, in_qsize 5, out_qsize 1\n",
      "2020-03-12 19:59:48,546 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:59:48,547 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:59:48,550 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:59:48,551 : INFO : EPOCH - 2 : training on 3681650 raw words (2916536 effective words) took 2.4s, 1192403 effective words/s\n",
      "2020-03-12 19:59:49,557 : INFO : EPOCH 3 - PROGRESS: at 42.66% examples, 1245234 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:59:50,561 : INFO : EPOCH 3 - PROGRESS: at 86.80% examples, 1260246 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 19:59:50,850 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:59:50,851 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:59:50,853 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:59:50,854 : INFO : EPOCH - 3 : training on 3681650 raw words (2916721 effective words) took 2.3s, 1269127 effective words/s\n",
      "2020-03-12 19:59:51,864 : INFO : EPOCH 4 - PROGRESS: at 62.37% examples, 1810451 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:59:52,711 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:59:52,712 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:59:52,714 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:59:52,715 : INFO : EPOCH - 4 : training on 3681650 raw words (2916289 effective words) took 1.9s, 1570105 effective words/s\n",
      "2020-03-12 19:59:53,724 : INFO : EPOCH 5 - PROGRESS: at 50.70% examples, 1477380 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 19:59:54,740 : INFO : EPOCH 5 - PROGRESS: at 94.73% examples, 1368249 words/s, in_qsize 6, out_qsize 2\n",
      "2020-03-12 19:59:54,841 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 19:59:54,842 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 19:59:54,844 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 19:59:54,844 : INFO : EPOCH - 5 : training on 3681650 raw words (2917234 effective words) took 2.1s, 1374111 effective words/s\n",
      "2020-03-12 19:59:54,845 : INFO : training on a 18408250 raw words (14582483 effective words) took 11.1s, 1316129 effective words/s\n",
      "2020-03-12 19:59:54,852 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  mathematics\n",
      "[('engineering', 0.8175453543663025), ('discipline', 0.8121922016143799), ('science', 0.7867183089256287), ('statistics', 0.7689366340637207), ('literacy', 0.748267650604248), ('physics', 0.745501697063446), ('nanotechnology', 0.744453489780426), ('practice', 0.7390854954719543), ('concepts', 0.7390586137771606), ('bioinformatics', 0.7379647493362427)]\n",
      "Most similar to:  console\n",
      "[('Tribolium', 0.9003188014030457), ('ns1', 0.8933048844337463), ('isomerase', 0.8924181461334229), ('stepping', 0.8923943042755127), ('tethering', 0.8914327621459961), ('tendencies', 0.8896902799606323), ('neon', 0.8886416554450989), ('dynein', 0.888285756111145), ('CF', 0.8881625533103943), ('Junctions', 0.8881528973579407)]\n",
      "Most similar to:  spring\n",
      "[('Spain', 0.8236149549484253), ('late', 0.7788383960723877), ('Seville', 0.7751665711402893), ('winter', 0.7727768421173096), ('1995', 0.7695266008377075), ('austral', 0.7557742595672607), ('Classic', 0.7542219161987305), ('civilization', 0.7468624114990234), ('24', 0.7466127872467041), ('1992', 0.7455421090126038)]\n",
      "Most similar to:  technology\n",
      "[('technologies', 0.7614506483078003), ('standards', 0.7095615863800049), ('biotechnology', 0.6999701261520386), ('vision', 0.69870924949646), ('company', 0.6907907128334045), ('curriculum', 0.6807369589805603), ('nanotechnology', 0.6750994920730591), ('infrastructure', 0.6588006615638733), ('emerging', 0.6585968732833862), ('industry', 0.6556260585784912)]\n",
      "Most similar to:  communication\n",
      "[('networking', 0.8474169969558716), ('transportation', 0.8268929123878479), ('security', 0.8222907185554504), ('communications', 0.8214027881622314), ('wireless', 0.8105034232139587), ('virtual', 0.799727201461792), ('intelligence', 0.7933773994445801), ('manufacturing', 0.7884081602096558), ('management', 0.7872737050056458), ('computing', 0.7858070135116577)]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# test with size (int), window(int), min_count(int), iter(int), sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text, window=3)\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 19:59:54,873 : INFO : collecting all words and their counts\n",
      "2020-03-12 19:59:54,874 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 19:59:55,401 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 19:59:55,402 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 19:59:55,476 : INFO : effective_min_count=5 retains 27041 unique words (23% of original 113911, drops 86870)\n",
      "2020-03-12 19:59:55,476 : INFO : effective_min_count=5 leaves 3552589 word corpus (96% of original 3681650, drops 129061)\n",
      "2020-03-12 19:59:55,538 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 19:59:55,541 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 19:59:55,541 : INFO : downsampling leaves estimated 2916612 word corpus (82.1% of prior 3552589)\n",
      "2020-03-12 19:59:55,591 : INFO : estimated required memory for 27041 words and 100 dimensions: 35153300 bytes\n",
      "2020-03-12 19:59:55,591 : INFO : resetting layer weights\n",
      "2020-03-12 19:59:59,529 : INFO : training model with 3 workers on 27041 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2020-03-12 20:00:00,535 : INFO : EPOCH 1 - PROGRESS: at 35.54% examples, 1036017 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 20:00:01,536 : INFO : EPOCH 1 - PROGRESS: at 75.00% examples, 1091746 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 20:00:02,226 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 20:00:02,236 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 20:00:02,240 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 20:00:02,241 : INFO : EPOCH - 1 : training on 3681650 raw words (2916701 effective words) took 2.7s, 1077233 effective words/s\n",
      "2020-03-12 20:00:03,252 : INFO : EPOCH 2 - PROGRESS: at 43.19% examples, 1255137 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 20:00:04,260 : INFO : EPOCH 2 - PROGRESS: at 80.34% examples, 1162666 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 20:00:04,785 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 20:00:04,786 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 20:00:04,787 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 20:00:04,788 : INFO : EPOCH - 2 : training on 3681650 raw words (2916698 effective words) took 2.5s, 1147233 effective words/s\n",
      "2020-03-12 20:00:05,794 : INFO : EPOCH 3 - PROGRESS: at 36.32% examples, 1057154 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 20:00:06,796 : INFO : EPOCH 3 - PROGRESS: at 72.79% examples, 1060081 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 20:00:07,408 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 20:00:07,409 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 20:00:07,415 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 20:00:07,416 : INFO : EPOCH - 3 : training on 3681650 raw words (2916486 effective words) took 2.6s, 1111260 effective words/s\n",
      "2020-03-12 20:00:08,423 : INFO : EPOCH 4 - PROGRESS: at 50.12% examples, 1456701 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 20:00:09,436 : INFO : EPOCH 4 - PROGRESS: at 94.46% examples, 1363786 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 20:00:09,569 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 20:00:09,570 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 20:00:09,580 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 20:00:09,580 : INFO : EPOCH - 4 : training on 3681650 raw words (2916252 effective words) took 2.2s, 1348458 effective words/s\n",
      "2020-03-12 20:00:10,587 : INFO : EPOCH 5 - PROGRESS: at 41.88% examples, 1219966 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 20:00:11,595 : INFO : EPOCH 5 - PROGRESS: at 79.80% examples, 1156608 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 20:00:12,125 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 20:00:12,128 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 20:00:12,137 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 20:00:12,138 : INFO : EPOCH - 5 : training on 3681650 raw words (2916406 effective words) took 2.6s, 1142132 effective words/s\n",
      "2020-03-12 20:00:12,138 : INFO : training on a 18408250 raw words (14582543 effective words) took 12.6s, 1156574 effective words/s\n",
      "2020-03-12 20:00:12,145 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  mathematics\n",
      "[('science', 0.8216568231582642), ('engineering', 0.7666447162628174), ('majors', 0.7465415596961975), ('physics', 0.7336078882217407), ('elementary', 0.7253873348236084), ('discipline', 0.7172554731369019), ('sciences', 0.7157300114631653), ('mathematical', 0.709143877029419), ('teachers', 0.7050789594650269), ('mathematicians', 0.7021793127059937)]\n",
      "Most similar to:  console\n",
      "[('optimizes', 0.8795520067214966), ('chirped', 0.8729230761528015), ('Patten', 0.8725773692131042), ('Andreev', 0.8694102168083191), ('02140', 0.8669188022613525), ('Reforming', 0.8665715456008911), ('ns1', 0.8617780208587646), ('bombardment', 0.8598340749740601), ('295', 0.859593391418457), ('Double', 0.8564720153808594)]\n",
      "Most similar to:  spring\n",
      "[('winter', 0.7506990432739258), ('late', 0.7324632406234741), ('1996', 0.710605263710022), ('season', 0.6982637047767639), ('period', 0.6915218830108643), ('Southeast', 0.690123438835144), ('cruises', 0.6875318884849548), ('southwestern', 0.6759716272354126), ('meridional', 0.6755629777908325), ('60', 0.6699525713920593)]\n",
      "Most similar to:  technology\n",
      "[('technologies', 0.8090913891792297), ('manufacturing', 0.7507535219192505), ('emerging', 0.7201215624809265), ('nanotechnology', 0.7132869958877563), ('innovations', 0.7049827575683594), ('innovation', 0.6995257139205933), ('nanoscience', 0.6971590518951416), ('telecommunications', 0.6957111954689026), ('industries', 0.6940517425537109), ('industry', 0.6908086538314819)]\n",
      "Most similar to:  communication\n",
      "[('wireless', 0.8739237785339355), ('networks', 0.850298285484314), ('networking', 0.8495720624923706), ('communications', 0.8357512950897217), ('security', 0.8353345394134521), ('virtual', 0.8216336965560913), ('intelligence', 0.8171989917755127), ('mobile', 0.8155486583709717), ('architectural', 0.8119405508041382), ('operations', 0.8109437227249146)]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# test with size (int), window(int), min_count(int), iter(int), sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text, window=10)\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results for 2a.\n",
    "\n",
    "First off; spring was apparently a terrible word to choose since there is nothing that seems to be close to it, winter does pop up occasionally but for all settings it feels kind of random what it picks to be similar.\n",
    "\n",
    "For me the most interesting variables is the skipgram. Incresing iterations in this scenario also felt valuable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing 2002 vs full abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "dir_path = \"awards_2002/\"\n",
    "root_dir = os.fsencode(dir_path)\n",
    "for directory in os.listdir(root_dir):\n",
    "    sub_directory = os.fsdecode(directory)\n",
    "    current_path = dir_path + sub_directory + \"/\"\n",
    "    \n",
    "    for file in os.listdir(dir_path + sub_directory):\n",
    "        with open(current_path + file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            documents.append(f.read())\n",
    "            \n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def extract_abstracts()\n",
    "with zipfile.ZipFile(\"abstracts.zip\", \"r\") as file:\n",
    "    file.extractall()\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.15\n",
      "  Using cached https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (1.11.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (1.12.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (0.8.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (3.0.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (0.33.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (1.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (1.1.0)\n",
      "Collecting gast==0.2.2 (from tensorflow==1.15)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (1.0.8)\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (0.8.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (1.17.2)\n",
      "Collecting tensorboard<1.16.0,>=1.15.0 (from tensorflow==1.15)\n",
      "  Using cached https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (3.9.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (1.23.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (0.1.7)\n",
      "Collecting tensorflow-estimator==1.15.1 (from tensorflow==1.15)\n",
      "  Using cached https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (41.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (0.15.6)\n",
      "Installing collected packages: gast, tensorboard, tensorflow-estimator, tensorflow\n",
      "  Found existing installation: gast 0.3.1\n",
      "    Uninstalling gast-0.3.1:\n",
      "\u001b[31mERROR: Could not install packages due to an EnvironmentError: [Errno 13] Permission denied: 'INSTALLER'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tensorflow_hub>=0.6.0 in /opt/conda/lib/python3.7/site-packages (0.7.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_hub>=0.6.0) (1.12.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_hub>=0.6.0) (3.9.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_hub>=0.6.0) (1.17.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.4.0->tensorflow_hub>=0.6.0) (41.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting tensorflow_text==1.15\n",
      "  Using cached https://files.pythonhosted.org/packages/a2/93/cfa6d4532d9cb7707028d7d4fd505fa7aab9d6e08275322e516bf6de509d/tensorflow_text-1.15.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting tensorflow<1.16,>=1.15.0 (from tensorflow_text==1.15)\n",
      "  Using cached https://files.pythonhosted.org/packages/5b/81/84fb7a323f9723f81edfc796d89e89aa95a9446ed7353c144195b3a3a3ba/tensorflow-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (1.0.8)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (1.12.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (1.11.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (1.17.2)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (3.9.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (0.1.7)\n",
      "Collecting tensorboard<1.16.0,>=1.15.0 (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15)\n",
      "  Using cached https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (0.33.4)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (3.0.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (0.8.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (1.1.0)\n",
      "Collecting tensorflow-estimator==1.15.1 (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15)\n",
      "  Using cached https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (1.23.0)\n",
      "Collecting gast==0.2.2 (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (0.8.0)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (2.10.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (41.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (0.15.6)\n",
      "Installing collected packages: tensorboard, tensorflow-estimator, gast, tensorflow, tensorflow-text\n",
      "\u001b[31mERROR: Could not install packages due to an EnvironmentError: [Errno 13] Permission denied: '/opt/conda/lib/python3.7/site-packages/tensorboard/__init__.py'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow==1.15\n",
    "#!pip install \"tensorflow_hub>=0.6.0\"\n",
    "#!pip3 install tensorflow_text==1.15\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "elmo = hub.KerasLayer(\"https://tfhub.dev/google/elmo/3\",signature=\"default\", as_dict=True trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elmo_vectors(sents):\n",
    "    embeddings = elmo(sents, )[\"elmo\"]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        return sess.run(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elmo_vectors(sents):\n",
    "    embeddings = elmo(sents, signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        return sess.run(embeddings)\n",
    "        #sess.run(tf.tables_initializer())\n",
    "        # return average of ELMo features as sentence vector\n",
    "        #return sess.run(tf.reduce_mean(embeddings,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "call() got an unexpected keyword argument 'signature'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-38e9191b0608>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"game\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0melmo_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melmo_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mword_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-20f49a7e45d4>\u001b[0m in \u001b[0;36melmo_vectors\u001b[0;34m(sents)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0melmo_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melmo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"default\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"elmo\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    850\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 851\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: call() got an unexpected keyword argument 'signature'"
     ]
    }
   ],
   "source": [
    "sents = \"\"\"The game ended quickly .\n",
    "He hunted some game for dinner .\n",
    "A game of swans in the river .*\n",
    "They played a game of chess .\n",
    "They were in a baseball game .\n",
    "She decided to eat som game .\n",
    "Game can be found in forests .\n",
    "Counterstrike is a popular game .\n",
    "They didn't follow the game .\n",
    "It was time to game .\"\"\".split('\\n')\n",
    "\n",
    "target = \"game\"\n",
    "\n",
    "elmo_vecs = elmo_vectors(sents)\n",
    "word_vecs = []\n",
    "for i, sent in enumerate(sents):\n",
    "    word_vecs.append(elmo_vecs[i][sent.split().index(target)])\n",
    "    print(\"Sentence: \", sent)\n",
    "    print(\"Vector for '%s:'\" % target, word_vecs[-1])\n",
    "    print()\n",
    "    \n",
    "print(\"Word vec size\", word_vecs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_vecs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-a72668cf7ad3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvec_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Similarities between '%s' vector in sentences:\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_vecs' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vec_size = word_vecs[0].shape[0]\n",
    "print(\"Similarities between '%s' vector in sentences:\" % target)\n",
    "for i in range(1, len(sents)):\n",
    "    print(\"Sent 0-%d:\" % i, cosine_similarity(word_vecs[0].reshape((1,vec_size)), \n",
    "                                              word_vecs[i].reshape((1,vec_size)))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

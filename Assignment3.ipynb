{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import heapq, numpy as np\n",
    "import random\n",
    "#!pip3 install gensim\n",
    "from gensim import corpora, models\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "dir_path = \"awards_2002/\"\n",
    "root_dir = os.fsencode(dir_path)\n",
    "for directory in os.listdir(root_dir):\n",
    "    sub_directory = os.fsdecode(directory)\n",
    "    current_path = dir_path + sub_directory + \"/\"\n",
    "    \n",
    "    for file in os.listdir(dir_path + sub_directory):\n",
    "        with open(current_path + file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            documents.append(f.read())\n",
    "            \n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_names(vectorizer, matrix):\n",
    "    features = tfidf_vectorizer.get_feature_names()\n",
    "    for doc_i in range(5):\n",
    "        print(\"\\nDocument %d, top terms by TF-IDF\" % doc_i)\n",
    "        for term, score in sorted(list(zip(features,matrix.toarray()[doc_i])), key=lambda x:-x[1])[:5]:\n",
    "            print(\"%.2f\\t%s\" % (score, term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clusters(matrix, clusters, n_keywords=10):\n",
    "    max_cluster = 10\n",
    "    for cluster in range(min(clusters), max_cluster):\n",
    "        cluster_docs = [i for i, c in enumerate(clusters) if c == cluster]\n",
    "        print(\"Cluster: %d (%d docs)\" % (cluster, len(cluster_docs)))\n",
    "        \n",
    "        # Keep scores for top n terms\n",
    "        new_matrix = np.zeros((len(cluster_docs), matrix.shape[1]))\n",
    "        for cluster_i, doc_vec in enumerate(matrix[cluster_docs].toarray()):\n",
    "            for idx, score in heapq.nlargest(n_keywords, enumerate(doc_vec), key=lambda x:x[1]):\n",
    "                new_matrix[cluster_i][idx] = score\n",
    "\n",
    "        # Aggregate scores for kept top terms\n",
    "        keywords = heapq.nlargest(n_keywords, zip(new_matrix.sum(axis=0), features))\n",
    "        print(', '.join([w for s,w in keywords]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a Experiment with KMeans and hierarchial clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=30, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=3, use_idf=True, sublinear_tf=True, max_df=0.1, max_features=100000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "matrix_sample = tfidf_matrix[:1000]\n",
    "km = KMeans(n_clusters=30, random_state=42, verbose=0)\n",
    "km.fit(matrix_sample)\n",
    "#print_clusters(matrix_sample, km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=2, use_idf=True,max_df=0.1, max_features=100000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "matrix_sample = tfidf_matrix[:1000]\n",
    "z = linkage(matrix_sample.todense(), metric=\"cosine\", method=\"complete\")\n",
    "clusters = fcluster(z, t=0.99, criterion=\"distance\")\n",
    "#print_clusters(matrix_sample, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a results\n",
    "\n",
    "#### Fcluster\n",
    "\n",
    "* With a min_df of 1 a lot of numbers started popping up and multiple clusters with the same terms\n",
    "* Higher min_df doesn't do much more than potentially hide \"high value\" terms\n",
    "* Mostly good terms with a decent setup\n",
    "* Small cluster size (# of docs) - related to the t in fcluster\n",
    "* Method to euclidian instead of complete didn't give much benefit\n",
    "\n",
    "#### KMeans\n",
    "\n",
    "* Large clusters\n",
    "* More numbers in the clusters (Potentially useless, potentially good ie. genes)\n",
    "* Seems dependant on the random_state\n",
    "* Higher than 2 min_df just leads to clusters that are too broad\n",
    "\n",
    "--\n",
    "\n",
    "In my experimentation I feel like the end-result that was best was the most recent hierarchial clustering. For one, none of the clusters had numbers which I atleast saw as a larger negative.\n",
    "\n",
    "That said it has it's pros and cons as well. The clusters are considerably smaller in size compared to the KMeans clusters, where these are about 10 or so docs in size, the KMeans clusters seem to be around 25 or so. This is both good and bad in the sense that a smaller cluster most likely means that it's more specific, but it might also mean that it just made multiple clusters that are very similar.\n",
    "\n",
    "As such I'll go with the fcluster that I have above. It uses\n",
    "\n",
    "\n",
    "linkage(metric=\"cosine\", method=\"complete\")\n",
    "\n",
    "fcluster(t=0.99, criterion=\"distance\")\n",
    "\n",
    "Changing the method only gave very similar or sparse clusters. The t value just made the clusters even smaller, to the point where a doc was basically its own cluster. The min_df and max_df seemed to be pretty optimal at these values, as changing them too much just made clusters too broad or made them have too many \"bad\" terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b label the clusters\n",
    "\n",
    "Copypaste the cluster just in case since i shuffle the docs at the start of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cluster: 1 (7 docs) - **Electrical engineering**\n",
    "\n",
    "multimedia, compiler, smt, hmd, asic, processors, ieee, multiuser, adaptable, fpga\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 2 (8 docs) - **Software verification**\n",
    "\n",
    "hybrid, verification, embedded, software, qos, certification, stanford, rtl, checking, device\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 3 (7 docs) - **Continental drifting / Seafloor geography**\n",
    "\n",
    "continental, rift, rifting, spreading, seafloor, extension, pilcomayo, gulf, deposits, rio\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 4 (9 docs) - **Geography statistics**\n",
    "\n",
    "mantle, antarctic, seismic, gps, geodetic, stations, fault, puget, permanent, recoverable\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 5 (10 docs) - **Seismic activity?**\n",
    "\n",
    "detachment, uplift, floreana, magmatic, tectonic, cordillera, arc, strike, mafic, plateau\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 6 (15 docs) - **Thermodynamics**\n",
    "\n",
    "equations, ergodic, differential, probability, volterra, singularities, hyperbolic, oscillations, boundary, partial\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 7 (4 docs) - **Linear algebra**\n",
    "\n",
    "spaces, operators, teichmueller, functions, operator, metric, hankel, toeplitz, green, holomorphic\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 8 (13 docs) - **Algebraic topology**\n",
    "\n",
    "manifolds, homotopy, dm, geometric, compact, algebras, surfaces, variables, ring, operators\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 9 (4 docs) - **Deforestation & poor countries**\n",
    "\n",
    "migrants, semantic, tenure, real, compositionality, semantics, migration, syntactic, web, deforestation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c pick out 2 good and 2 bad clusters\n",
    "\n",
    "Clusters 7 & 8 are both good in my opinion.\n",
    "\n",
    "7 is a little small in size, however the terms are almost all related and for example the 3 names all correspond to functions related to algebra, and obviously functions are also in the picture.\n",
    "\n",
    "8 is also grouped in a similar way, where the terms can all be related back to topology, where for example homotpoty and manifolds are both main branches of topology.\n",
    "\n",
    "As for bad clusters, from these 10 I'd say it would be cluster 9 and cluster 3. (5 by extension)\n",
    "\n",
    "Cluster 9 is simply too hard to interpret. It has a mix of very different terms that are hard to group together. It could be correlated to the Amazon rainforest and the deforestation there but where do semantics come into the picture there.\n",
    "\n",
    "Cluster 3 in turn isn't that bad, however I feel like its too similar to that of cluster 5. THey're both related to seismic activity, and it's essentially just one being the seafloor, the other being mountains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d LDA modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf2_vectorizer = TfidfVectorizer()\n",
    "word_tokenizer = tfidf2_vectorizer.build_tokenizer()\n",
    "tokenized_text = [word_tokenizer(doc) for doc in documents]\n",
    "\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "lda_corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "lda_model = models.LdaModel(lda_corpus, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "0.0075\tProgram\n",
      "0.0071\tNSF\n",
      "0.0065\tEstimated\n",
      "0.0061\tDate\n",
      "0.0044\t2002\n",
      "0.0043\tUniversity\n",
      "0.0040\t2003\n",
      "0.0039\tcurrent\n",
      "0.0037\tFile\n",
      "0.0036\tPrincipal\n",
      "\n",
      "Topic 1\n",
      "0.0107\tProgram\n",
      "0.0106\t2002\n",
      "0.0105\tNSF\n",
      "0.0098\tEstimated\n",
      "0.0094\tDate\n",
      "0.0087\tcurrent\n",
      "0.0087\tPrincipal\n",
      "0.0055\tTitle\n",
      "0.0054\tExpires\n",
      "0.0053\tPrgm\n",
      "\n",
      "Topic 2\n",
      "0.0053\tNSF\n",
      "0.0048\tcurrent\n",
      "0.0047\tProgram\n",
      "0.0045\tEstimated\n",
      "0.0045\tDate\n",
      "0.0042\tPrincipal\n",
      "0.0038\t2002\n",
      "0.0031\tproject\n",
      "0.0027\thave\n",
      "0.0025\ttheir\n",
      "\n",
      "Topic 3\n",
      "0.0075\tNSF\n",
      "0.0074\tProgram\n",
      "0.0072\tDate\n",
      "0.0068\tEstimated\n",
      "0.0062\tcurrent\n",
      "0.0057\t2002\n",
      "0.0051\tPrincipal\n",
      "0.0039\tFld\n",
      "0.0039\tproject\n",
      "0.0038\tRef\n",
      "\n",
      "Topic 4\n",
      "0.0051\tNSF\n",
      "0.0050\tEstimated\n",
      "0.0047\tDate\n",
      "0.0045\tProgram\n",
      "0.0044\tcurrent\n",
      "0.0036\tPrincipal\n",
      "0.0036\t2002\n",
      "0.0028\tRef\n",
      "0.0027\tdata\n",
      "0.0026\tInstr\n",
      "\n",
      "Topic 5\n",
      "0.0055\tProgram\n",
      "0.0053\tDate\n",
      "0.0051\tNSF\n",
      "0.0051\tEstimated\n",
      "0.0038\t2002\n",
      "0.0036\tPrincipal\n",
      "0.0033\tcurrent\n",
      "0.0029\thave\n",
      "0.0029\tAbstract\n",
      "0.0029\twhich\n",
      "\n",
      "Topic 6\n",
      "0.0046\tNSF\n",
      "0.0046\tDate\n",
      "0.0045\tProgram\n",
      "0.0044\tEstimated\n",
      "0.0039\tcurrent\n",
      "0.0035\tPrincipal\n",
      "0.0032\t2002\n",
      "0.0032\tmaterials\n",
      "0.0030\tsystems\n",
      "0.0030\tproject\n",
      "\n",
      "Topic 7\n",
      "0.0085\tstudents\n",
      "0.0076\tPrincipal\n",
      "0.0076\tcurrent\n",
      "0.0072\tProgram\n",
      "0.0061\tNSF\n",
      "0.0056\tDate\n",
      "0.0055\tEstimated\n",
      "0.0053\tCo\n",
      "0.0051\tscience\n",
      "0.0048\tUniversity\n",
      "\n",
      "Topic 8\n",
      "0.0037\tfault\n",
      "0.0024\tEstimated\n",
      "0.0022\tNSF\n",
      "0.0020\tProgram\n",
      "0.0018\tDate\n",
      "0.0015\tcurrent\n",
      "0.0015\tPrincipal\n",
      "0.0015\t2002\n",
      "0.0012\tmantle\n",
      "0.0012\tFOR\n",
      "\n",
      "Topic 9\n",
      "0.0034\tNSF\n",
      "0.0029\ttraining\n",
      "0.0028\tDate\n",
      "0.0026\tProgram\n",
      "0.0024\t2002\n",
      "0.0024\tPrincipal\n",
      "0.0024\tEstimated\n",
      "0.0023\tcurrent\n",
      "0.0023\tFellowship\n",
      "0.0020\tPostdoctoral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect topics\n",
    "for i, topic in lda_model.show_topics(num_words=50, formatted=False):\n",
    "    print(\"Topic\", i)\n",
    "    printed_terms = 0\n",
    "    for term, score in topic:\n",
    "        if printed_terms >= 10:\n",
    "            break\n",
    "        elif term in \"Award Investigator research this these will that the This of OF and to for in or The is be may an a with at are on by as from can\".split():\n",
    "            continue\n",
    "        printed_terms += 1\n",
    "        print(\"%.4f\\t%s\" % (score,term))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case a lot of the topics seem to be very similar if not almost identical, however because of how LDA is intended to work this does make some sense. Since this modelling is designed so that a document can fall under multiple topics.\n",
    "\n",
    "After removing some stopwords and also removing some terms that occured in every listed topic, you can see that there are some differences between the topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_words = [\"mathematics\", \"console\", \"spring\", \"technology\", \"communication\"]\n",
    "tfidf2_vectorizer = TfidfVectorizer()\n",
    "word_tokenizer = tfidf2_vectorizer.build_tokenizer()\n",
    "tokenized_text = [word_tokenizer(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 18:12:36,748 : INFO : collecting all words and their counts\n",
      "2020-03-12 18:12:36,752 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 18:12:37,405 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 18:12:37,406 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 18:12:37,495 : INFO : effective_min_count=5 retains 27041 unique words (23% of original 113911, drops 86870)\n",
      "2020-03-12 18:12:37,496 : INFO : effective_min_count=5 leaves 3552589 word corpus (96% of original 3681650, drops 129061)\n",
      "2020-03-12 18:12:37,566 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 18:12:37,569 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 18:12:37,570 : INFO : downsampling leaves estimated 2916612 word corpus (82.1% of prior 3552589)\n",
      "2020-03-12 18:12:37,634 : INFO : estimated required memory for 27041 words and 100 dimensions: 35153300 bytes\n",
      "2020-03-12 18:12:37,634 : INFO : resetting layer weights\n",
      "2020-03-12 18:12:41,790 : INFO : training model with 3 workers on 27041 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 18:12:42,802 : INFO : EPOCH 1 - PROGRESS: at 41.21% examples, 1199999 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:12:43,813 : INFO : EPOCH 1 - PROGRESS: at 80.83% examples, 1170577 words/s, in_qsize 5, out_qsize 1\n",
      "2020-03-12 18:12:44,168 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:12:44,169 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:12:44,176 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:12:44,177 : INFO : EPOCH - 1 : training on 3681650 raw words (2916703 effective words) took 2.4s, 1227447 effective words/s\n",
      "2020-03-12 18:12:45,184 : INFO : EPOCH 2 - PROGRESS: at 44.13% examples, 1282293 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:12:46,192 : INFO : EPOCH 2 - PROGRESS: at 86.90% examples, 1259237 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 18:12:46,512 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:12:46,516 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:12:46,521 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:12:46,522 : INFO : EPOCH - 2 : training on 3681650 raw words (2916335 effective words) took 2.3s, 1245774 effective words/s\n",
      "2020-03-12 18:12:47,544 : INFO : EPOCH 3 - PROGRESS: at 37.69% examples, 1088433 words/s, in_qsize 4, out_qsize 1\n",
      "2020-03-12 18:12:48,547 : INFO : EPOCH 3 - PROGRESS: at 84.80% examples, 1227043 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:12:48,903 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:12:48,906 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:12:48,913 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:12:48,914 : INFO : EPOCH - 3 : training on 3681650 raw words (2916373 effective words) took 2.4s, 1224682 effective words/s\n",
      "2020-03-12 18:12:49,918 : INFO : EPOCH 4 - PROGRESS: at 39.31% examples, 1146637 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:12:50,920 : INFO : EPOCH 4 - PROGRESS: at 77.53% examples, 1130385 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:12:51,502 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:12:51,504 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:12:51,506 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:12:51,507 : INFO : EPOCH - 4 : training on 3681650 raw words (2916786 effective words) took 2.6s, 1126637 effective words/s\n",
      "2020-03-12 18:12:52,523 : INFO : EPOCH 5 - PROGRESS: at 44.13% examples, 1271549 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:12:53,529 : INFO : EPOCH 5 - PROGRESS: at 81.94% examples, 1182759 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 18:12:53,998 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 18:12:54,005 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 18:12:54,006 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 18:12:54,007 : INFO : EPOCH - 5 : training on 3681650 raw words (2916284 effective words) took 2.5s, 1168579 effective words/s\n",
      "2020-03-12 18:12:54,007 : INFO : training on a 18408250 raw words (14582481 effective words) took 12.2s, 1193704 effective words/s\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'most_similar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ed9634680c37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# test with size, window, min_count, iter, sg, negative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors_Default\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Most similar to: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'most_similar' is not defined"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# test with size, window, min_count, iter, sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text)\n",
    "most_similar(vectors_Default, seed_words)\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# test with size (int), window(int), min_count(int), iter(int), sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text, si)\n",
    "most_similar(vectors_Default, seed_words)\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_hub in /opt/conda/lib/python3.7/site-packages (0.7.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_hub) (1.12.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_hub) (3.9.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_hub) (1.17.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.4.0->tensorflow_hub) (41.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (2.0.0rc0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.9.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.1.7)\n",
      "Requirement already satisfied: gast>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.3.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.23.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.0.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019080602,>=1.14.0.dev2019080601 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.14.0.dev2019080601)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.8.0)\n",
      "Requirement already satisfied: tb-nightly<1.15.0a20190807,>=1.15.0a20190806 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.15.0a20190806)\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.8.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.33.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow) (41.2.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow) (0.15.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow) (2.10.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.15\n",
      "  Using cached https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (1.11.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (1.12.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (0.8.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (3.0.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (0.33.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (1.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (1.1.0)\n",
      "Collecting gast==0.2.2 (from tensorflow==1.15)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (1.0.8)\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (0.8.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (1.17.2)\n",
      "Collecting tensorboard<1.16.0,>=1.15.0 (from tensorflow==1.15)\n",
      "  Using cached https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (3.9.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (1.23.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.15) (0.1.7)\n",
      "Collecting tensorflow-estimator==1.15.1 (from tensorflow==1.15)\n",
      "  Using cached https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (41.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (0.15.6)\n",
      "Installing collected packages: gast, tensorboard, tensorflow-estimator, tensorflow\n",
      "  Found existing installation: gast 0.3.1\n",
      "    Uninstalling gast-0.3.1:\n",
      "\u001b[31mERROR: Could not install packages due to an EnvironmentError: [Errno 13] Permission denied: 'INSTALLER'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tensorflow_hub>=0.6.0 in /opt/conda/lib/python3.7/site-packages (0.7.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_hub>=0.6.0) (1.12.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_hub>=0.6.0) (3.9.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_hub>=0.6.0) (1.17.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.4.0->tensorflow_hub>=0.6.0) (41.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting tensorflow_text==1.15\n",
      "  Using cached https://files.pythonhosted.org/packages/a2/93/cfa6d4532d9cb7707028d7d4fd505fa7aab9d6e08275322e516bf6de509d/tensorflow_text-1.15.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting tensorflow<1.16,>=1.15.0 (from tensorflow_text==1.15)\n",
      "  Using cached https://files.pythonhosted.org/packages/5b/81/84fb7a323f9723f81edfc796d89e89aa95a9446ed7353c144195b3a3a3ba/tensorflow-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (1.0.8)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (1.12.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (1.11.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (1.17.2)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (3.9.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (0.1.7)\n",
      "Collecting tensorboard<1.16.0,>=1.15.0 (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15)\n",
      "  Using cached https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (0.33.4)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (3.0.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (0.8.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (1.1.0)\n",
      "Collecting tensorflow-estimator==1.15.1 (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15)\n",
      "  Using cached https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (1.23.0)\n",
      "Collecting gast==0.2.2 (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (0.8.0)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (2.10.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (41.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<1.16,>=1.15.0->tensorflow_text==1.15) (0.15.6)\n",
      "Installing collected packages: tensorboard, tensorflow-estimator, gast, tensorflow, tensorflow-text\n",
      "\u001b[31mERROR: Could not install packages due to an EnvironmentError: [Errno 13] Permission denied: '/opt/conda/lib/python3.7/site-packages/tensorboard/__init__.py'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==1.15\n",
    "!pip install \"tensorflow_hub>=0.6.0\"\n",
    "!pip3 install tensorflow_text==1.15\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "elmo = hub.KerasLayer(\"https://tfhub.dev/google/elmo/3\",signature=\"default\", as_dict=True trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elmo_vectors(sents):\n",
    "    embeddings = elmo(sents, )[\"elmo\"]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        return sess.run(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elmo_vectors(sents):\n",
    "    embeddings = elmo(sents, signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        return sess.run(embeddings)\n",
    "        #sess.run(tf.tables_initializer())\n",
    "        # return average of ELMo features as sentence vector\n",
    "        #return sess.run(tf.reduce_mean(embeddings,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "call() got an unexpected keyword argument 'signature'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-38e9191b0608>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"game\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0melmo_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melmo_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mword_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-20f49a7e45d4>\u001b[0m in \u001b[0;36melmo_vectors\u001b[0;34m(sents)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0melmo_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melmo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"default\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"elmo\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    850\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 851\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: call() got an unexpected keyword argument 'signature'"
     ]
    }
   ],
   "source": [
    "sents = \"\"\"The game ended quickly .\n",
    "He hunted some game for dinner .\n",
    "A game of swans in the river .*\n",
    "They played a game of chess .\n",
    "They were in a baseball game .\n",
    "She decided to eat som game .\n",
    "Game can be found in forests .\n",
    "Counterstrike is a popular game .\n",
    "They didn't follow the game .\n",
    "It was time to game .\"\"\".split('\\n')\n",
    "\n",
    "target = \"game\"\n",
    "\n",
    "elmo_vecs = elmo_vectors(sents)\n",
    "word_vecs = []\n",
    "for i, sent in enumerate(sents):\n",
    "    word_vecs.append(elmo_vecs[i][sent.split().index(target)])\n",
    "    print(\"Sentence: \", sent)\n",
    "    print(\"Vector for '%s:'\" % target, word_vecs[-1])\n",
    "    print()\n",
    "    \n",
    "print(\"Word vec size\", word_vecs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_vecs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-a72668cf7ad3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvec_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Similarities between '%s' vector in sentences:\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_vecs' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vec_size = word_vecs[0].shape[0]\n",
    "print(\"Similarities between '%s' vector in sentences:\" % target)\n",
    "for i in range(1, len(sents)):\n",
    "    print(\"Sent 0-%d:\" % i, cosine_similarity(word_vecs[0].reshape((1,vec_size)), \n",
    "                                              word_vecs[i].reshape((1,vec_size)))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

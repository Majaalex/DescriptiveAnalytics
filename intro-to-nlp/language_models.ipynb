{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language modelling\n",
    "\n",
    "* The task: assign a probability to a sequence of words $w_1,\\ldots ,w_n$\n",
    "* How likely it is that this sequence *belongs to the language*\n",
    "* Typically modelled by factoring as follows: $\\prod_i P(w_i|w_1,\\ldots w_{i-1})$\n",
    "\n",
    "## N-gram models\n",
    "\n",
    "* Traditional approach to language modelling\n",
    "* $P(w_i|w_1\\ldots w_{i-1})$ can be approximated using $P(w_i|w_{i-N},\\ldots,w_{i-1})$ for some sufficiently small $N$\n",
    "* and then estimated using the maximum-likelihood estimate $\\frac{C(w_{i-N},\\ldots,w_i)}{C(w_{i-N},\\ldots,w_{i-1})}$ where $C(x)$ is the count of occurrences of $x$ in some large corpus of text\n",
    "  * $P(barks|the\\ dog)=\\frac{C(the\\ dog\\ barks)}{C(the\\ dog)}$ ... this would be called a 3-gram model\n",
    "  * 5-gram models quite common\n",
    "\n",
    "## Traditional applications\n",
    "\n",
    "* Decoding: choose the most likely from several options\n",
    "* Speech recognizer\n",
    "  * At any moment provides a probability distribution over the alphabet\n",
    "  * Language model used to choose the most likely path: **decoding**\n",
    "* Decoding in machine translation\n",
    "  * Language model used to find a natural sounding translation\n",
    "* Text filtering and post-processing\n",
    "  * Recognize segments of text which are not natural\n",
    "  \n",
    "## Generation\n",
    "\n",
    "* The probabilistic formulation above can also be used to generate language from the models\n",
    "* At any point one can sample from the distribution $P(w_i|w_1\\ldots w_{i-1})$ and get the next word\n",
    "* Generate the text one word at a time:\n",
    "    * https://corpora.linguistik.uni-erlangen.de/cgi-bin/demos/TextParrot/parrot.perl\n",
    "    * Why do you think it is *this* bad?\n",
    "\n",
    "## Modern language models\n",
    "\n",
    "* Language modelling can be seen as a classification problem\n",
    "* Given a context, predict which word is the best continuation\n",
    "* The N-gram model is a very simple probabilistic way to achieve this\n",
    "* Modern methods employ considerably more powerful techniques\n",
    "* And most importantly: consider a much longer context (hundreds of words in length)\n",
    "\n",
    "* https://talktotransformer.com/\n",
    "* https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/\n",
    "* See the difference?\n",
    "\n",
    "## Modern language models transcend their role\n",
    "\n",
    "* Clearly, to generate language, one needs to understand it well\n",
    "* Modern language models are not used only for their ability to generate text, but also for their ability to encode the context\n",
    "* The model internally encodes the context, and uses this encoding to generate the output\n",
    "* It turns out this encoding is extremely useful as input for almost any task in NLP\n",
    "* Massive pre-trained language models yield the state-of-the-art across the board in today's NLP\n",
    "* For those interested: https://arxiv.org/pdf/1706.03762\n",
    "* Or attend our deep learning in NLP course (Turku / 4th period)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

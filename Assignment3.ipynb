{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import heapq, numpy as np\n",
    "import random\n",
    "#!pip3 install gensim\n",
    "from gensim import corpora, models\n",
    "import logging\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "dir_path = \"awards_2002/\"\n",
    "root_dir = os.fsencode(dir_path)\n",
    "for directory in os.listdir(root_dir):\n",
    "    sub_directory = os.fsdecode(directory)\n",
    "    current_path = dir_path + sub_directory + \"/\"\n",
    "    \n",
    "    for file in os.listdir(dir_path + sub_directory):\n",
    "        with open(current_path + file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            documents.append(f.read())\n",
    "            \n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_names(vectorizer, matrix):\n",
    "    features = tfidf_vectorizer.get_feature_names()\n",
    "    for doc_i in range(5):\n",
    "        print(\"\\nDocument %d, top terms by TF-IDF\" % doc_i)\n",
    "        for term, score in sorted(list(zip(features,matrix.toarray()[doc_i])), key=lambda x:-x[1])[:5]:\n",
    "            print(\"%.2f\\t%s\" % (score, term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clusters(matrix, clusters, n_keywords=10):\n",
    "    max_cluster = 10\n",
    "    for cluster in range(min(clusters), max_cluster):\n",
    "        cluster_docs = [i for i, c in enumerate(clusters) if c == cluster]\n",
    "        print(\"Cluster: %d (%d docs)\" % (cluster, len(cluster_docs)))\n",
    "        \n",
    "        # Keep scores for top n terms\n",
    "        new_matrix = np.zeros((len(cluster_docs), matrix.shape[1]))\n",
    "        for cluster_i, doc_vec in enumerate(matrix[cluster_docs].toarray()):\n",
    "            for idx, score in heapq.nlargest(n_keywords, enumerate(doc_vec), key=lambda x:x[1]):\n",
    "                new_matrix[cluster_i][idx] = score\n",
    "\n",
    "        # Aggregate scores for kept top terms\n",
    "        keywords = heapq.nlargest(n_keywords, zip(new_matrix.sum(axis=0), features))\n",
    "        print(', '.join([w for s,w in keywords]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a Experiment with KMeans and hierarchial clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 0 (61 docs)\n",
      "genes, sexual, drosophila, reproductive, arabidopsis, signaling, gene, brain, pathways, stress\n",
      "\n",
      "Cluster: 1 (36 docs)\n",
      "cts, thermal, micro, plasma, liquid, tip, discharge, sapphire, transport, nanoscale\n",
      "\n",
      "Cluster: 2 (50 docs)\n",
      "arctic, co2, nitrogen, ice, nutrient, sea, ecosystem, variability, lakes, holocene\n",
      "\n",
      "Cluster: 3 (26 docs)\n",
      "scholars, 1536, scholarship, women, csems, csem, mathematics, scholarships, coe, sbc\n",
      "\n",
      "Cluster: 4 (32 docs)\n",
      "manufacturing, product, machining, video, task, movement, miniature, aluminum, powder, maintenance\n",
      "\n",
      "Cluster: 5 (15 docs)\n",
      "topology, manifolds, algebraic, symplectic, proposer, topological, manifold, quantization, denton, bpa\n",
      "\n",
      "Cluster: 6 (60 docs)\n",
      "teachers, school, teacher, esi, technicians, curriculum, rec, middle, districts, eec\n",
      "\n",
      "Cluster: 7 (42 docs)\n",
      "eia, tree, michigan, seeds, digital, webber, mammals, phylogeny, ecological, seed\n",
      "\n",
      "Cluster: 8 (48 docs)\n",
      "protein, cell, powder, ii, display, coating, porphyrin, charge, ray, circuits\n",
      "\n",
      "Cluster: 9 (23 docs)\n",
      "dge, none, fellowship, fellowships, 7172, sheppard, minority, postdoctoral, 8252, nato\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=3, use_idf=True, sublinear_tf=True, max_df=0.1, max_features=100000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "features = tfidf_vectorizer.get_feature_names()\n",
    "matrix_sample = tfidf_matrix[:1000]\n",
    "km = KMeans(n_clusters=30, random_state=42, verbose=0)\n",
    "km.fit(matrix_sample)\n",
    "print_clusters(matrix_sample, km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 1 (5 docs)\n",
      "meeting, vigre, steroids, sensory, abs, travel, nervous, integrative, archbold, comparative\n",
      "\n",
      "Cluster: 2 (5 docs)\n",
      "symposium, pachavis, shall, self, cardiovascular, nanoengineered, boston, heart, 20th, mrs\n",
      "\n",
      "Cluster: 3 (5 docs)\n",
      "ozone, oh, atmospheric, hynes, ho2, o3, buoys, mesosphere, tropospheric, springtime\n",
      "\n",
      "Cluster: 4 (4 docs)\n",
      "nitrogen, meliloti, isotope, herbivores, symbiosis, cycling, frozen, herbivore, grasslands, ucsb\n",
      "\n",
      "Cluster: 5 (6 docs)\n",
      "terrorist, attacks, americans, attack, wtc, terrorism, events, agenda, identity, outcomes\n",
      "\n",
      "Cluster: 6 (7 docs)\n",
      "co2, respiration, ecosystem, land, tundra, africa, desert, latitudinal, soil, elevated\n",
      "\n",
      "Cluster: 7 (6 docs)\n",
      "birds, mammals, variation, coat, plumage, genetically, endocrine, avian, craniodental, evolutionary\n",
      "\n",
      "Cluster: 8 (4 docs)\n",
      "wisconsin, madison, complexity, inference, conditional, worst, case, hardness, moment, vertebrate\n",
      "\n",
      "Cluster: 9 (6 docs)\n",
      "arctic, ice, nitrogen, bering, pack, iasc, basin, amerasian, opp, yanling\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=2, use_idf=True,max_df=0.1, max_features=100000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "features = tfidf_vectorizer.get_feature_names()\n",
    "matrix_sample = tfidf_matrix[:1000]\n",
    "z = linkage(matrix_sample.todense(), metric=\"cosine\", method=\"complete\")\n",
    "clusters = fcluster(z, t=0.99, criterion=\"distance\")\n",
    "print_clusters(matrix_sample, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a results\n",
    "\n",
    "#### Fcluster\n",
    "\n",
    "* With a min_df of 1 a lot of numbers started popping up and multiple clusters with the same terms\n",
    "* Higher min_df doesn't do much more than potentially hide \"high value\" terms\n",
    "* Mostly good terms with a decent setup\n",
    "* Small cluster size (# of docs) - related to the t in fcluster\n",
    "* Method to euclidian instead of complete didn't give much benefit\n",
    "\n",
    "#### KMeans\n",
    "\n",
    "* Large clusters\n",
    "* More numbers in the clusters (Potentially useless, potentially good ie. genes)\n",
    "* Seems dependant on the random_state\n",
    "* Higher than 2 min_df just leads to clusters that are too broad\n",
    "\n",
    "--\n",
    "\n",
    "In my experimentation I feel like the end-result that was best was the most recent hierarchial clustering. For one, none of the clusters had numbers which I atleast saw as a larger negative.\n",
    "\n",
    "That said it has it's pros and cons as well. The clusters are considerably smaller in size compared to the KMeans clusters, where these are about 10 or so docs in size, the KMeans clusters seem to be around 25 or so. This is both good and bad in the sense that a smaller cluster most likely means that it's more specific, but it might also mean that it just made multiple clusters that are very similar.\n",
    "\n",
    "As such I'll go with the fcluster that I have above. It uses\n",
    "\n",
    "\n",
    "linkage(metric=\"cosine\", method=\"complete\")\n",
    "\n",
    "fcluster(t=0.99, criterion=\"distance\")\n",
    "\n",
    "Changing the method only gave very similar or sparse clusters. The t value just made the clusters even smaller, to the point where a doc was basically its own cluster. The min_df and max_df seemed to be pretty optimal at these values, as changing them too much just made clusters too broad or made them have too many \"bad\" terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b label the clusters\n",
    "\n",
    "Copypaste the cluster just in case since i shuffle the docs at the start of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cluster: 1 (7 docs) - **Electrical engineering**\n",
    "\n",
    "multimedia, compiler, smt, hmd, asic, processors, ieee, multiuser, adaptable, fpga\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 2 (8 docs) - **Software verification**\n",
    "\n",
    "hybrid, verification, embedded, software, qos, certification, stanford, rtl, checking, device\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 3 (7 docs) - **Continental drifting / Seafloor geography**\n",
    "\n",
    "continental, rift, rifting, spreading, seafloor, extension, pilcomayo, gulf, deposits, rio\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 4 (9 docs) - **Geography statistics**\n",
    "\n",
    "mantle, antarctic, seismic, gps, geodetic, stations, fault, puget, permanent, recoverable\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 5 (10 docs) - **Seismic activity?**\n",
    "\n",
    "detachment, uplift, floreana, magmatic, tectonic, cordillera, arc, strike, mafic, plateau\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 6 (15 docs) - **Thermodynamics**\n",
    "\n",
    "equations, ergodic, differential, probability, volterra, singularities, hyperbolic, oscillations, boundary, partial\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 7 (4 docs) - **Linear algebra**\n",
    "\n",
    "spaces, operators, teichmueller, functions, operator, metric, hankel, toeplitz, green, holomorphic\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 8 (13 docs) - **Algebraic topology**\n",
    "\n",
    "manifolds, homotopy, dm, geometric, compact, algebras, surfaces, variables, ring, operators\n",
    "\n",
    "\n",
    "\n",
    "* Cluster: 9 (4 docs) - **Deforestation & poor countries**\n",
    "\n",
    "migrants, semantic, tenure, real, compositionality, semantics, migration, syntactic, web, deforestation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c pick out 2 good and 2 bad clusters\n",
    "\n",
    "Clusters 7 & 8 are both good in my opinion.\n",
    "\n",
    "7 is a little small in size, however the terms are almost all related and for example the 3 names all correspond to functions related to algebra, and obviously functions are also in the picture.\n",
    "\n",
    "8 is also grouped in a similar way, where the terms can all be related back to topology, where for example homotpoty and manifolds are both main branches of topology.\n",
    "\n",
    "As for bad clusters, from these 10 I'd say it would be cluster 9 and cluster 3. (5 by extension)\n",
    "\n",
    "Cluster 9 is simply too hard to interpret. It has a mix of very different terms that are hard to group together. It could be correlated to the Amazon rainforest and the deforestation there but where do semantics come into the picture there.\n",
    "\n",
    "Cluster 3 in turn isn't that bad, however I feel like its too similar to that of cluster 5. THey're both related to seismic activity, and it's essentially just one being the seafloor, the other being mountains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d LDA modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf2_vectorizer = TfidfVectorizer()\n",
    "word_tokenizer = tfidf2_vectorizer.build_tokenizer()\n",
    "tokenized_text = [word_tokenizer(doc) for doc in documents]\n",
    "\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "lda_corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "lda_model = models.LdaModel(lda_corpus, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "0.0081\tCHE\n",
      "0.0061\tChemistry\n",
      "0.0057\tProgram\n",
      "0.0054\tUniversity\n",
      "0.0052\tDate\n",
      "0.0052\tNSF\n",
      "0.0049\tEstimated\n",
      "0.0042\tcurrent\n",
      "0.0038\tPrincipal\n",
      "0.0037\tCHEMISTRY\n",
      "\n",
      "Topic 1\n",
      "0.0073\tDate\n",
      "0.0065\tProgram\n",
      "0.0065\tNSF\n",
      "0.0065\tEstimated\n",
      "0.0062\tPrincipal\n",
      "0.0060\tcurrent\n",
      "0.0056\t2002\n",
      "0.0039\tdata\n",
      "0.0036\tTitle\n",
      "0.0036\tApplictn\n",
      "\n",
      "Topic 2\n",
      "0.0094\tNSF\n",
      "0.0085\tcurrent\n",
      "0.0085\tProgram\n",
      "0.0079\tPrincipal\n",
      "0.0075\tDate\n",
      "0.0074\tEstimated\n",
      "0.0071\tstudents\n",
      "0.0069\t2002\n",
      "0.0059\tUniversity\n",
      "0.0042\tPrgm\n",
      "\n",
      "Topic 3\n",
      "0.0047\tNSF\n",
      "0.0046\tcurrent\n",
      "0.0045\tDate\n",
      "0.0044\tEstimated\n",
      "0.0041\tProgram\n",
      "0.0039\tPrincipal\n",
      "0.0033\tproject\n",
      "0.0033\t2002\n",
      "0.0025\tscience\n",
      "0.0025\thave\n",
      "\n",
      "Topic 4\n",
      "0.0058\tNSF\n",
      "0.0057\tProgram\n",
      "0.0053\tEstimated\n",
      "0.0052\tDate\n",
      "0.0043\tcurrent\n",
      "0.0039\t2002\n",
      "0.0038\tPrincipal\n",
      "0.0030\tSponsor\n",
      "0.0029\tFile\n",
      "0.0028\tRef\n",
      "\n",
      "Topic 5\n",
      "0.0082\tCSEMS\n",
      "0.0074\tstudents\n",
      "0.0063\t1536\n",
      "0.0053\tDUE\n",
      "0.0044\tscholarships\n",
      "0.0043\tCS\n",
      "0.0041\tComputer\n",
      "0.0039\tprogram\n",
      "0.0037\tMATH\n",
      "0.0037\tscholarship\n",
      "\n",
      "Topic 6\n",
      "0.0070\tNSF\n",
      "0.0069\tProgram\n",
      "0.0065\tEstimated\n",
      "0.0063\tDate\n",
      "0.0058\t2002\n",
      "0.0053\tcurrent\n",
      "0.0046\tPrincipal\n",
      "0.0036\tproject\n",
      "0.0035\tAbstract\n",
      "0.0034\tManager\n",
      "\n",
      "Topic 7\n",
      "0.0008\tNSF\n",
      "0.0008\tDate\n",
      "0.0008\tEstimated\n",
      "0.0007\tcurrent\n",
      "0.0007\tProgram\n",
      "0.0005\tPrincipal\n",
      "0.0005\t2002\n",
      "0.0004\tsystems\n",
      "0.0004\tproject\n",
      "0.0004\tFile\n",
      "\n",
      "Topic 8\n",
      "0.0069\tmaterials\n",
      "0.0050\tPrincipal\n",
      "0.0049\tcurrent\n",
      "0.0048\tEstimated\n",
      "0.0044\tProgram\n",
      "0.0044\tNSF\n",
      "0.0041\tDate\n",
      "0.0036\tnew\n",
      "0.0035\tstudents\n",
      "0.0034\t2002\n",
      "\n",
      "Topic 9\n",
      "0.0056\tProgram\n",
      "0.0052\tNSF\n",
      "0.0050\tDate\n",
      "0.0050\tEstimated\n",
      "0.0039\tcurrent\n",
      "0.0038\tPrincipal\n",
      "0.0037\t2002\n",
      "0.0030\tproject\n",
      "0.0028\tAbstract\n",
      "0.0027\tNumber\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect topics\n",
    "for i, topic in lda_model.show_topics(num_words=50, formatted=False):\n",
    "    print(\"Topic\", i)\n",
    "    printed_terms = 0\n",
    "    for term, score in topic:\n",
    "        if printed_terms >= 10:\n",
    "            break\n",
    "        elif term in \"Award Investigator research this these will that the This of OF and to for in or The is be may an a with at are on by as from can\".split():\n",
    "            continue\n",
    "        printed_terms += 1\n",
    "        print(\"%.4f\\t%s\" % (score,term))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case a lot of the topics seem to be very similar if not almost identical, however because of how LDA is intended to work this does make some sense. Since this modelling is designed so that a document can fall under multiple topics.\n",
    "\n",
    "After removing some stopwords and also removing some terms that occured in every listed topic, you can see that there are some differences between the topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_words = [\"mathematics\", \"console\", \"spring\", \"technology\", \"communication\"]\n",
    "tfidf2_vectorizer = TfidfVectorizer()\n",
    "word_tokenizer = tfidf2_vectorizer.build_tokenizer()\n",
    "tokenized_text = [word_tokenizer(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base\n",
    "\n",
    "Nothing unusual here, I'm not surprised to see that the base settings are decent. Interestingly technology is only 0.77 similar to technologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 21:44:47,691 : INFO : collecting all words and their counts\n",
      "2020-03-12 21:44:47,694 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 21:44:48,305 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 21:44:48,306 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 21:44:48,395 : INFO : effective_min_count=5 retains 27041 unique words (23% of original 113911, drops 86870)\n",
      "2020-03-12 21:44:48,396 : INFO : effective_min_count=5 leaves 3552589 word corpus (96% of original 3681650, drops 129061)\n",
      "2020-03-12 21:44:48,470 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 21:44:48,474 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 21:44:48,475 : INFO : downsampling leaves estimated 2916612 word corpus (82.1% of prior 3552589)\n",
      "2020-03-12 21:44:48,537 : INFO : estimated required memory for 27041 words and 100 dimensions: 35153300 bytes\n",
      "2020-03-12 21:44:48,538 : INFO : resetting layer weights\n",
      "2020-03-12 21:44:52,545 : INFO : training model with 3 workers on 27041 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 21:44:53,559 : INFO : EPOCH 1 - PROGRESS: at 37.75% examples, 1094232 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:44:54,569 : INFO : EPOCH 1 - PROGRESS: at 76.79% examples, 1107359 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:44:55,161 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:44:55,167 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:44:55,174 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:44:55,175 : INFO : EPOCH - 1 : training on 3681650 raw words (2916453 effective words) took 2.6s, 1111477 effective words/s\n",
      "2020-03-12 21:44:56,181 : INFO : EPOCH 2 - PROGRESS: at 56.16% examples, 1636458 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:44:57,041 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:44:57,042 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:44:57,044 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:44:57,045 : INFO : EPOCH - 2 : training on 3681650 raw words (2915939 effective words) took 1.9s, 1563213 effective words/s\n",
      "2020-03-12 21:44:58,053 : INFO : EPOCH 3 - PROGRESS: at 39.32% examples, 1145566 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:44:59,063 : INFO : EPOCH 3 - PROGRESS: at 90.19% examples, 1306251 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:44:59,305 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:44:59,315 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:44:59,318 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:44:59,319 : INFO : EPOCH - 3 : training on 3681650 raw words (2916592 effective words) took 2.3s, 1285275 effective words/s\n",
      "2020-03-12 21:45:00,325 : INFO : EPOCH 4 - PROGRESS: at 38.80% examples, 1131365 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:45:01,328 : INFO : EPOCH 4 - PROGRESS: at 86.01% examples, 1250010 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:45:01,676 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:45:01,679 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:45:01,681 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:45:01,682 : INFO : EPOCH - 4 : training on 3681650 raw words (2916732 effective words) took 2.4s, 1236508 effective words/s\n",
      "2020-03-12 21:45:02,688 : INFO : EPOCH 5 - PROGRESS: at 42.80% examples, 1248363 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:45:03,669 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:45:03,677 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:45:03,680 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:45:03,680 : INFO : EPOCH - 5 : training on 3681650 raw words (2916622 effective words) took 2.0s, 1462552 effective words/s\n",
      "2020-03-12 21:45:03,682 : INFO : training on a 18408250 raw words (14582338 effective words) took 11.1s, 1309494 effective words/s\n",
      "2020-03-12 21:45:03,682 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  mathematics\n",
      "[('engineering', 0.813104510307312), ('science', 0.8105961084365845), ('discipline', 0.7897181510925293), ('physics', 0.7478721141815186), ('sciences', 0.7468525767326355), ('practice', 0.7271543145179749), ('disciplines', 0.7233316898345947), ('majors', 0.7194168567657471), ('physicists', 0.7163714170455933), ('concepts', 0.7058789134025574)]\n",
      "Most similar to:  console\n",
      "[('upwind', 0.9170166850090027), ('seat', 0.8907194137573242), ('chirped', 0.8875600099563599), ('ROV', 0.8860045671463013), ('alternately', 0.8854444622993469), ('reproducibly', 0.8848638534545898), ('gabbro', 0.8848577737808228), ('HCl', 0.8817570805549622), ('rectangular', 0.8816057443618774), ('GDP', 0.88057541847229)]\n",
      "Most similar to:  spring\n",
      "[('Pretoria', 0.7438228130340576), ('meridional', 0.7216531038284302), ('Sept', 0.7069242000579834), ('Toronto', 0.6955479383468628), ('03', 0.6943494081497192), ('Oct', 0.6931256055831909), ('23', 0.6827120780944824), ('late', 0.6820248365402222), ('21', 0.6789968609809875), ('Epley', 0.6661702394485474)]\n",
      "Most similar to:  technology\n",
      "[('technologies', 0.8077903985977173), ('manufacturing', 0.7065224647521973), ('vision', 0.6963142156600952), ('innovation', 0.6869412660598755), ('infrastructure', 0.6851321458816528), ('emerging', 0.6836101412773132), ('industry', 0.6827294826507568), ('standards', 0.6818342804908752), ('industries', 0.6811015009880066), ('nanotechnology', 0.6797153949737549)]\n",
      "Most similar to:  communication\n",
      "[('security', 0.8439438343048096), ('transportation', 0.8330591917037964), ('networking', 0.8318458795547485), ('intelligence', 0.825870156288147), ('wireless', 0.8235416412353516), ('communications', 0.8114982843399048), ('networks', 0.8064514398574829), ('computing', 0.799705445766449), ('personal', 0.7966629862785339), ('distributed', 0.7947649359703064)]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# test with size, window, min_count, iter, sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text)\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size = 5, Size = 200\n",
    "\n",
    "A low size is a huge detriment too the quality of the model. It underfits the model and it then thinks everything is very similar to the given word.\n",
    "\n",
    "A larger value than the default 100 doesn't necessarily change much. At least in this case. The order of words and such that are similar does change a little, but it does not seem to give any proper quantitative value. Maybe with a larger corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 21:45:03,714 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2020-03-12 21:45:03,715 : INFO : collecting all words and their counts\n",
      "2020-03-12 21:45:03,718 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 21:45:04,255 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 21:45:04,256 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 21:45:04,575 : INFO : effective_min_count=5 retains 27041 unique words (23% of original 113911, drops 86870)\n",
      "2020-03-12 21:45:04,576 : INFO : effective_min_count=5 leaves 3552589 word corpus (96% of original 3681650, drops 129061)\n",
      "2020-03-12 21:45:04,650 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 21:45:04,653 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 21:45:04,654 : INFO : downsampling leaves estimated 2916612 word corpus (82.1% of prior 3552589)\n",
      "2020-03-12 21:45:04,712 : INFO : estimated required memory for 27041 words and 5 dimensions: 14602140 bytes\n",
      "2020-03-12 21:45:04,713 : INFO : resetting layer weights\n",
      "2020-03-12 21:45:08,654 : INFO : training model with 3 workers on 27041 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 21:45:09,661 : INFO : EPOCH 1 - PROGRESS: at 61.98% examples, 1810075 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-12 21:45:10,347 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:45:10,350 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:45:10,352 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:45:10,353 : INFO : EPOCH - 1 : training on 3681650 raw words (2917455 effective words) took 1.7s, 1723608 effective words/s\n",
      "2020-03-12 21:45:11,357 : INFO : EPOCH 2 - PROGRESS: at 56.42% examples, 1647582 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:45:12,145 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:45:12,146 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:45:12,147 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:45:12,147 : INFO : EPOCH - 2 : training on 3681650 raw words (2916735 effective words) took 1.8s, 1628657 effective words/s\n",
      "2020-03-12 21:45:13,165 : INFO : EPOCH 3 - PROGRESS: at 58.30% examples, 1690617 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:45:13,870 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:45:13,875 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:45:13,878 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:45:13,879 : INFO : EPOCH - 3 : training on 3681650 raw words (2916707 effective words) took 1.7s, 1695808 effective words/s\n",
      "2020-03-12 21:45:14,887 : INFO : EPOCH 4 - PROGRESS: at 57.21% examples, 1666876 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:45:15,661 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:45:15,662 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:45:15,667 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:45:15,668 : INFO : EPOCH - 4 : training on 3681650 raw words (2916925 effective words) took 1.8s, 1635064 effective words/s\n",
      "2020-03-12 21:45:16,674 : INFO : EPOCH 5 - PROGRESS: at 58.07% examples, 1690626 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:45:17,386 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:45:17,391 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:45:17,395 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:45:17,395 : INFO : EPOCH - 5 : training on 3681650 raw words (2916246 effective words) took 1.7s, 1692367 effective words/s\n",
      "2020-03-12 21:45:17,396 : INFO : training on a 18408250 raw words (14584068 effective words) took 8.7s, 1668531 effective words/s\n",
      "2020-03-12 21:45:17,405 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  mathematics\n",
      "[('degree', 0.9968880414962769), ('awareness', 0.9961378574371338), ('science', 0.9955247640609741), ('disciplines', 0.9952576160430908), ('emphasis', 0.9933594465255737), ('public', 0.9914232492446899), ('industrial', 0.9895184636116028), ('literacy', 0.9889904260635376), ('recruitment', 0.9889792203903198), ('expose', 0.9889682531356812)]\n",
      "Most similar to:  console\n",
      "[('inflexible', 0.9982613325119019), ('EBSD', 0.9980884790420532), ('reflectors', 0.9975018501281738), ('tional', 0.9973971247673035), ('HCl', 0.9973499178886414), ('duplex', 0.9966325759887695), ('dbi', 0.9961663484573364), ('SAR', 0.9958171248435974), ('aptamers', 0.995576798915863), ('1664', 0.9949343204498291)]\n",
      "Most similar to:  spring\n",
      "[('yr', 0.9788516163825989), ('40', 0.9675351977348328), ('percent', 0.964647650718689), ('Koenker', 0.962959885597229), ('hostplant', 0.9621850848197937), ('Cretaceous', 0.9587557315826416), ('late', 0.9558874368667603), ('Miocene', 0.9533337950706482), ('Suter', 0.9515513777732849), ('billion', 0.9496740698814392)]\n",
      "Most similar to:  technology\n",
      "[('advanced', 0.9974872469902039), ('integration', 0.994974672794342), ('networking', 0.9945353269577026), ('computer', 0.9925031661987305), ('services', 0.9922170639038086), ('standards', 0.9901372194290161), ('resources', 0.9899858236312866), ('creating', 0.9894241094589233), ('security', 0.9885736107826233), ('developers', 0.9876440763473511)]\n",
      "Most similar to:  communication\n",
      "[('manufacturing', 0.9978423714637756), ('multimedia', 0.9942615032196045), ('generation', 0.9926227927207947), ('visualization', 0.9926086664199829), ('security', 0.9921014904975891), ('technologies', 0.9920940399169922), ('communications', 0.9911773204803467), ('applications', 0.9898582100868225), ('networks', 0.9885921478271484), ('vehicle', 0.9873433113098145)]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# test with size (int), window(int), min_count(int), iter(int), sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text, size=5)\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 21:45:17,423 : INFO : collecting all words and their counts\n",
      "2020-03-12 21:45:17,424 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 21:45:17,966 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 21:45:17,967 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 21:45:18,051 : INFO : effective_min_count=5 retains 27041 unique words (23% of original 113911, drops 86870)\n",
      "2020-03-12 21:45:18,051 : INFO : effective_min_count=5 leaves 3552589 word corpus (96% of original 3681650, drops 129061)\n",
      "2020-03-12 21:45:18,117 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 21:45:18,120 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 21:45:18,120 : INFO : downsampling leaves estimated 2916612 word corpus (82.1% of prior 3552589)\n",
      "2020-03-12 21:45:18,178 : INFO : estimated required memory for 27041 words and 200 dimensions: 56786100 bytes\n",
      "2020-03-12 21:45:18,179 : INFO : resetting layer weights\n",
      "2020-03-12 21:45:22,163 : INFO : training model with 3 workers on 27041 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 21:45:23,181 : INFO : EPOCH 1 - PROGRESS: at 29.68% examples, 858853 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:45:24,198 : INFO : EPOCH 1 - PROGRESS: at 60.16% examples, 865447 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:45:25,212 : INFO : EPOCH 1 - PROGRESS: at 90.46% examples, 866601 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:45:25,506 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:45:25,507 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:45:25,521 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:45:25,522 : INFO : EPOCH - 1 : training on 3681650 raw words (2915947 effective words) took 3.4s, 869671 effective words/s\n",
      "2020-03-12 21:45:26,526 : INFO : EPOCH 2 - PROGRESS: at 33.48% examples, 978273 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:45:27,527 : INFO : EPOCH 2 - PROGRESS: at 63.57% examples, 927830 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:45:28,535 : INFO : EPOCH 2 - PROGRESS: at 93.41% examples, 905111 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:45:28,728 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:45:28,729 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:45:28,742 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:45:28,743 : INFO : EPOCH - 2 : training on 3681650 raw words (2917074 effective words) took 3.2s, 906769 effective words/s\n",
      "2020-03-12 21:45:29,765 : INFO : EPOCH 3 - PROGRESS: at 30.76% examples, 885130 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:45:30,766 : INFO : EPOCH 3 - PROGRESS: at 61.22% examples, 885571 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:45:31,768 : INFO : EPOCH 3 - PROGRESS: at 91.26% examples, 881229 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:45:32,043 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:45:32,048 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:45:32,050 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:45:32,051 : INFO : EPOCH - 3 : training on 3681650 raw words (2916909 effective words) took 3.3s, 883001 effective words/s\n",
      "2020-03-12 21:45:33,057 : INFO : EPOCH 4 - PROGRESS: at 29.42% examples, 860655 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:45:34,071 : INFO : EPOCH 4 - PROGRESS: at 59.91% examples, 867986 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:45:35,089 : INFO : EPOCH 4 - PROGRESS: at 90.19% examples, 867187 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:45:35,397 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:45:35,411 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:45:35,415 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:45:35,415 : INFO : EPOCH - 4 : training on 3681650 raw words (2916250 effective words) took 3.4s, 868250 effective words/s\n",
      "2020-03-12 21:45:36,434 : INFO : EPOCH 5 - PROGRESS: at 31.01% examples, 895323 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:45:37,438 : INFO : EPOCH 5 - PROGRESS: at 62.78% examples, 908446 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:45:38,460 : INFO : EPOCH 5 - PROGRESS: at 94.20% examples, 903506 words/s, in_qsize 5, out_qsize 1\n",
      "2020-03-12 21:45:38,618 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:45:38,632 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:45:38,637 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:45:38,638 : INFO : EPOCH - 5 : training on 3681650 raw words (2916704 effective words) took 3.2s, 906508 effective words/s\n",
      "2020-03-12 21:45:38,638 : INFO : training on a 18408250 raw words (14582884 effective words) took 16.5s, 885184 effective words/s\n",
      "2020-03-12 21:45:38,646 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  mathematics\n",
      "[('engineering', 0.8135855793952942), ('science', 0.7961062788963318), ('discipline', 0.7698987722396851), ('physics', 0.7622895240783691), ('sciences', 0.7398695945739746), ('majors', 0.7313045263290405), ('practice', 0.7287845611572266), ('mathematicians', 0.7127530574798584), ('concepts', 0.7096660137176514), ('literacy', 0.7071012258529663)]\n",
      "Most similar to:  console\n",
      "[('phot1', 0.8887844085693359), ('metaphase', 0.8727130889892578), ('Moon', 0.8691428899765015), ('effluent', 0.8685858845710754), ('quenched', 0.8675808906555176), ('sizing', 0.8673232793807983), ('scFISH', 0.8638952970504761), ('NbTi', 0.8633050918579102), ('label', 0.8616794943809509), ('leech', 0.8604432940483093)]\n",
      "Most similar to:  spring\n",
      "[('late', 0.8091793060302734), ('winter', 0.7742407321929932), ('north', 0.7732067108154297), ('Miocene', 0.7693005800247192), ('Tertiary', 0.7668789625167847), ('Peru', 0.750515341758728), ('Located', 0.7456796765327454), ('meridional', 0.7448593378067017), ('eastern', 0.736425518989563), ('season', 0.7323265075683594)]\n",
      "Most similar to:  technology\n",
      "[('technologies', 0.7512329816818237), ('vision', 0.7050291299819946), ('standards', 0.6890392303466797), ('nanotechnology', 0.6882448196411133), ('innovation', 0.6842256784439087), ('emerging', 0.6683863997459412), ('technological', 0.6660467982292175), ('manufacturing', 0.6648385524749756), ('curriculum', 0.6633661985397339), ('industry', 0.656575083732605)]\n",
      "Most similar to:  communication\n",
      "[('networking', 0.8654577136039734), ('security', 0.8454043865203857), ('wireless', 0.8420361280441284), ('communications', 0.8281426429748535), ('secure', 0.8261332511901855), ('transportation', 0.824290931224823), ('networks', 0.8216021060943604), ('computing', 0.8087650537490845), ('architectural', 0.8072676658630371), ('virtual', 0.8052777051925659)]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# test with size (int), window(int), min_count(int), iter(int), sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text, size=200)\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min_count=2,10\n",
    "\n",
    "Going too high on min_count simply causes situations where there are no words that are similar.\n",
    "\n",
    "For words similar to mathematics it does not make a big difference, since they seem to occur so often in this scope of documents. For words similar to spring and console however, you quite quickly lose a lot of words from the list, and it just fills them with new ones. For whatever reason this does increase the similarity compared to base though.\n",
    "\n",
    "With a high count that is too high, in this case it just doesn't know some words. In my case console doesn't exist with a mincount that is 7 or higher. At this point console and similarities simply feels random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 21:45:38,691 : INFO : collecting all words and their counts\n",
      "2020-03-12 21:45:38,693 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 21:45:39,234 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 21:45:39,235 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 21:45:39,582 : INFO : effective_min_count=2 retains 53696 unique words (47% of original 113911, drops 60215)\n",
      "2020-03-12 21:45:39,583 : INFO : effective_min_count=2 leaves 3621435 word corpus (98% of original 3681650, drops 60215)\n",
      "2020-03-12 21:45:39,714 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 21:45:39,716 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 21:45:39,717 : INFO : downsampling leaves estimated 2993497 word corpus (82.7% of prior 3621435)\n",
      "2020-03-12 21:45:39,837 : INFO : estimated required memory for 53696 words and 100 dimensions: 69804800 bytes\n",
      "2020-03-12 21:45:39,838 : INFO : resetting layer weights\n",
      "2020-03-12 21:45:47,622 : INFO : training model with 3 workers on 53696 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 21:45:48,628 : INFO : EPOCH 1 - PROGRESS: at 46.06% examples, 1379111 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:45:49,640 : INFO : EPOCH 1 - PROGRESS: at 88.87% examples, 1321580 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:45:49,927 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:45:49,928 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:45:49,931 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:45:49,931 : INFO : EPOCH - 1 : training on 3681650 raw words (2993220 effective words) took 2.3s, 1299266 effective words/s\n",
      "2020-03-12 21:45:50,946 : INFO : EPOCH 2 - PROGRESS: at 36.11% examples, 1072982 words/s, in_qsize 5, out_qsize 1\n",
      "2020-03-12 21:45:51,946 : INFO : EPOCH 2 - PROGRESS: at 73.46% examples, 1094099 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:45:52,650 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:45:52,651 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:45:52,663 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:45:52,663 : INFO : EPOCH - 2 : training on 3681650 raw words (2994500 effective words) took 2.7s, 1097707 effective words/s\n",
      "2020-03-12 21:45:53,687 : INFO : EPOCH 3 - PROGRESS: at 37.48% examples, 1103938 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:45:54,690 : INFO : EPOCH 3 - PROGRESS: at 75.67% examples, 1119491 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:45:55,318 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:45:55,319 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:45:55,322 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:45:55,323 : INFO : EPOCH - 3 : training on 3681650 raw words (2993936 effective words) took 2.7s, 1127889 effective words/s\n",
      "2020-03-12 21:45:56,327 : INFO : EPOCH 4 - PROGRESS: at 35.84% examples, 1076383 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:45:57,331 : INFO : EPOCH 4 - PROGRESS: at 72.36% examples, 1081487 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:45:58,070 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:45:58,072 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:45:58,074 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:45:58,075 : INFO : EPOCH - 4 : training on 3681650 raw words (2993128 effective words) took 2.7s, 1089164 effective words/s\n",
      "2020-03-12 21:45:59,092 : INFO : EPOCH 5 - PROGRESS: at 37.75% examples, 1117827 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:00,103 : INFO : EPOCH 5 - PROGRESS: at 74.54% examples, 1102128 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:00,783 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:46:00,784 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:46:00,795 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:46:00,796 : INFO : EPOCH - 5 : training on 3681650 raw words (2993070 effective words) took 2.7s, 1101603 effective words/s\n",
      "2020-03-12 21:46:00,796 : INFO : training on a 18408250 raw words (14967854 effective words) took 13.2s, 1136194 effective words/s\n",
      "2020-03-12 21:46:00,805 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  mathematics\n",
      "[('science', 0.8264923691749573), ('engineering', 0.8224689960479736), ('discipline', 0.7762817144393921), ('sciences', 0.7714868783950806), ('physics', 0.7629547119140625), ('practice', 0.7457550764083862), ('astronomy', 0.7379782199859619), ('majors', 0.72240149974823), ('literacy', 0.7207099795341492), ('careers', 0.7131826877593994)]\n",
      "Most similar to:  console\n",
      "[('disjunction', 0.8920546770095825), ('MIKE', 0.8914703130722046), ('slowing', 0.89117830991745), ('cabinetry', 0.8909517526626587), ('chirped', 0.8897838592529297), ('cloacal', 0.88832026720047), ('remnant', 0.886172354221344), ('Willy', 0.8849791288375854), ('bivalve', 0.8842459917068481), ('telechelic', 0.8838270306587219)]\n",
      "Most similar to:  spring\n",
      "[('late', 0.7426997423171997), ('Lahore', 0.73345947265625), ('Miocene', 0.7269890308380127), ('fall', 0.7182335257530212), ('winter', 0.7125062942504883), ('north', 0.7117009162902832), ('1996', 0.7115656733512878), ('Peru', 0.7019992470741272), ('Paleogene', 0.6845003366470337), ('Nisqually', 0.6797735691070557)]\n",
      "Most similar to:  technology\n",
      "[('technologies', 0.8225076198577881), ('manufacturing', 0.744360089302063), ('innovation', 0.7310715317726135), ('industry', 0.7202657461166382), ('standards', 0.7177245616912842), ('emerging', 0.7100335359573364), ('vision', 0.7055850625038147), ('nanotechnology', 0.6969549655914307), ('infrastructure', 0.6890676021575928), ('workforce', 0.6882977485656738)]\n",
      "Most similar to:  communication\n",
      "[('networking', 0.8810948729515076), ('security', 0.8637852072715759), ('wireless', 0.8566749691963196), ('transportation', 0.848667562007904), ('communications', 0.8428139686584473), ('architectural', 0.8298460245132446), ('networks', 0.8256322741508484), ('computing', 0.8240475654602051), ('allocation', 0.8180989027023315), ('scheduling', 0.8164153695106506)]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig()\n",
    "# test with size (int), window(int), min_count(int), iter(int), sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text, min_count=2)\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 21:46:00,849 : INFO : collecting all words and their counts\n",
      "2020-03-12 21:46:00,850 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 21:46:01,410 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 21:46:01,411 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 21:46:01,729 : INFO : effective_min_count=6 retains 24178 unique words (21% of original 113911, drops 89733)\n",
      "2020-03-12 21:46:01,729 : INFO : effective_min_count=6 leaves 3538274 word corpus (96% of original 3681650, drops 143376)\n",
      "2020-03-12 21:46:01,797 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 21:46:01,800 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 21:46:01,801 : INFO : downsampling leaves estimated 2900621 word corpus (82.0% of prior 3538274)\n",
      "2020-03-12 21:46:01,858 : INFO : estimated required memory for 24178 words and 100 dimensions: 31431400 bytes\n",
      "2020-03-12 21:46:01,859 : INFO : resetting layer weights\n",
      "2020-03-12 21:46:05,534 : INFO : training model with 3 workers on 24178 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 21:46:06,542 : INFO : EPOCH 1 - PROGRESS: at 42.03% examples, 1218536 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:07,548 : INFO : EPOCH 1 - PROGRESS: at 86.01% examples, 1240880 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:46:07,903 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:46:07,904 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:46:07,908 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:46:07,908 : INFO : EPOCH - 1 : training on 3681650 raw words (2900179 effective words) took 2.4s, 1224179 effective words/s\n",
      "2020-03-12 21:46:08,917 : INFO : EPOCH 2 - PROGRESS: at 40.14% examples, 1163157 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:46:09,919 : INFO : EPOCH 2 - PROGRESS: at 81.81% examples, 1181663 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:46:10,286 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:46:10,293 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:46:10,300 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:46:10,301 : INFO : EPOCH - 2 : training on 3681650 raw words (2900993 effective words) took 2.4s, 1215366 effective words/s\n",
      "2020-03-12 21:46:11,307 : INFO : EPOCH 3 - PROGRESS: at 39.88% examples, 1158839 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:46:12,311 : INFO : EPOCH 3 - PROGRESS: at 80.21% examples, 1158739 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:12,788 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:46:12,789 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:46:12,799 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:46:12,799 : INFO : EPOCH - 3 : training on 3681650 raw words (2900066 effective words) took 2.5s, 1163219 effective words/s\n",
      "2020-03-12 21:46:13,809 : INFO : EPOCH 4 - PROGRESS: at 39.88% examples, 1157322 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:46:14,810 : INFO : EPOCH 4 - PROGRESS: at 79.99% examples, 1156783 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:15,304 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:46:15,305 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:46:15,307 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:46:15,307 : INFO : EPOCH - 4 : training on 3681650 raw words (2900660 effective words) took 2.5s, 1160240 effective words/s\n",
      "2020-03-12 21:46:16,325 : INFO : EPOCH 5 - PROGRESS: at 40.14% examples, 1151783 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:17,327 : INFO : EPOCH 5 - PROGRESS: at 80.21% examples, 1153248 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:17,804 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:46:17,805 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:46:17,815 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:46:17,815 : INFO : EPOCH - 5 : training on 3681650 raw words (2900817 effective words) took 2.5s, 1158797 effective words/s\n",
      "2020-03-12 21:46:17,815 : INFO : training on a 18408250 raw words (14502715 effective words) took 12.3s, 1180945 effective words/s\n",
      "2020-03-12 21:46:17,830 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  mathematics\n",
      "[('engineering', 0.7995164394378662), ('science', 0.7989711761474609), ('discipline', 0.758630633354187), ('practice', 0.7436290383338928), ('sciences', 0.7373279929161072), ('physics', 0.7358354330062866), ('majors', 0.7308528423309326), ('concepts', 0.7071372866630554), ('bioengineering', 0.7014557123184204), ('nanotechnology', 0.699437141418457)]\n",
      "Most similar to:  console\n",
      "[('apartment', 0.9072535037994385), ('fluorometer', 0.8948256969451904), ('Lynntech', 0.888268232345581), ('microanalysis', 0.8870493173599243), ('Soboyejo', 0.8870485424995422), ('calibrating', 0.8837348222732544), ('USER', 0.8793085813522339), ('translating', 0.8791844844818115), ('DISSEMINATION', 0.8790374994277954), ('Multibeam', 0.8785663843154907)]\n",
      "Most similar to:  spring\n",
      "[('late', 0.7465955018997192), ('coast', 0.6969091892242432), ('Spain', 0.6968727707862854), ('Peru', 0.6955786347389221), ('summers', 0.6930745840072632), ('season', 0.689520537853241), ('24', 0.6895022988319397), ('Miocene', 0.688862144947052), ('northern', 0.6851586699485779), ('Spring', 0.6837981939315796)]\n",
      "Most similar to:  technology\n",
      "[('technologies', 0.7773727178573608), ('manufacturing', 0.712040901184082), ('innovation', 0.702988862991333), ('vision', 0.6800564527511597), ('technological', 0.6754603385925293), ('industry', 0.6728625297546387), ('nanotechnology', 0.6674739122390747), ('emerging', 0.6652483940124512), ('workforce', 0.6650598049163818), ('company', 0.6588358879089355)]\n",
      "Most similar to:  communication\n",
      "[('networking', 0.8493826389312744), ('wireless', 0.841497540473938), ('security', 0.8259178996086121), ('secure', 0.8132664561271667), ('reality', 0.8103145360946655), ('transportation', 0.8061657547950745), ('communications', 0.8025147318840027), ('distributed', 0.7964283227920532), ('multimedia', 0.7923852205276489), ('networks', 0.7885435223579407)]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig()\n",
    "# test with size (int), window(int), min_count(int), iter(int), sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text, min_count=6)\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iter = 10\n",
    "\n",
    "Going higher on the iterations seem to make things more accurate, especially looking at mathematics and technology. The same word in different forms gets higher in similarity compared to the base, when looking at the order instead of the value. So essentially this tells us that the base values were underfitting our data. With too many iterations however you would potentially be looking at iterations, this didn't feel like it started occuring yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 21:46:17,851 : INFO : collecting all words and their counts\n",
      "2020-03-12 21:46:17,852 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 21:46:18,426 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 21:46:18,428 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 21:46:18,737 : INFO : effective_min_count=5 retains 27041 unique words (23% of original 113911, drops 86870)\n",
      "2020-03-12 21:46:18,738 : INFO : effective_min_count=5 leaves 3552589 word corpus (96% of original 3681650, drops 129061)\n",
      "2020-03-12 21:46:18,804 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 21:46:18,807 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 21:46:18,807 : INFO : downsampling leaves estimated 2916612 word corpus (82.1% of prior 3552589)\n",
      "2020-03-12 21:46:18,866 : INFO : estimated required memory for 27041 words and 100 dimensions: 35153300 bytes\n",
      "2020-03-12 21:46:18,866 : INFO : resetting layer weights\n",
      "2020-03-12 21:46:22,871 : INFO : training model with 3 workers on 27041 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 21:46:23,879 : INFO : EPOCH 1 - PROGRESS: at 39.58% examples, 1153362 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:24,890 : INFO : EPOCH 1 - PROGRESS: at 78.90% examples, 1140863 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:25,421 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:46:25,422 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:46:25,424 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:46:25,424 : INFO : EPOCH - 1 : training on 3681650 raw words (2916536 effective words) took 2.5s, 1144587 effective words/s\n",
      "2020-03-12 21:46:26,430 : INFO : EPOCH 2 - PROGRESS: at 40.68% examples, 1187668 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:27,431 : INFO : EPOCH 2 - PROGRESS: at 87.28% examples, 1270689 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:27,719 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:46:27,722 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:46:27,731 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:46:27,732 : INFO : EPOCH - 2 : training on 3681650 raw words (2916252 effective words) took 2.3s, 1266376 effective words/s\n",
      "2020-03-12 21:46:28,739 : INFO : EPOCH 3 - PROGRESS: at 38.23% examples, 1114773 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:29,753 : INFO : EPOCH 3 - PROGRESS: at 77.87% examples, 1124085 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:46:30,303 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:46:30,312 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:46:30,312 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:46:30,313 : INFO : EPOCH - 3 : training on 3681650 raw words (2916441 effective words) took 2.6s, 1131703 effective words/s\n",
      "2020-03-12 21:46:31,320 : INFO : EPOCH 4 - PROGRESS: at 40.40% examples, 1178150 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:46:32,321 : INFO : EPOCH 4 - PROGRESS: at 86.53% examples, 1258617 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:32,630 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:46:32,632 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:46:32,642 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:46:32,643 : INFO : EPOCH - 4 : training on 3681650 raw words (2916038 effective words) took 2.3s, 1253933 effective words/s\n",
      "2020-03-12 21:46:33,655 : INFO : EPOCH 5 - PROGRESS: at 40.14% examples, 1163904 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:46:34,658 : INFO : EPOCH 5 - PROGRESS: at 81.03% examples, 1173326 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:46:35,118 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:46:35,119 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:46:35,123 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:46:35,124 : INFO : EPOCH - 5 : training on 3681650 raw words (2916823 effective words) took 2.5s, 1177650 effective words/s\n",
      "2020-03-12 21:46:36,138 : INFO : EPOCH 6 - PROGRESS: at 40.95% examples, 1185004 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:37,142 : INFO : EPOCH 6 - PROGRESS: at 82.34% examples, 1190530 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:37,555 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:46:37,556 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:46:37,565 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:46:37,565 : INFO : EPOCH - 6 : training on 3681650 raw words (2916663 effective words) took 2.4s, 1196708 effective words/s\n",
      "2020-03-12 21:46:38,578 : INFO : EPOCH 7 - PROGRESS: at 40.68% examples, 1179218 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:39,593 : INFO : EPOCH 7 - PROGRESS: at 82.33% examples, 1185540 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:40,009 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:46:40,018 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:46:40,023 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:46:40,023 : INFO : EPOCH - 7 : training on 3681650 raw words (2916795 effective words) took 2.5s, 1188739 effective words/s\n",
      "2020-03-12 21:46:41,041 : INFO : EPOCH 8 - PROGRESS: at 38.53% examples, 1111435 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:42,047 : INFO : EPOCH 8 - PROGRESS: at 78.11% examples, 1126347 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:42,567 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:46:42,577 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:46:42,577 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:46:42,578 : INFO : EPOCH - 8 : training on 3681650 raw words (2916968 effective words) took 2.6s, 1143585 effective words/s\n",
      "2020-03-12 21:46:43,586 : INFO : EPOCH 9 - PROGRESS: at 46.35% examples, 1346326 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:44,593 : INFO : EPOCH 9 - PROGRESS: at 85.47% examples, 1238732 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:46:44,950 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:46:44,960 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:46:44,963 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:46:44,963 : INFO : EPOCH - 9 : training on 3681650 raw words (2917205 effective words) took 2.4s, 1224799 effective words/s\n",
      "2020-03-12 21:46:45,968 : INFO : EPOCH 10 - PROGRESS: at 40.14% examples, 1171941 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:46,975 : INFO : EPOCH 10 - PROGRESS: at 81.55% examples, 1182787 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:46:47,418 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:46:47,419 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:46:47,429 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:46:47,429 : INFO : EPOCH - 10 : training on 3681650 raw words (2917373 effective words) took 2.5s, 1184923 effective words/s\n",
      "2020-03-12 21:46:47,430 : INFO : training on a 36816500 raw words (29167094 effective words) took 24.6s, 1187685 effective words/s\n",
      "2020-03-12 21:46:47,438 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  mathematics\n",
      "[('discipline', 0.6984312534332275), ('majors', 0.6910269856452942), ('physics', 0.675421953201294), ('mathematical', 0.6634280681610107), ('science', 0.6609295010566711), ('engineering', 0.6540749669075012), ('statistics', 0.639667809009552), ('college', 0.633525013923645), ('teachers', 0.6284337043762207), ('algebra', 0.6130471229553223)]\n",
      "Most similar to:  console\n",
      "[('chirped', 0.7431447505950928), ('melter', 0.7244747877120972), ('alternately', 0.7210508584976196), ('McCorriston', 0.7093610167503357), ('compass', 0.7071682214736938), ('carcinogen', 0.7053254842758179), ('compose', 0.7018460035324097), ('electrospray', 0.7004756927490234), ('collector', 0.6969977617263794), ('interferometer', 0.696153998374939)]\n",
      "Most similar to:  spring\n",
      "[('late', 0.7416752576828003), ('Miocene', 0.7361701726913452), ('winter', 0.6955665946006775), ('rainy', 0.6827518939971924), ('outbreak', 0.6766948103904724), ('Holocene', 0.6724553108215332), ('1996', 0.6696754693984985), ('interglacial', 0.6691102981567383), ('Glacial', 0.6632134318351746), ('Period', 0.6617318391799927)]\n",
      "Most similar to:  technology\n",
      "[('technologies', 0.7486552596092224), ('technological', 0.6377798318862915), ('industry', 0.6056122779846191), ('innovation', 0.6015154123306274), ('biotechnology', 0.5839470028877258), ('electronics', 0.5835738182067871), ('manufacturing', 0.5799727439880371), ('nanotechnology', 0.5781658291816711), ('workforce', 0.5769208669662476), ('capability', 0.5707526803016663)]\n",
      "Most similar to:  communication\n",
      "[('communications', 0.7797142267227173), ('networking', 0.7726008892059326), ('wireless', 0.7713886499404907), ('secure', 0.7433719038963318), ('security', 0.7176461815834045), ('middleware', 0.680264949798584), ('networks', 0.6749275922775269), ('distributed', 0.6712852716445923), ('client', 0.6629833579063416), ('routing', 0.662278950214386)]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# test with size (int), window(int), min_count(int), iter(int), sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text, iter=(10))\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 21:46:47,459 : INFO : collecting all words and their counts\n",
      "2020-03-12 21:46:47,460 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 21:46:47,988 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 21:46:47,988 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 21:46:48,066 : INFO : effective_min_count=5 retains 27041 unique words (23% of original 113911, drops 86870)\n",
      "2020-03-12 21:46:48,066 : INFO : effective_min_count=5 leaves 3552589 word corpus (96% of original 3681650, drops 129061)\n",
      "2020-03-12 21:46:48,140 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 21:46:48,143 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 21:46:48,143 : INFO : downsampling leaves estimated 2916612 word corpus (82.1% of prior 3552589)\n",
      "2020-03-12 21:46:48,200 : INFO : estimated required memory for 27041 words and 100 dimensions: 35153300 bytes\n",
      "2020-03-12 21:46:48,200 : INFO : resetting layer weights\n",
      "2020-03-12 21:46:52,179 : INFO : training model with 3 workers on 27041 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 21:46:53,191 : INFO : EPOCH 1 - PROGRESS: at 39.32% examples, 1141878 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:54,205 : INFO : EPOCH 1 - PROGRESS: at 79.43% examples, 1145078 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:54,756 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:46:54,757 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:46:54,768 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:46:54,769 : INFO : EPOCH - 1 : training on 3681650 raw words (2916114 effective words) took 2.6s, 1128564 effective words/s\n",
      "2020-03-12 21:46:55,772 : INFO : EPOCH 2 - PROGRESS: at 43.34% examples, 1265809 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:56,773 : INFO : EPOCH 2 - PROGRESS: at 88.87% examples, 1294807 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:57,047 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:46:57,048 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:46:57,050 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:46:57,051 : INFO : EPOCH - 2 : training on 3681650 raw words (2917078 effective words) took 2.3s, 1279398 effective words/s\n",
      "2020-03-12 21:46:58,057 : INFO : EPOCH 3 - PROGRESS: at 38.23% examples, 1117235 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:46:59,059 : INFO : EPOCH 3 - PROGRESS: at 77.87% examples, 1131854 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:46:59,628 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:46:59,630 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:46:59,632 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:46:59,633 : INFO : EPOCH - 3 : training on 3681650 raw words (2916727 effective words) took 2.6s, 1131761 effective words/s\n",
      "2020-03-12 21:47:00,640 : INFO : EPOCH 4 - PROGRESS: at 51.42% examples, 1496227 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:01,643 : INFO : EPOCH 4 - PROGRESS: at 91.83% examples, 1334165 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:01,827 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:47:01,836 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:47:01,841 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:47:01,841 : INFO : EPOCH - 4 : training on 3681650 raw words (2916519 effective words) took 2.2s, 1323188 effective words/s\n",
      "2020-03-12 21:47:02,851 : INFO : EPOCH 5 - PROGRESS: at 41.78% examples, 1214613 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:03,859 : INFO : EPOCH 5 - PROGRESS: at 82.61% examples, 1195481 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:47:04,282 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:47:04,284 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:47:04,294 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:47:04,295 : INFO : EPOCH - 5 : training on 3681650 raw words (2917526 effective words) took 2.4s, 1191540 effective words/s\n",
      "2020-03-12 21:47:05,301 : INFO : EPOCH 6 - PROGRESS: at 40.68% examples, 1186408 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:06,305 : INFO : EPOCH 6 - PROGRESS: at 81.28% examples, 1180228 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:06,754 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:47:06,756 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:47:06,759 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:47:06,759 : INFO : EPOCH - 6 : training on 3681650 raw words (2916730 effective words) took 2.5s, 1185480 effective words/s\n",
      "2020-03-12 21:47:07,769 : INFO : EPOCH 7 - PROGRESS: at 40.68% examples, 1182414 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:08,774 : INFO : EPOCH 7 - PROGRESS: at 81.55% examples, 1181078 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:09,106 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:47:09,109 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:47:09,114 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:47:09,115 : INFO : EPOCH - 7 : training on 3681650 raw words (2915769 effective words) took 2.4s, 1240261 effective words/s\n",
      "2020-03-12 21:47:10,128 : INFO : EPOCH 8 - PROGRESS: at 38.80% examples, 1124534 words/s, in_qsize 5, out_qsize 1\n",
      "2020-03-12 21:47:11,128 : INFO : EPOCH 8 - PROGRESS: at 94.71% examples, 1374689 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:47:11,207 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:47:11,210 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:47:11,216 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:47:11,216 : INFO : EPOCH - 8 : training on 3681650 raw words (2917190 effective words) took 2.1s, 1390731 effective words/s\n",
      "2020-03-12 21:47:12,231 : INFO : EPOCH 9 - PROGRESS: at 41.53% examples, 1200750 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:13,233 : INFO : EPOCH 9 - PROGRESS: at 81.55% examples, 1180326 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:13,696 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:47:13,707 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:47:13,709 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:47:13,710 : INFO : EPOCH - 9 : training on 3681650 raw words (2916734 effective words) took 2.5s, 1172087 effective words/s\n",
      "2020-03-12 21:47:14,714 : INFO : EPOCH 10 - PROGRESS: at 40.68% examples, 1188679 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:15,718 : INFO : EPOCH 10 - PROGRESS: at 81.28% examples, 1181373 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:16,184 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:47:16,187 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:47:16,189 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:47:16,190 : INFO : EPOCH - 10 : training on 3681650 raw words (2916460 effective words) took 2.5s, 1177930 effective words/s\n",
      "2020-03-12 21:47:17,193 : INFO : EPOCH 11 - PROGRESS: at 42.80% examples, 1248521 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:18,208 : INFO : EPOCH 11 - PROGRESS: at 82.07% examples, 1184994 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:18,660 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:47:18,663 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:47:18,664 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:47:18,664 : INFO : EPOCH - 11 : training on 3681650 raw words (2916501 effective words) took 2.5s, 1179301 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 21:47:19,682 : INFO : EPOCH 12 - PROGRESS: at 44.17% examples, 1272334 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:20,692 : INFO : EPOCH 12 - PROGRESS: at 85.47% examples, 1230910 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:21,030 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:47:21,038 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:47:21,043 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:47:21,044 : INFO : EPOCH - 12 : training on 3681650 raw words (2916749 effective words) took 2.4s, 1227907 effective words/s\n",
      "2020-03-12 21:47:22,052 : INFO : EPOCH 13 - PROGRESS: at 39.88% examples, 1163038 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:23,069 : INFO : EPOCH 13 - PROGRESS: at 79.15% examples, 1142020 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:23,589 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:47:23,597 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:47:23,599 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:47:23,600 : INFO : EPOCH - 13 : training on 3681650 raw words (2917026 effective words) took 2.6s, 1143890 effective words/s\n",
      "2020-03-12 21:47:24,606 : INFO : EPOCH 14 - PROGRESS: at 39.58% examples, 1154736 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:25,618 : INFO : EPOCH 14 - PROGRESS: at 79.15% examples, 1144558 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:26,133 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:47:26,143 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:47:26,146 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:47:26,146 : INFO : EPOCH - 14 : training on 3681650 raw words (2916134 effective words) took 2.5s, 1146774 effective words/s\n",
      "2020-03-12 21:47:27,155 : INFO : EPOCH 15 - PROGRESS: at 58.59% examples, 1701624 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:27,971 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:47:27,974 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:47:27,975 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:47:27,976 : INFO : EPOCH - 15 : training on 3681650 raw words (2916396 effective words) took 1.8s, 1597557 effective words/s\n",
      "2020-03-12 21:47:27,976 : INFO : training on a 55224750 raw words (43749653 effective words) took 35.8s, 1222201 effective words/s\n",
      "2020-03-12 21:47:27,984 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  mathematics\n",
      "[('discipline', 0.6427435874938965), ('mathematical', 0.6408028602600098), ('majors', 0.6399889588356018), ('teaching', 0.6306318044662476), ('engineering', 0.6289055347442627), ('physics', 0.6157816648483276), ('science', 0.6121276617050171), ('teachers', 0.6106493473052979), ('algebra', 0.5951567888259888), ('college', 0.5874304175376892)]\n",
      "Most similar to:  console\n",
      "[('airplane', 0.6556798815727234), ('collector', 0.6512964963912964), ('NSA', 0.6271592378616333), ('LC', 0.6028128862380981), ('inhibitor', 0.6010703444480896), ('elliptical', 0.5919283628463745), ('880', 0.5847868323326111), ('chirped', 0.5824361443519592), ('saltwater', 0.5803298950195312), ('kN', 0.5766463279724121)]\n",
      "Most similar to:  spring\n",
      "[('winter', 0.6461237072944641), ('dives', 0.64360511302948), ('austral', 0.6333377361297607), ('aboard', 0.6298508048057556), ('Located', 0.6257637739181519), ('Peninsula', 0.6229163408279419), ('fall', 0.6150477528572083), ('Sonoran', 0.6133734583854675), ('late', 0.6116336584091187), ('1996', 0.6074075102806091)]\n",
      "Most similar to:  technology\n",
      "[('technologies', 0.7539259195327759), ('industry', 0.5866354703903198), ('technological', 0.5704957246780396), ('electronics', 0.5676801800727844), ('capability', 0.5460439920425415), ('nanotechnology', 0.5445791482925415), ('devices', 0.5437442064285278), ('business', 0.5436065196990967), ('biotechnology', 0.5385543704032898), ('IT', 0.5310311317443848)]\n",
      "Most similar to:  communication\n",
      "[('communications', 0.8011804819107056), ('wireless', 0.7408273816108704), ('networking', 0.7287518978118896), ('secure', 0.6484889388084412), ('computing', 0.6403952240943909), ('network', 0.6261250972747803), ('distributed', 0.6229052543640137), ('intelligence', 0.6225178241729736), ('security', 0.6204857230186462), ('mobile', 0.6198967695236206)]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# test with size (int), window(int), min_count(int), iter(int), sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text, iter=(15))\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sg = 1\n",
    "\n",
    "Changing to skip-gram definitely feels like it helped with regards to 3 of the words here, and their similar words. Console and spring seem to just be an extremely bad fit for this dataset, however for the other 3 words it was quick to decide on very similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 21:47:28,006 : INFO : collecting all words and their counts\n",
      "2020-03-12 21:47:28,007 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 21:47:28,535 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 21:47:28,536 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 21:47:28,831 : INFO : effective_min_count=5 retains 27041 unique words (23% of original 113911, drops 86870)\n",
      "2020-03-12 21:47:28,832 : INFO : effective_min_count=5 leaves 3552589 word corpus (96% of original 3681650, drops 129061)\n",
      "2020-03-12 21:47:28,910 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 21:47:28,912 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 21:47:28,913 : INFO : downsampling leaves estimated 2916612 word corpus (82.1% of prior 3552589)\n",
      "2020-03-12 21:47:28,969 : INFO : estimated required memory for 27041 words and 100 dimensions: 35153300 bytes\n",
      "2020-03-12 21:47:28,970 : INFO : resetting layer weights\n",
      "2020-03-12 21:47:32,940 : INFO : training model with 3 workers on 27041 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 21:47:33,991 : INFO : EPOCH 1 - PROGRESS: at 10.60% examples, 294378 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:47:35,021 : INFO : EPOCH 1 - PROGRESS: at 22.01% examples, 309377 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:47:36,050 : INFO : EPOCH 1 - PROGRESS: at 33.48% examples, 314550 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:47:37,065 : INFO : EPOCH 1 - PROGRESS: at 44.17% examples, 312828 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:38,102 : INFO : EPOCH 1 - PROGRESS: at 55.62% examples, 314792 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:39,103 : INFO : EPOCH 1 - PROGRESS: at 66.55% examples, 315186 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:40,120 : INFO : EPOCH 1 - PROGRESS: at 77.61% examples, 314679 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:47:41,154 : INFO : EPOCH 1 - PROGRESS: at 88.34% examples, 313619 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:42,161 : INFO : EPOCH 1 - PROGRESS: at 99.23% examples, 313702 words/s, in_qsize 4, out_qsize 0\n",
      "2020-03-12 21:47:42,175 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:47:42,221 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:47:42,228 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:47:42,229 : INFO : EPOCH - 1 : training on 3681650 raw words (2916253 effective words) took 9.3s, 314024 effective words/s\n",
      "2020-03-12 21:47:43,247 : INFO : EPOCH 2 - PROGRESS: at 10.88% examples, 313820 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:44,260 : INFO : EPOCH 2 - PROGRESS: at 21.72% examples, 314466 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:45,271 : INFO : EPOCH 2 - PROGRESS: at 32.91% examples, 317504 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:46,272 : INFO : EPOCH 2 - PROGRESS: at 43.91% examples, 317946 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:47,323 : INFO : EPOCH 2 - PROGRESS: at 54.83% examples, 314913 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:48,342 : INFO : EPOCH 2 - PROGRESS: at 66.54% examples, 318201 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:47:49,362 : INFO : EPOCH 2 - PROGRESS: at 78.11% examples, 319310 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:50,366 : INFO : EPOCH 2 - PROGRESS: at 88.63% examples, 317857 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:51,355 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:47:51,387 : INFO : EPOCH 2 - PROGRESS: at 99.77% examples, 317888 words/s, in_qsize 1, out_qsize 1\n",
      "2020-03-12 21:47:51,388 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:47:51,392 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:47:51,393 : INFO : EPOCH - 2 : training on 3681650 raw words (2916610 effective words) took 9.2s, 318565 effective words/s\n",
      "2020-03-12 21:47:52,402 : INFO : EPOCH 3 - PROGRESS: at 10.60% examples, 307463 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:53,406 : INFO : EPOCH 3 - PROGRESS: at 22.01% examples, 320634 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:54,406 : INFO : EPOCH 3 - PROGRESS: at 33.19% examples, 322528 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:55,410 : INFO : EPOCH 3 - PROGRESS: at 44.47% examples, 323450 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:56,414 : INFO : EPOCH 3 - PROGRESS: at 55.90% examples, 325353 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:47:57,429 : INFO : EPOCH 3 - PROGRESS: at 67.36% examples, 325716 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:47:58,459 : INFO : EPOCH 3 - PROGRESS: at 79.15% examples, 326443 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:47:59,499 : INFO : EPOCH 3 - PROGRESS: at 93.67% examples, 337012 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:47:59,873 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:47:59,874 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:47:59,903 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:47:59,904 : INFO : EPOCH - 3 : training on 3681650 raw words (2916373 effective words) took 8.5s, 342825 effective words/s\n",
      "2020-03-12 21:48:00,918 : INFO : EPOCH 4 - PROGRESS: at 10.36% examples, 298539 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:48:01,920 : INFO : EPOCH 4 - PROGRESS: at 20.92% examples, 304654 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:02,946 : INFO : EPOCH 4 - PROGRESS: at 31.84% examples, 306850 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:03,965 : INFO : EPOCH 4 - PROGRESS: at 43.34% examples, 312328 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:04,978 : INFO : EPOCH 4 - PROGRESS: at 54.60% examples, 314307 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:06,000 : INFO : EPOCH 4 - PROGRESS: at 65.98% examples, 316251 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:48:07,048 : INFO : EPOCH 4 - PROGRESS: at 78.15% examples, 318620 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:48:08,096 : INFO : EPOCH 4 - PROGRESS: at 89.93% examples, 320256 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:08,916 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:48:08,959 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:48:08,970 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:48:08,971 : INFO : EPOCH - 4 : training on 3681650 raw words (2916311 effective words) took 9.1s, 321817 effective words/s\n",
      "2020-03-12 21:48:09,978 : INFO : EPOCH 5 - PROGRESS: at 11.15% examples, 323860 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:48:10,988 : INFO : EPOCH 5 - PROGRESS: at 22.29% examples, 323763 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:48:12,053 : INFO : EPOCH 5 - PROGRESS: at 33.48% examples, 317940 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:13,104 : INFO : EPOCH 5 - PROGRESS: at 44.73% examples, 316329 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:14,146 : INFO : EPOCH 5 - PROGRESS: at 55.90% examples, 315717 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:48:15,169 : INFO : EPOCH 5 - PROGRESS: at 67.10% examples, 316080 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:16,183 : INFO : EPOCH 5 - PROGRESS: at 78.34% examples, 316720 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:17,185 : INFO : EPOCH 5 - PROGRESS: at 89.90% examples, 319474 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:18,063 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:48:18,104 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:48:18,111 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:48:18,112 : INFO : EPOCH - 5 : training on 3681650 raw words (2916834 effective words) took 9.1s, 319255 effective words/s\n",
      "2020-03-12 21:48:18,112 : INFO : training on a 18408250 raw words (14582381 effective words) took 45.2s, 322820 effective words/s\n",
      "2020-03-12 21:48:18,121 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  mathematics\n",
      "[('literacy', 0.7666188478469849), ('algebra', 0.7601535320281982), ('science', 0.7597874402999878), ('majoring', 0.7580490708351135), ('pedagogy', 0.7315536737442017), ('humanities', 0.7310315370559692), ('discipline', 0.7301062941551208), ('profession', 0.7285811901092529), ('introductory', 0.7223597764968872), ('collegiate', 0.7176905274391174)]\n",
      "Most similar to:  console\n",
      "[('attachments', 0.9332625269889832), ('interferometers', 0.9270851612091064), ('EBSD', 0.9261435270309448), ('melter', 0.9259473085403442), ('FFS', 0.9237853288650513), ('installing', 0.9235818982124329), ('ankle', 0.9231774210929871), ('PSA', 0.9216408729553223), ('CL', 0.9180670976638794), ('flex', 0.9177895784378052)]\n",
      "Most similar to:  spring\n",
      "[('austral', 0.8871985673904419), ('winter', 0.8050752878189087), ('dives', 0.7881208658218384), ('interglacial', 0.7875773906707764), ('season', 0.7783920764923096), ('BATS', 0.7748808264732361), ('downwelling', 0.7726856470108032), ('brief', 0.7722377777099609), ('upwelling', 0.7704768180847168), ('LGM', 0.7661758661270142)]\n",
      "Most similar to:  technology\n",
      "[('technologies', 0.7973124980926514), ('proffered', 0.7254695892333984), ('avionics', 0.7244876623153687), ('doors', 0.7186490297317505), ('biotechnology', 0.714654266834259), ('nanosystems', 0.7089239954948425), ('competencies', 0.7064351439476013), ('assistive', 0.7051752805709839), ('electronics', 0.7019164562225342), ('tomorrow', 0.7002787590026855)]\n",
      "Most similar to:  communication\n",
      "[('communications', 0.8027662634849548), ('wired', 0.7914907932281494), ('wireless', 0.781848669052124), ('networking', 0.7578418850898743), ('LANs', 0.7531979084014893), ('interconnection', 0.7502541542053223), ('latencies', 0.7427887916564941), ('reconfigure', 0.7304162979125977), ('virtualization', 0.7229990363121033), ('caching', 0.7222534418106079)]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# test with size (int), window(int), min_count(int), iter(int), sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text, sg=1)\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### negative = 5, 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 21:48:18,141 : INFO : collecting all words and their counts\n",
      "2020-03-12 21:48:18,142 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 21:48:18,681 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 21:48:18,682 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 21:48:18,765 : INFO : effective_min_count=5 retains 27041 unique words (23% of original 113911, drops 86870)\n",
      "2020-03-12 21:48:18,766 : INFO : effective_min_count=5 leaves 3552589 word corpus (96% of original 3681650, drops 129061)\n",
      "2020-03-12 21:48:18,835 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 21:48:18,838 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 21:48:18,839 : INFO : downsampling leaves estimated 2916612 word corpus (82.1% of prior 3552589)\n",
      "2020-03-12 21:48:18,902 : INFO : estimated required memory for 27041 words and 100 dimensions: 35153300 bytes\n",
      "2020-03-12 21:48:18,903 : INFO : resetting layer weights\n",
      "2020-03-12 21:48:22,834 : INFO : training model with 3 workers on 27041 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 21:48:23,853 : INFO : EPOCH 1 - PROGRESS: at 46.06% examples, 1326264 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:48:24,855 : INFO : EPOCH 1 - PROGRESS: at 84.68% examples, 1224341 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:25,235 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:48:25,245 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:48:25,249 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:48:25,249 : INFO : EPOCH - 1 : training on 3681650 raw words (2916872 effective words) took 2.4s, 1210079 effective words/s\n",
      "2020-03-12 21:48:26,260 : INFO : EPOCH 2 - PROGRESS: at 39.05% examples, 1135138 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:27,269 : INFO : EPOCH 2 - PROGRESS: at 78.39% examples, 1132345 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:27,807 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:48:27,817 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:48:27,820 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:48:27,821 : INFO : EPOCH - 2 : training on 3681650 raw words (2916564 effective words) took 2.6s, 1136203 effective words/s\n",
      "2020-03-12 21:48:28,836 : INFO : EPOCH 3 - PROGRESS: at 39.32% examples, 1137811 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:29,842 : INFO : EPOCH 3 - PROGRESS: at 79.18% examples, 1143881 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:30,351 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:48:30,362 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:48:30,365 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:48:30,365 : INFO : EPOCH - 3 : training on 3681650 raw words (2917051 effective words) took 2.5s, 1148776 effective words/s\n",
      "2020-03-12 21:48:31,383 : INFO : EPOCH 4 - PROGRESS: at 39.32% examples, 1138319 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:48:32,384 : INFO : EPOCH 4 - PROGRESS: at 83.66% examples, 1211573 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:32,788 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:48:32,798 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:48:32,801 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:48:32,801 : INFO : EPOCH - 4 : training on 3681650 raw words (2916229 effective words) took 2.4s, 1201104 effective words/s\n",
      "2020-03-12 21:48:33,810 : INFO : EPOCH 5 - PROGRESS: at 51.15% examples, 1484308 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:34,817 : INFO : EPOCH 5 - PROGRESS: at 90.97% examples, 1318861 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:35,024 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:48:35,029 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:48:35,033 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:48:35,033 : INFO : EPOCH - 5 : training on 3681650 raw words (2916303 effective words) took 2.2s, 1308743 effective words/s\n",
      "2020-03-12 21:48:35,034 : INFO : training on a 18408250 raw words (14583019 effective words) took 12.2s, 1195414 effective words/s\n",
      "2020-03-12 21:48:35,042 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  mathematics\n",
      "[('science', 0.8271150588989258), ('engineering', 0.7937756776809692), ('physics', 0.7783791422843933), ('discipline', 0.7678360939025879), ('sciences', 0.7533905506134033), ('practice', 0.7275657057762146), ('majors', 0.7274656295776367), ('nanotechnology', 0.7078384160995483), ('concepts', 0.7015020251274109), ('teachers', 0.6945022344589233)]\n",
      "Most similar to:  console\n",
      "[('enlarged', 0.8307855129241943), ('airplane', 0.8245298266410828), ('peroxide', 0.8241102695465088), ('adversary', 0.810947597026825), ('acrylate', 0.8101605176925659), ('intermediary', 0.8079272508621216), ('PEB', 0.8065509796142578), ('en', 0.8046492338180542), ('GDP', 0.800044596195221), ('chirped', 0.7992343902587891)]\n",
      "Most similar to:  spring\n",
      "[('meridional', 0.7816545963287354), ('ha', 0.7276901006698608), ('Peru', 0.7264009118080139), ('late', 0.7252197265625), ('42', 0.718497097492218), ('north', 0.7100715041160583), ('Tertiary', 0.7027775049209595), ('southern', 0.7023163437843323), ('Miocene', 0.6978045701980591), ('northwest', 0.6954830884933472)]\n",
      "Most similar to:  technology\n",
      "[('technologies', 0.7926293611526489), ('vision', 0.7246078848838806), ('manufacturing', 0.7147825956344604), ('standards', 0.7075295448303223), ('innovation', 0.6999636888504028), ('biotechnology', 0.69352787733078), ('emerging', 0.6934373378753662), ('technological', 0.6890543103218079), ('nanotechnology', 0.6875382661819458), ('industry', 0.6853565573692322)]\n",
      "Most similar to:  communication\n",
      "[('networking', 0.8663899898529053), ('transportation', 0.8379961848258972), ('security', 0.8305901885032654), ('wireless', 0.8269981145858765), ('communications', 0.8188385963439941), ('networks', 0.8046975135803223), ('virtual', 0.8011627197265625), ('distributed', 0.7977383136749268), ('secure', 0.7910494804382324), ('intelligence', 0.7906814217567444)]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# test with size (int), window(int), min_count(int), iter(int), sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text, negative=5)\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 21:48:35,064 : INFO : collecting all words and their counts\n",
      "2020-03-12 21:48:35,064 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 21:48:35,594 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 21:48:35,595 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 21:48:35,919 : INFO : effective_min_count=5 retains 27041 unique words (23% of original 113911, drops 86870)\n",
      "2020-03-12 21:48:35,920 : INFO : effective_min_count=5 leaves 3552589 word corpus (96% of original 3681650, drops 129061)\n",
      "2020-03-12 21:48:35,986 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 21:48:35,989 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 21:48:35,989 : INFO : downsampling leaves estimated 2916612 word corpus (82.1% of prior 3552589)\n",
      "2020-03-12 21:48:36,045 : INFO : estimated required memory for 27041 words and 100 dimensions: 35153300 bytes\n",
      "2020-03-12 21:48:36,046 : INFO : resetting layer weights\n",
      "2020-03-12 21:48:40,146 : INFO : training model with 3 workers on 27041 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=20 window=5\n",
      "2020-03-12 21:48:41,161 : INFO : EPOCH 1 - PROGRESS: at 22.84% examples, 660764 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:42,173 : INFO : EPOCH 1 - PROGRESS: at 42.32% examples, 611555 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:43,206 : INFO : EPOCH 1 - PROGRESS: at 58.85% examples, 562634 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:44,211 : INFO : EPOCH 1 - PROGRESS: at 74.85% examples, 537711 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:45,228 : INFO : EPOCH 1 - PROGRESS: at 91.54% examples, 525919 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:45,697 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:48:45,701 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:48:45,725 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:48:45,725 : INFO : EPOCH - 1 : training on 3681650 raw words (2917540 effective words) took 5.6s, 523424 effective words/s\n",
      "2020-03-12 21:48:46,740 : INFO : EPOCH 2 - PROGRESS: at 16.96% examples, 491336 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:47,740 : INFO : EPOCH 2 - PROGRESS: at 33.98% examples, 494186 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:48:48,741 : INFO : EPOCH 2 - PROGRESS: at 50.88% examples, 492956 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:48:49,747 : INFO : EPOCH 2 - PROGRESS: at 68.10% examples, 494871 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:48:50,775 : INFO : EPOCH 2 - PROGRESS: at 85.72% examples, 495305 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:51,560 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:48:51,587 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:48:51,595 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:48:51,596 : INFO : EPOCH - 2 : training on 3681650 raw words (2916793 effective words) took 5.9s, 497212 effective words/s\n",
      "2020-03-12 21:48:52,607 : INFO : EPOCH 3 - PROGRESS: at 20.65% examples, 600785 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:53,637 : INFO : EPOCH 3 - PROGRESS: at 40.66% examples, 584061 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:54,641 : INFO : EPOCH 3 - PROGRESS: at 56.96% examples, 547039 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:55,651 : INFO : EPOCH 3 - PROGRESS: at 73.72% examples, 530918 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:56,678 : INFO : EPOCH 3 - PROGRESS: at 90.97% examples, 522537 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:57,183 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:48:57,201 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:48:57,215 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:48:57,216 : INFO : EPOCH - 3 : training on 3681650 raw words (2916678 effective words) took 5.6s, 519405 effective words/s\n",
      "2020-03-12 21:48:58,240 : INFO : EPOCH 4 - PROGRESS: at 16.13% examples, 463320 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:48:59,280 : INFO : EPOCH 4 - PROGRESS: at 33.46% examples, 474941 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:49:00,297 : INFO : EPOCH 4 - PROGRESS: at 50.10% examples, 474748 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:49:01,316 : INFO : EPOCH 4 - PROGRESS: at 70.82% examples, 504354 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:49:02,321 : INFO : EPOCH 4 - PROGRESS: at 88.34% examples, 504937 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:49:02,977 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:49:02,999 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:49:03,010 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:49:03,011 : INFO : EPOCH - 4 : training on 3681650 raw words (2916407 effective words) took 5.8s, 503598 effective words/s\n",
      "2020-03-12 21:49:04,020 : INFO : EPOCH 5 - PROGRESS: at 23.36% examples, 679520 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:49:05,026 : INFO : EPOCH 5 - PROGRESS: at 47.15% examples, 683855 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:49:06,045 : INFO : EPOCH 5 - PROGRESS: at 71.08% examples, 684336 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:49:07,049 : INFO : EPOCH 5 - PROGRESS: at 94.45% examples, 682719 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:49:07,253 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:49:07,275 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:49:07,278 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:49:07,279 : INFO : EPOCH - 5 : training on 3681650 raw words (2916286 effective words) took 4.3s, 684101 effective words/s\n",
      "2020-03-12 21:49:07,279 : INFO : training on a 18408250 raw words (14583704 effective words) took 27.1s, 537513 effective words/s\n",
      "2020-03-12 21:49:07,287 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  mathematics\n",
      "[('discipline', 0.7327420711517334), ('engineering', 0.7312050461769104), ('majors', 0.7304700613021851), ('concepts', 0.6940078139305115), ('physics', 0.6881793737411499), ('science', 0.6670185923576355), ('literacy', 0.6650533676147461), ('careers', 0.6524648666381836), ('instruction', 0.649006187915802), ('teachers', 0.6455676555633545)]\n",
      "Most similar to:  console\n",
      "[('ankle', 0.9394474029541016), ('airfoil', 0.9373698830604553), ('chirped', 0.9349576830863953), ('NbTi', 0.9348865747451782), ('Achilles', 0.9297915101051331), ('attenuated', 0.9260783791542053), ('modulator', 0.9256703853607178), ('undocumented', 0.9254209995269775), ('sorbent', 0.925312340259552), ('SEC', 0.9253010153770447)]\n",
      "Most similar to:  spring\n",
      "[('austral', 0.7583223581314087), ('creeping', 0.7536946535110474), ('winter', 0.7472171783447266), ('late', 0.7462772130966187), ('downwelling', 0.7435829639434814), ('inception', 0.7423288226127625), ('BC', 0.7378535866737366), ('Tertiary', 0.7362013459205627), ('interglacial', 0.7348266243934631), ('Miocene', 0.7324913144111633)]\n",
      "Most similar to:  technology\n",
      "[('technologies', 0.7678936719894409), ('manufacturing', 0.6396459341049194), ('innovation', 0.6356807947158813), ('technological', 0.6351584792137146), ('nanotechnology', 0.6346638798713684), ('paradigm', 0.625953197479248), ('infrastructure', 0.6227245926856995), ('electronics', 0.6118147969245911), ('device', 0.6108677387237549), ('standards', 0.6045964360237122)]\n",
      "Most similar to:  communication\n",
      "[('networking', 0.835072934627533), ('wireless', 0.8207738995552063), ('communications', 0.8083603382110596), ('transportation', 0.7922121286392212), ('security', 0.7780587077140808), ('secure', 0.7624005079269409), ('mobile', 0.7589998245239258), ('computing', 0.7464074492454529), ('embedded', 0.7406396269798279), ('networks', 0.7397851943969727)]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# test with size (int), window(int), min_count(int), iter(int), sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text, negative=20)\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window = 3, 10\n",
    "\n",
    "On its own, windows doesn't seem more interesting than the others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 21:49:07,308 : INFO : collecting all words and their counts\n",
      "2020-03-12 21:49:07,309 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 21:49:07,847 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 21:49:07,848 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 21:49:07,929 : INFO : effective_min_count=5 retains 27041 unique words (23% of original 113911, drops 86870)\n",
      "2020-03-12 21:49:07,929 : INFO : effective_min_count=5 leaves 3552589 word corpus (96% of original 3681650, drops 129061)\n",
      "2020-03-12 21:49:07,992 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 21:49:07,994 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 21:49:07,995 : INFO : downsampling leaves estimated 2916612 word corpus (82.1% of prior 3552589)\n",
      "2020-03-12 21:49:08,051 : INFO : estimated required memory for 27041 words and 100 dimensions: 35153300 bytes\n",
      "2020-03-12 21:49:08,052 : INFO : resetting layer weights\n",
      "2020-03-12 21:49:12,000 : INFO : training model with 3 workers on 27041 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
      "2020-03-12 21:49:13,016 : INFO : EPOCH 1 - PROGRESS: at 42.32% examples, 1222757 words/s, in_qsize 5, out_qsize 1\n",
      "2020-03-12 21:49:14,027 : INFO : EPOCH 1 - PROGRESS: at 85.47% examples, 1232163 words/s, in_qsize 6, out_qsize 2\n",
      "2020-03-12 21:49:14,341 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:49:14,350 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:49:14,354 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:49:14,355 : INFO : EPOCH - 1 : training on 3681650 raw words (2916137 effective words) took 2.3s, 1240959 effective words/s\n",
      "2020-03-12 21:49:15,375 : INFO : EPOCH 2 - PROGRESS: at 40.68% examples, 1169711 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:49:16,311 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:49:16,314 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:49:16,317 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:49:16,318 : INFO : EPOCH - 2 : training on 3681650 raw words (2916731 effective words) took 2.0s, 1489336 effective words/s\n",
      "2020-03-12 21:49:17,328 : INFO : EPOCH 3 - PROGRESS: at 44.43% examples, 1296888 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:49:18,337 : INFO : EPOCH 3 - PROGRESS: at 87.83% examples, 1274232 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:49:18,603 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:49:18,612 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:49:18,617 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:49:18,617 : INFO : EPOCH - 3 : training on 3681650 raw words (2917460 effective words) took 2.3s, 1273752 effective words/s\n",
      "2020-03-12 21:49:19,625 : INFO : EPOCH 4 - PROGRESS: at 42.80% examples, 1247803 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:49:20,628 : INFO : EPOCH 4 - PROGRESS: at 86.01% examples, 1249214 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:49:20,940 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:49:20,941 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:49:20,944 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:49:20,944 : INFO : EPOCH - 4 : training on 3681650 raw words (2916784 effective words) took 2.3s, 1255844 effective words/s\n",
      "2020-03-12 21:49:21,951 : INFO : EPOCH 5 - PROGRESS: at 42.80% examples, 1248406 words/s, in_qsize 5, out_qsize 1\n",
      "2020-03-12 21:49:22,956 : INFO : EPOCH 5 - PROGRESS: at 85.47% examples, 1240695 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:49:23,291 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:49:23,300 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:49:23,303 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:49:23,304 : INFO : EPOCH - 5 : training on 3681650 raw words (2916692 effective words) took 2.4s, 1238381 effective words/s\n",
      "2020-03-12 21:49:23,304 : INFO : training on a 18408250 raw words (14583804 effective words) took 11.3s, 1290216 effective words/s\n",
      "2020-03-12 21:49:23,312 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  mathematics\n",
      "[('engineering', 0.7921909093856812), ('science', 0.7879029512405396), ('discipline', 0.7671724557876587), ('nanotechnology', 0.7544541358947754), ('literacy', 0.7493030428886414), ('practice', 0.7444681525230408), ('sciences', 0.7409480810165405), ('statistics', 0.739732563495636), ('physics', 0.7288577556610107), ('bioinformatics', 0.7244324088096619)]\n",
      "Most similar to:  console\n",
      "[('interferometer', 0.8811743259429932), ('inhibitor', 0.8769178986549377), ('epithelium', 0.8633572459220886), ('fibrosis', 0.8618849515914917), ('gabbro', 0.8556831479072571), ('Aspect', 0.8556816577911377), ('peroxide', 0.8534140586853027), ('photodetector', 0.8525289297103882), ('sonoluminescence', 0.852136492729187), ('SSV', 0.8492079973220825)]\n",
      "Most similar to:  spring\n",
      "[('late', 0.7845708131790161), ('winter', 0.7706813216209412), ('interglacial', 0.7648404240608215), ('Tertiary', 0.7565097808837891), ('20th', 0.7413555383682251), ('24', 0.7385187149047852), ('shrublands', 0.7322454452514648), ('Classic', 0.7285686731338501), ('west', 0.718942403793335), ('Spain', 0.7167454957962036)]\n",
      "Most similar to:  technology\n",
      "[('technologies', 0.7618275880813599), ('standards', 0.7081260681152344), ('company', 0.6974826455116272), ('infrastructure', 0.690550684928894), ('nanotechnology', 0.676532506942749), ('vision', 0.6687910556793213), ('industry', 0.6619904041290283), ('curricula', 0.6615157127380371), ('assurance', 0.6527606248855591), ('biotechnology', 0.6514216065406799)]\n",
      "Most similar to:  communication\n",
      "[('networking', 0.860970139503479), ('security', 0.8540621995925903), ('communications', 0.8228682279586792), ('transportation', 0.8205509185791016), ('intelligence', 0.8063579797744751), ('wireless', 0.8028706908226013), ('personal', 0.7963840961456299), ('management', 0.7940849661827087), ('allocation', 0.7928149700164795), ('computing', 0.7820618152618408)]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# test with size (int), window(int), min_count(int), iter(int), sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text, window=3)\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 21:49:23,334 : INFO : collecting all words and their counts\n",
      "2020-03-12 21:49:23,334 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 21:49:23,887 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 21:49:23,888 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 21:49:24,189 : INFO : effective_min_count=5 retains 27041 unique words (23% of original 113911, drops 86870)\n",
      "2020-03-12 21:49:24,190 : INFO : effective_min_count=5 leaves 3552589 word corpus (96% of original 3681650, drops 129061)\n",
      "2020-03-12 21:49:24,261 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 21:49:24,264 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 21:49:24,265 : INFO : downsampling leaves estimated 2916612 word corpus (82.1% of prior 3552589)\n",
      "2020-03-12 21:49:24,321 : INFO : estimated required memory for 27041 words and 100 dimensions: 35153300 bytes\n",
      "2020-03-12 21:49:24,322 : INFO : resetting layer weights\n",
      "2020-03-12 21:49:28,276 : INFO : training model with 3 workers on 27041 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2020-03-12 21:49:29,288 : INFO : EPOCH 1 - PROGRESS: at 42.80% examples, 1241913 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:49:30,302 : INFO : EPOCH 1 - PROGRESS: at 78.90% examples, 1137114 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:49:30,863 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:49:30,865 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:49:30,873 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:49:30,874 : INFO : EPOCH - 1 : training on 3681650 raw words (2916758 effective words) took 2.6s, 1124786 effective words/s\n",
      "2020-03-12 21:49:31,888 : INFO : EPOCH 2 - PROGRESS: at 40.40% examples, 1169266 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:49:32,900 : INFO : EPOCH 2 - PROGRESS: at 77.61% examples, 1117383 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:49:33,501 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:49:33,511 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:49:33,516 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:49:33,517 : INFO : EPOCH - 2 : training on 3681650 raw words (2916470 effective words) took 2.6s, 1105045 effective words/s\n",
      "2020-03-12 21:49:34,532 : INFO : EPOCH 3 - PROGRESS: at 36.66% examples, 1060409 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:49:35,538 : INFO : EPOCH 3 - PROGRESS: at 73.16% examples, 1058698 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:49:36,259 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:49:36,270 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:49:36,275 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:49:36,276 : INFO : EPOCH - 3 : training on 3681650 raw words (2916272 effective words) took 2.8s, 1058664 effective words/s\n",
      "2020-03-12 21:49:37,285 : INFO : EPOCH 4 - PROGRESS: at 35.84% examples, 1043837 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:49:38,287 : INFO : EPOCH 4 - PROGRESS: at 72.12% examples, 1048892 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:49:39,050 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:49:39,061 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:49:39,066 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:49:39,067 : INFO : EPOCH - 4 : training on 3681650 raw words (2917956 effective words) took 2.8s, 1047150 effective words/s\n",
      "2020-03-12 21:49:40,082 : INFO : EPOCH 5 - PROGRESS: at 43.91% examples, 1277858 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:49:41,084 : INFO : EPOCH 5 - PROGRESS: at 80.74% examples, 1173085 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:49:41,452 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:49:41,462 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:49:41,464 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:49:41,465 : INFO : EPOCH - 5 : training on 3681650 raw words (2916431 effective words) took 2.4s, 1222323 effective words/s\n",
      "2020-03-12 21:49:41,465 : INFO : training on a 18408250 raw words (14583887 effective words) took 13.2s, 1105809 effective words/s\n",
      "2020-03-12 21:49:41,474 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  mathematics\n",
      "[('science', 0.7861557602882385), ('discipline', 0.765587329864502), ('elementary', 0.7522879838943481), ('engineering', 0.7503076791763306), ('physics', 0.73476642370224), ('majors', 0.7052773237228394), ('sciences', 0.7045080661773682), ('mathematical', 0.6979876756668091), ('physicists', 0.6924373507499695), ('subjects', 0.6894881725311279)]\n",
      "Most similar to:  console\n",
      "[('fluorometer', 0.855572521686554), ('AES', 0.8468203544616699), ('12CO2', 0.8442041873931885), ('cabinetry', 0.8427667021751404), ('apartment', 0.8338295221328735), ('quadrapole', 0.8321656584739685), ('couplers', 0.8253879547119141), ('collector', 0.824892520904541), ('Actuator', 0.8238655924797058), ('GaMnN', 0.8224331736564636)]\n",
      "Most similar to:  spring\n",
      "[('meridional', 0.7854409217834473), ('unmapped', 0.7782730460166931), ('cruises', 0.7611277103424072), ('winter', 0.759924054145813), ('late', 0.7491304278373718), ('1999', 0.7395031452178955), ('monsoon', 0.7293879985809326), ('Peru', 0.7216346859931946), ('Hell', 0.7209315299987793), ('Peninsula', 0.7202963829040527)]\n",
      "Most similar to:  technology\n",
      "[('technologies', 0.80028235912323), ('manufacturing', 0.7667759656906128), ('emerging', 0.7268421649932861), ('commercialization', 0.7186828851699829), ('innovation', 0.7110608816146851), ('electronics', 0.7085988521575928), ('microelectronics', 0.7085675001144409), ('communications', 0.7077606916427612), ('industry', 0.7072857618331909), ('technological', 0.7071331739425659)]\n",
      "Most similar to:  communication\n",
      "[('wireless', 0.8812304735183716), ('security', 0.8751596808433533), ('networks', 0.8652626276016235), ('networking', 0.8502070903778076), ('communications', 0.8381949663162231), ('computing', 0.820081353187561), ('intelligence', 0.8198156952857971), ('transportation', 0.8184633255004883), ('routing', 0.8137835264205933), ('ad', 0.8099515438079834)]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# test with size (int), window(int), min_count(int), iter(int), sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text, window=10)\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results for 2a.\n",
    "\n",
    "First off; spring was apparently a terrible word to choose since there is nothing that seems to be close to it, winter does pop up occasionally but for all settings it feels kind of random what it picks to be similar.\n",
    "\n",
    "For me the most interesting variables is the skipgram. Incresing iterations in this scenario also felt valuable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing 2002 vs full abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_abstracts():  \n",
    "    with zipfile.ZipFile(\"abstracts.zip\", \"r\") as file:\n",
    "        file.extractall()\n",
    "        print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_files(dir):\n",
    "    \n",
    "    paths =  []\n",
    "    \n",
    "    for dirpath, dirnames, filenames in os.walk(dir):\n",
    "        for filename in [f for f in filenames if f.endswith(\".txt\")]:\n",
    "            \n",
    "            path = os.path.join(dirpath, filename)\n",
    "            paths.append(path)\n",
    "            \n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = find_all_files(\"abstracts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_docu = []\n",
    "\n",
    "for path in paths:\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        full_docu.append(f.read())\n",
    "        \n",
    "random.shuffle(full_docu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_words = [\"mathematics\", \"console\", \"spring\", \"technology\", \"communication\"]\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "word_tokenizer = tfidf2_vectorizer.build_tokenizer()\n",
    "tokenized_text_base = [word_tokenizer(doc) for doc in documents]\n",
    "tokenized_text_full = [word_tokenizer(doc) for doc in full_docu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 21:51:42,833 : INFO : collecting all words and their counts\n",
      "2020-03-12 21:51:42,837 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 21:51:43,905 : INFO : collected 113911 word types from a corpus of 3681650 raw words and 9923 sentences\n",
      "2020-03-12 21:51:43,906 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 21:51:43,998 : INFO : effective_min_count=5 retains 27041 unique words (23% of original 113911, drops 86870)\n",
      "2020-03-12 21:51:43,998 : INFO : effective_min_count=5 leaves 3552589 word corpus (96% of original 3681650, drops 129061)\n",
      "2020-03-12 21:51:44,062 : INFO : deleting the raw counts dictionary of 113911 items\n",
      "2020-03-12 21:51:44,065 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2020-03-12 21:51:44,066 : INFO : downsampling leaves estimated 2916612 word corpus (82.1% of prior 3552589)\n",
      "2020-03-12 21:51:44,121 : INFO : estimated required memory for 27041 words and 100 dimensions: 35153300 bytes\n",
      "2020-03-12 21:51:44,121 : INFO : resetting layer weights\n",
      "2020-03-12 21:51:48,096 : INFO : training model with 3 workers on 27041 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 21:51:49,117 : INFO : EPOCH 1 - PROGRESS: at 48.23% examples, 1395595 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:51:50,138 : INFO : EPOCH 1 - PROGRESS: at 85.76% examples, 1231287 words/s, in_qsize 4, out_qsize 2\n",
      "2020-03-12 21:51:50,495 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:51:50,504 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:51:50,509 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:51:50,510 : INFO : EPOCH - 1 : training on 3681650 raw words (2916182 effective words) took 2.4s, 1214795 effective words/s\n",
      "2020-03-12 21:51:51,522 : INFO : EPOCH 2 - PROGRESS: at 36.66% examples, 1069818 words/s, in_qsize 5, out_qsize 2\n",
      "2020-03-12 21:51:52,523 : INFO : EPOCH 2 - PROGRESS: at 75.09% examples, 1093547 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:51:53,185 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:51:53,188 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:51:53,190 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:51:53,190 : INFO : EPOCH - 2 : training on 3681650 raw words (2916898 effective words) took 2.7s, 1092425 effective words/s\n",
      "2020-03-12 21:51:54,202 : INFO : EPOCH 3 - PROGRESS: at 46.35% examples, 1341418 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:51:55,206 : INFO : EPOCH 3 - PROGRESS: at 87.83% examples, 1272537 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:51:55,511 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:51:55,522 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:51:55,525 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:51:55,525 : INFO : EPOCH - 3 : training on 3681650 raw words (2916133 effective words) took 2.3s, 1250950 effective words/s\n",
      "2020-03-12 21:51:56,551 : INFO : EPOCH 4 - PROGRESS: at 37.48% examples, 1081361 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:51:57,559 : INFO : EPOCH 4 - PROGRESS: at 75.09% examples, 1083153 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:51:58,203 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:51:58,213 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:51:58,217 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:51:58,217 : INFO : EPOCH - 4 : training on 3681650 raw words (2916022 effective words) took 2.7s, 1088423 effective words/s\n",
      "2020-03-12 21:51:59,222 : INFO : EPOCH 5 - PROGRESS: at 48.81% examples, 1422408 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:52:00,232 : INFO : EPOCH 5 - PROGRESS: at 87.03% examples, 1262320 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:52:00,558 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:52:00,568 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:52:00,571 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:52:00,572 : INFO : EPOCH - 5 : training on 3681650 raw words (2917046 effective words) took 2.4s, 1241168 effective words/s\n",
      "2020-03-12 21:52:00,573 : INFO : training on a 18408250 raw words (14582281 effective words) took 12.5s, 1168872 effective words/s\n",
      "2020-03-12 21:52:01,428 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  mathematics\n",
      "[('science', 0.8101097345352173), ('engineering', 0.8010798692703247), ('discipline', 0.7948856353759766), ('physics', 0.747256875038147), ('practice', 0.7466083765029907), ('sciences', 0.7356523871421814), ('majors', 0.7277352809906006), ('literacy', 0.7165915966033936), ('concepts', 0.703219473361969), ('mathematical', 0.6995234489440918)]\n",
      "Most similar to:  console\n",
      "[('chirped', 0.9077774286270142), ('0017', 0.8973997831344604), ('AES', 0.8972253799438477), ('steerable', 0.8968645334243774), ('kN', 0.8956530690193176), ('NbTi', 0.8947874307632446), ('Eastmark', 0.8940984606742859), ('microtomography', 0.8940542936325073), ('01063', 0.8935415148735046), ('PB', 0.8926355838775635)]\n",
      "Most similar to:  spring\n",
      "[('late', 0.7497243881225586), ('interglacial', 0.744472861289978), ('austral', 0.7415764331817627), ('winter', 0.7167822122573853), ('inception', 0.7161902189254761), ('meridional', 0.7069535851478577), ('Miocene', 0.7060791850090027), ('23', 0.7051582336425781), ('Paleozoic', 0.703187108039856), ('season', 0.7024444937705994)]\n",
      "Most similar to:  technology\n",
      "[('technologies', 0.793857216835022), ('innovation', 0.7008261680603027), ('emerging', 0.6988540291786194), ('manufacturing', 0.6938198804855347), ('nanotechnology', 0.6848446726799011), ('industry', 0.6747710704803467), ('vision', 0.6745443344116211), ('assurance', 0.6711281538009644), ('company', 0.6699226498603821), ('biotechnology', 0.6659247875213623)]\n",
      "Most similar to:  communication\n",
      "[('security', 0.8493036031723022), ('networking', 0.8457667231559753), ('transportation', 0.8334161043167114), ('wireless', 0.8257659673690796), ('communications', 0.8250710964202881), ('computing', 0.8138442635536194), ('sharing', 0.787452220916748), ('secure', 0.7858853340148926), ('networks', 0.7852212190628052), ('client', 0.782680869102478)]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# test with size, window, min_count, iter, sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text_base)\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 21:52:01,585 : INFO : collecting all words and their counts\n",
      "2020-03-12 21:52:01,586 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 21:52:02,420 : INFO : PROGRESS: at sentence #10000, processed 3096593 words, keeping 110456 word types\n",
      "2020-03-12 21:52:03,277 : INFO : PROGRESS: at sentence #20000, processed 6188649 words, keeping 172944 word types\n",
      "2020-03-12 21:52:04,263 : INFO : PROGRESS: at sentence #30000, processed 9295825 words, keeping 226132 word types\n",
      "2020-03-12 21:52:05,450 : INFO : PROGRESS: at sentence #40000, processed 12380992 words, keeping 274592 word types\n",
      "2020-03-12 21:52:06,497 : INFO : PROGRESS: at sentence #50000, processed 15482098 words, keeping 319960 word types\n",
      "2020-03-12 21:52:07,875 : INFO : PROGRESS: at sentence #60000, processed 18571217 words, keeping 362506 word types\n",
      "2020-03-12 21:52:09,155 : INFO : PROGRESS: at sentence #70000, processed 21671727 words, keeping 403470 word types\n",
      "2020-03-12 21:52:10,382 : INFO : PROGRESS: at sentence #80000, processed 24735431 words, keeping 442709 word types\n",
      "2020-03-12 21:52:11,590 : INFO : PROGRESS: at sentence #90000, processed 27848205 words, keeping 480969 word types\n",
      "2020-03-12 21:52:12,770 : INFO : PROGRESS: at sentence #100000, processed 30960551 words, keeping 518207 word types\n",
      "2020-03-12 21:52:13,928 : INFO : PROGRESS: at sentence #110000, processed 34069558 words, keeping 554545 word types\n",
      "2020-03-12 21:52:15,032 : INFO : PROGRESS: at sentence #120000, processed 37171368 words, keeping 589715 word types\n",
      "2020-03-12 21:52:15,933 : INFO : PROGRESS: at sentence #130000, processed 40270270 words, keeping 624592 word types\n",
      "2020-03-12 21:52:16,093 : INFO : collected 632794 word types from a corpus of 41001976 raw words and 132372 sentences\n",
      "2020-03-12 21:52:16,094 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 21:53:23,753 : INFO : effective_min_count=5 retains 101376 unique words (16% of original 632794, drops 531418)\n",
      "2020-03-12 21:53:23,763 : INFO : effective_min_count=5 leaves 40268991 word corpus (98% of original 41001976, drops 732985)\n",
      "2020-03-12 21:53:24,175 : INFO : deleting the raw counts dictionary of 632794 items\n",
      "2020-03-12 21:53:24,236 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2020-03-12 21:53:24,238 : INFO : downsampling leaves estimated 33125457 word corpus (82.3% of prior 40268991)\n",
      "2020-03-12 21:53:24,607 : INFO : estimated required memory for 101376 words and 100 dimensions: 131788800 bytes\n",
      "2020-03-12 21:53:24,608 : INFO : resetting layer weights\n",
      "2020-03-12 21:53:39,819 : INFO : training model with 3 workers on 101376 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 21:53:40,848 : INFO : EPOCH 1 - PROGRESS: at 1.45% examples, 473693 words/s, in_qsize 4, out_qsize 1\n",
      "2020-03-12 21:53:41,851 : INFO : EPOCH 1 - PROGRESS: at 3.57% examples, 580674 words/s, in_qsize 4, out_qsize 2\n",
      "2020-03-12 21:53:42,862 : INFO : EPOCH 1 - PROGRESS: at 5.54% examples, 604359 words/s, in_qsize 3, out_qsize 2\n",
      "2020-03-12 21:53:43,876 : INFO : EPOCH 1 - PROGRESS: at 7.57% examples, 618993 words/s, in_qsize 4, out_qsize 2\n",
      "2020-03-12 21:53:44,888 : INFO : EPOCH 1 - PROGRESS: at 9.62% examples, 626948 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-12 21:53:45,912 : INFO : EPOCH 1 - PROGRESS: at 11.83% examples, 642526 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:53:46,928 : INFO : EPOCH 1 - PROGRESS: at 13.79% examples, 642208 words/s, in_qsize 5, out_qsize 1\n",
      "2020-03-12 21:53:47,931 : INFO : EPOCH 1 - PROGRESS: at 15.63% examples, 638091 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:53:48,934 : INFO : EPOCH 1 - PROGRESS: at 17.54% examples, 637654 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:53:49,941 : INFO : EPOCH 1 - PROGRESS: at 19.37% examples, 633738 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:53:50,950 : INFO : EPOCH 1 - PROGRESS: at 21.14% examples, 629275 words/s, in_qsize 3, out_qsize 2\n",
      "2020-03-12 21:53:51,976 : INFO : EPOCH 1 - PROGRESS: at 23.01% examples, 627773 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:53:52,978 : INFO : EPOCH 1 - PROGRESS: at 24.75% examples, 623344 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:53:53,985 : INFO : EPOCH 1 - PROGRESS: at 26.38% examples, 617179 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:53:54,995 : INFO : EPOCH 1 - PROGRESS: at 28.09% examples, 613222 words/s, in_qsize 4, out_qsize 1\n",
      "2020-03-12 21:53:55,999 : INFO : EPOCH 1 - PROGRESS: at 29.76% examples, 609048 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:53:57,009 : INFO : EPOCH 1 - PROGRESS: at 31.33% examples, 603338 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-12 21:53:58,033 : INFO : EPOCH 1 - PROGRESS: at 32.93% examples, 598587 words/s, in_qsize 4, out_qsize 1\n",
      "2020-03-12 21:53:59,038 : INFO : EPOCH 1 - PROGRESS: at 34.71% examples, 598287 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:54:00,064 : INFO : EPOCH 1 - PROGRESS: at 36.45% examples, 596163 words/s, in_qsize 4, out_qsize 1\n",
      "2020-03-12 21:54:01,088 : INFO : EPOCH 1 - PROGRESS: at 38.02% examples, 592061 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:54:02,101 : INFO : EPOCH 1 - PROGRESS: at 39.72% examples, 590083 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:54:03,117 : INFO : EPOCH 1 - PROGRESS: at 41.13% examples, 584489 words/s, in_qsize 6, out_qsize 2\n",
      "2020-03-12 21:54:04,123 : INFO : EPOCH 1 - PROGRESS: at 42.58% examples, 580215 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:54:05,160 : INFO : EPOCH 1 - PROGRESS: at 43.94% examples, 574339 words/s, in_qsize 6, out_qsize 2\n",
      "2020-03-12 21:54:06,183 : INFO : EPOCH 1 - PROGRESS: at 45.33% examples, 569242 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:54:07,211 : INFO : EPOCH 1 - PROGRESS: at 46.75% examples, 564980 words/s, in_qsize 3, out_qsize 2\n",
      "2020-03-12 21:54:08,219 : INFO : EPOCH 1 - PROGRESS: at 48.26% examples, 562526 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:54:09,225 : INFO : EPOCH 1 - PROGRESS: at 49.65% examples, 558965 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:54:10,262 : INFO : EPOCH 1 - PROGRESS: at 51.16% examples, 556373 words/s, in_qsize 6, out_qsize 2\n",
      "2020-03-12 21:54:11,271 : INFO : EPOCH 1 - PROGRESS: at 52.61% examples, 553913 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:54:12,276 : INFO : EPOCH 1 - PROGRESS: at 53.94% examples, 550209 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-12 21:54:13,302 : INFO : EPOCH 1 - PROGRESS: at 55.12% examples, 544961 words/s, in_qsize 4, out_qsize 2\n",
      "2020-03-12 21:54:14,335 : INFO : EPOCH 1 - PROGRESS: at 56.47% examples, 541570 words/s, in_qsize 6, out_qsize 2\n",
      "2020-03-12 21:54:15,343 : INFO : EPOCH 1 - PROGRESS: at 57.68% examples, 537412 words/s, in_qsize 5, out_qsize 2\n",
      "2020-03-12 21:54:16,365 : INFO : EPOCH 1 - PROGRESS: at 59.00% examples, 534147 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-12 21:54:17,372 : INFO : EPOCH 1 - PROGRESS: at 60.21% examples, 530386 words/s, in_qsize 4, out_qsize 1\n",
      "2020-03-12 21:54:18,375 : INFO : EPOCH 1 - PROGRESS: at 61.42% examples, 526851 words/s, in_qsize 3, out_qsize 2\n",
      "2020-03-12 21:54:19,422 : INFO : EPOCH 1 - PROGRESS: at 62.65% examples, 523110 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:54:20,428 : INFO : EPOCH 1 - PROGRESS: at 63.91% examples, 520495 words/s, in_qsize 3, out_qsize 2\n",
      "2020-03-12 21:54:21,431 : INFO : EPOCH 1 - PROGRESS: at 65.21% examples, 518453 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:54:22,436 : INFO : EPOCH 1 - PROGRESS: at 66.44% examples, 515882 words/s, in_qsize 4, out_qsize 1\n",
      "2020-03-12 21:54:23,440 : INFO : EPOCH 1 - PROGRESS: at 67.74% examples, 513848 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:54:24,462 : INFO : EPOCH 1 - PROGRESS: at 69.06% examples, 512031 words/s, in_qsize 4, out_qsize 1\n",
      "2020-03-12 21:54:25,462 : INFO : EPOCH 1 - PROGRESS: at 70.30% examples, 509809 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-12 21:54:26,474 : INFO : EPOCH 1 - PROGRESS: at 71.59% examples, 507963 words/s, in_qsize 6, out_qsize 2\n",
      "2020-03-12 21:54:27,485 : INFO : EPOCH 1 - PROGRESS: at 72.85% examples, 506025 words/s, in_qsize 4, out_qsize 2\n",
      "2020-03-12 21:54:28,490 : INFO : EPOCH 1 - PROGRESS: at 74.03% examples, 503709 words/s, in_qsize 4, out_qsize 1\n",
      "2020-03-12 21:54:29,491 : INFO : EPOCH 1 - PROGRESS: at 75.40% examples, 502663 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 21:54:30,558 : INFO : EPOCH 1 - PROGRESS: at 76.66% examples, 500376 words/s, in_qsize 6, out_qsize 2\n",
      "2020-03-12 21:54:31,571 : INFO : EPOCH 1 - PROGRESS: at 77.93% examples, 498738 words/s, in_qsize 3, out_qsize 2\n",
      "2020-03-12 21:54:32,619 : INFO : EPOCH 1 - PROGRESS: at 79.28% examples, 497396 words/s, in_qsize 3, out_qsize 2\n",
      "2020-03-12 21:54:33,635 : INFO : EPOCH 1 - PROGRESS: at 80.52% examples, 495658 words/s, in_qsize 3, out_qsize 1\n",
      "2020-03-12 21:54:34,635 : INFO : EPOCH 1 - PROGRESS: at 81.70% examples, 493701 words/s, in_qsize 5, out_qsize 2\n",
      "2020-03-12 21:54:35,680 : INFO : EPOCH 1 - PROGRESS: at 82.87% examples, 491431 words/s, in_qsize 5, out_qsize 3\n",
      "2020-03-12 21:54:36,684 : INFO : EPOCH 1 - PROGRESS: at 84.05% examples, 489586 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:54:37,690 : INFO : EPOCH 1 - PROGRESS: at 85.12% examples, 487248 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:54:38,725 : INFO : EPOCH 1 - PROGRESS: at 86.31% examples, 485284 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:54:39,730 : INFO : EPOCH 1 - PROGRESS: at 87.59% examples, 484156 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:54:40,766 : INFO : EPOCH 1 - PROGRESS: at 88.80% examples, 482587 words/s, in_qsize 3, out_qsize 2\n",
      "2020-03-12 21:54:41,790 : INFO : EPOCH 1 - PROGRESS: at 90.10% examples, 481660 words/s, in_qsize 6, out_qsize 2\n",
      "2020-03-12 21:54:42,817 : INFO : EPOCH 1 - PROGRESS: at 91.43% examples, 480854 words/s, in_qsize 5, out_qsize 1\n",
      "2020-03-12 21:54:43,848 : INFO : EPOCH 1 - PROGRESS: at 92.74% examples, 479913 words/s, in_qsize 3, out_qsize 2\n",
      "2020-03-12 21:54:44,890 : INFO : EPOCH 1 - PROGRESS: at 93.99% examples, 478583 words/s, in_qsize 3, out_qsize 2\n",
      "2020-03-12 21:54:45,891 : INFO : EPOCH 1 - PROGRESS: at 95.39% examples, 478298 words/s, in_qsize 6, out_qsize 2\n",
      "2020-03-12 21:54:46,907 : INFO : EPOCH 1 - PROGRESS: at 96.96% examples, 478860 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-12 21:54:47,947 : INFO : EPOCH 1 - PROGRESS: at 98.46% examples, 478776 words/s, in_qsize 3, out_qsize 2\n",
      "2020-03-12 21:54:48,990 : INFO : EPOCH 1 - PROGRESS: at 99.87% examples, 478327 words/s, in_qsize 4, out_qsize 2\n",
      "2020-03-12 21:54:49,013 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 21:54:49,019 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 21:54:49,023 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 21:54:49,024 : INFO : EPOCH - 1 : training on 41001976 raw words (33124487 effective words) took 69.2s, 478706 effective words/s\n",
      "2020-03-12 21:54:50,048 : INFO : EPOCH 2 - PROGRESS: at 3.08% examples, 1013844 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:54:51,064 : INFO : EPOCH 2 - PROGRESS: at 6.22% examples, 1018709 words/s, in_qsize 4, out_qsize 1\n",
      "2020-03-12 21:54:52,070 : INFO : EPOCH 2 - PROGRESS: at 9.57% examples, 1044176 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:54:53,076 : INFO : EPOCH 2 - PROGRESS: at 12.63% examples, 1035628 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:54:54,092 : INFO : EPOCH 2 - PROGRESS: at 15.75% examples, 1032962 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:54:55,095 : INFO : EPOCH 2 - PROGRESS: at 18.84% examples, 1030939 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:54:56,107 : INFO : EPOCH 2 - PROGRESS: at 22.49% examples, 1055465 words/s, in_qsize 4, out_qsize 1\n",
      "2020-03-12 21:54:57,110 : INFO : EPOCH 2 - PROGRESS: at 25.84% examples, 1061076 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:54:58,125 : INFO : EPOCH 2 - PROGRESS: at 28.89% examples, 1053556 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:54:59,132 : INFO : EPOCH 2 - PROGRESS: at 31.91% examples, 1047604 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:55:00,132 : INFO : EPOCH 2 - PROGRESS: at 34.90% examples, 1042573 words/s, in_qsize 5, out_qsize 2\n",
      "2020-03-12 21:55:01,136 : INFO : EPOCH 2 - PROGRESS: at 38.34% examples, 1049802 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:55:02,138 : INFO : EPOCH 2 - PROGRESS: at 41.51% examples, 1049557 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:55:03,139 : INFO : EPOCH 2 - PROGRESS: at 44.45% examples, 1044407 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:55:04,139 : INFO : EPOCH 2 - PROGRESS: at 47.42% examples, 1039940 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:55:05,150 : INFO : EPOCH 2 - PROGRESS: at 50.30% examples, 1033770 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:55:06,161 : INFO : EPOCH 2 - PROGRESS: at 53.18% examples, 1028854 words/s, in_qsize 4, out_qsize 1\n",
      "2020-03-12 21:55:07,163 : INFO : EPOCH 2 - PROGRESS: at 56.15% examples, 1025861 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:55:08,166 : INFO : EPOCH 2 - PROGRESS: at 59.00% examples, 1020752 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:55:09,167 : INFO : EPOCH 2 - PROGRESS: at 61.87% examples, 1016824 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:55:10,168 : INFO : EPOCH 2 - PROGRESS: at 64.95% examples, 1017089 words/s, in_qsize 4, out_qsize 0\n",
      "2020-03-12 21:55:11,199 : INFO : EPOCH 2 - PROGRESS: at 67.96% examples, 1015243 words/s, in_qsize 6, out_qsize 1\n",
      "2020-03-12 21:55:12,212 : INFO : EPOCH 2 - PROGRESS: at 70.91% examples, 1013297 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:55:13,223 : INFO : EPOCH 2 - PROGRESS: at 73.86% examples, 1011571 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:55:14,226 : INFO : EPOCH 2 - PROGRESS: at 77.67% examples, 1021441 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:55:15,230 : INFO : EPOCH 2 - PROGRESS: at 80.75% examples, 1021624 words/s, in_qsize 5, out_qsize 1\n",
      "2020-03-12 21:55:16,241 : INFO : EPOCH 2 - PROGRESS: at 84.63% examples, 1030621 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:55:17,245 : INFO : EPOCH 2 - PROGRESS: at 87.72% examples, 1030185 words/s, in_qsize 5, out_qsize 0\n",
      "2020-03-12 21:55:18,246 : INFO : EPOCH 2 - PROGRESS: at 90.93% examples, 1031568 words/s, in_qsize 6, out_qsize 0\n",
      "2020-03-12 21:55:19,262 : INFO : EPOCH 2 - PROGRESS: at 94.26% examples, 1033389 words/s, in_qsize 5, out_qsize 0\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# test with size, window, min_count, iter, sg, negative\n",
    "vectors = models.Word2Vec(tokenized_text_full)\n",
    "print(\"Most similar to: \", seed_words[0])\n",
    "print(vectors.wv.most_similar(seed_words[0]))\n",
    "print(\"Most similar to: \", seed_words[1])\n",
    "print(vectors.wv.most_similar(seed_words[1]))\n",
    "print(\"Most similar to: \", seed_words[2])\n",
    "print(vectors.wv.most_similar(seed_words[2]))\n",
    "print(\"Most similar to: \", seed_words[3])\n",
    "print(vectors.wv.most_similar(seed_words[3]))\n",
    "print(\"Most similar to: \", seed_words[4])\n",
    "print(vectors.wv.most_similar(seed_words[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of 2002 vs full comparison:\n",
    "\n",
    "The model has clearly learnt a lot more with that much more data. It's hard to quantify exactly at what amount of documents it did learn this much, however in for example the spring example, it clearly recognizes that it is a season and that the other seasons are similar to it.\n",
    "\n",
    "Similarly the other similarities also got, what I feel, a lot more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow==1.15\n",
    "#!pip install \"tensorflow_hub>=0.6.0\"\n",
    "#!pip3 install tensorflow_text==1.15\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "elmo = hub.KerasLayer(\"https://tfhub.dev/google/elmo/3\",signature=\"default\", as_dict=True trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elmo_vectors(sents):\n",
    "    embeddings = elmo(sents, )[\"elmo\"]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        return sess.run(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elmo_vectors(sents):\n",
    "    embeddings = elmo(sents, signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        return sess.run(embeddings)\n",
    "        #sess.run(tf.tables_initializer())\n",
    "        # return average of ELMo features as sentence vector\n",
    "        #return sess.run(tf.reduce_mean(embeddings,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = \"\"\"The game ended quickly .\n",
    "He hunted some game for dinner .\n",
    "A game of swans in the river .*\n",
    "They played a game of chess .\n",
    "They were in a baseball game .\n",
    "She decided to eat som game .\n",
    "Game can be found in forests .\n",
    "Counterstrike is a popular game .\n",
    "They didn't follow the game .\n",
    "It was time to game .\"\"\".split('\\n')\n",
    "\n",
    "target = \"game\"\n",
    "\n",
    "elmo_vecs = elmo_vectors(sents)\n",
    "word_vecs = []\n",
    "for i, sent in enumerate(sents):\n",
    "    word_vecs.append(elmo_vecs[i][sent.split().index(target)])\n",
    "    print(\"Sentence: \", sent)\n",
    "    print(\"Vector for '%s:'\" % target, word_vecs[-1])\n",
    "    print()\n",
    "    \n",
    "print(\"Word vec size\", word_vecs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vec_size = word_vecs[0].shape[0]\n",
    "print(\"Similarities between '%s' vector in sentences:\" % target)\n",
    "for i in range(1, len(sents)):\n",
    "    print(\"Sent 0-%d:\" % i, cosine_similarity(word_vecs[0].reshape((1,vec_size)), \n",
    "                                              word_vecs[i].reshape((1,vec_size)))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised methods in NLP\n",
    "\n",
    "\n",
    "\n",
    "### Themes:\n",
    "- Part I: Analyzing topics (unsupervised NLP basics)\n",
    "    - Term weighting with TF-IDF\n",
    "    - Document clustering\n",
    "- Part II: Transfer learning (modern NLP)\n",
    "    - Word vectors (word2vec)\n",
    "    - Pre-trained language models (ELMo, BERT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "- Goal: identify meaningful relationships in raw text *internally*\n",
    "    - No need for manual annotation (as in supervised learning)\n",
    "- Unsupervised methods involve:\n",
    "    - Counting frequencies of word occurence in different contexts\n",
    "    - Counting/estimating frequencies of word co-occurrence in different contexts\n",
    "    - Machine learning methods for clustering objects (e.g. documents)\n",
    "    - Machine learning methods for modeling word sequences (language modeling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part I: Analyzing topics (unsupervised NLP basics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Term weighting with TF-IDF\n",
    "\n",
    "### Recap from \"Basic NLP\" lecture: _tf-idf weighting_\n",
    "\n",
    "* TF = term frequency *tf(t, d)*, how many times the term _t_ appears in document *d*\n",
    "* DF = document frequency *df(t)*, in how many documents the term *t* appears\n",
    "* IDF = inverse document frequency, *m/df(t)*, where *m* is the total number of documents in your collection\n",
    "* TF-IDF = **tf(t, d) * idf(t)**\n",
    "    * Usually calculated using logaritmic (sublinear) scale --> tf(t, d) * log(idf(t))\n",
    "    \n",
    "* common in information retrieval, also used in document classification and clustering\n",
    "* scale down the impact of tokens that occur very frequently in many documents and are hence empirically less informative/distinctive than words that occur in a small fraction of the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "More reading on [tf-idf](https://nlp.stanford.edu/IR-book/html/htmledition/term-frequency-and-weighting-1.html) and [scaling and normalization variations](https://nlp.stanford.edu/IR-book/html/htmledition/variant-tf-idf-functions-1.html) (in the context of information retrieval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### Calculate TF-IDF scores on a corpus\n",
    "\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "with open(\"Data/patents.txt\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    documents = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Set parameters and initialize\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=2, use_idf=True, sublinear_tf=True, max_df=1.0, max_features=20000)\n",
    "# Tip: the vectorizer also supports extracting n-gram features (common short sequences of words), which may be more descriptive but also much less frequent\n",
    "\n",
    "# Calcualate term-document matrix with tf-idf scores\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Check matrix shape\n",
    "tfidf_matrix.toarray().shape # N_docs x N_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Inspect terms in vocabulary\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "print(tfidf_vectorizer.get_feature_names()[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Inspect document frequencies (counts) of terms\n",
    "\n",
    "from collections import Counter\n",
    "terms_in_docs = tfidf_vectorizer.inverse_transform(tfidf_matrix)\n",
    "token_counter = Counter()\n",
    "for terms in terms_in_docs:\n",
    "    token_counter.update(terms)\n",
    "\n",
    "for term, count in token_counter.most_common(20):\n",
    "    print(\"%d\\t%s\" % (count, term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Alternatively: Inspect IDF values directly\n",
    "#print(sorted(zip(features,tfidf_vectorizer._tfidf.idf_),key=lambda x:x[1])[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Inspect top terms per document\n",
    "\n",
    "features = tfidf_vectorizer.get_feature_names()\n",
    "for doc_i in range(5):\n",
    "    print(\"\\nDocument %d, top terms by TF-IDF\" % doc_i)\n",
    "    for term, score in sorted(list(zip(features,tfidf_matrix.toarray()[doc_i])), key=lambda x:-x[1])[:5]:\n",
    "        print(\"%.2f\\t%s\" % (score, term))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Document clustering\n",
    "\n",
    "- Clustering: grouping of similar objects in order to structure a set\n",
    "    - E.g., clustering documents in a corpus -> reveal common topics and provide overview\n",
    "- TF-IDF weighting for extracting meaningful *features* of documents, highlighting descriptive terms\n",
    "- Goal: Given features, calculate similarities between documents and assign them into clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The term-document matrix (`tfidf_matrix`) represents the corpus as tf-idf weights (features)\n",
    "    - Each document represented as a (row) vector\n",
    "    - Document vectors are sparse:\n",
    "        - Dimensionality equal to the number of terms extracted from the corpus\n",
    "        - Non-zero values only for the fraction of terms in document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Document vector length:\", tfidf_matrix.shape[1])\n",
    "for i in range(5):\n",
    "    print(\"Non-zero dimensions for document %d: %d\" % (i, len([x for x in tfidf_matrix.toarray()[i] if x > 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Sample word:\", features[1000])\n",
    "print(\"Occurs in %d documents\" % len([x for x in tfidf_matrix.toarray()[:][1000] if x > 0]))\n",
    "print(\"out of %d documents\" % len(tfidf_matrix.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vector space model\n",
    "\n",
    "- A vector space model encodes meaning as coordinates in a high-dimensional space\n",
    "    - E.g., tf-idf term vectors describe the topic of a document\n",
    "- Semantic similarities are measured as distances in the space:\n",
    "    - For vectors *x* and _y_:\n",
    "        - Euclidean distance: $$d = \\sqrt{ (x_1 - y_1)^2 + (x_2 - y_2)^2 + ... + (x_n - y_n)^2 }$$\n",
    "        - Cosine similarity (distance = 1-similarity): $$s = cos(\\sigma) = \\frac{x \\cdot y}{|x||y|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We use vector space notion of semantic similarity to group documents into clusters\n",
    "    - Find common topics, go from many documents to fewer clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "More reading on the [vector space model](https://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### k-means clustering\n",
    "\n",
    "- We will cluster documents based on their tf-idf vectors \n",
    "- and the relatively simple and widely used *k-means* method\n",
    "    - Takes as argument *k* number of clusters to be found\n",
    "    - Uses a random initialization (i.e., results can vary between runs)\n",
    "    - Works according to this pseudo code (from the [Mining Massive Datasets book](http://infolab.stanford.edu/~ullman/mmds/ch7.pdf)):\n",
    "\n",
    "<code>Initially choose k points that are likely to be in different clusters;\n",
    "Make these points the centroids of their clusters;\n",
    "FOR each remaining point p DO \n",
    "    find the centroid to which p is closest;\n",
    "    Add p to the cluster of that centroid;\n",
    "    Adjust the centroid of that cluster to account for p;\n",
    "END;</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We use a subset of the documents in clustering for faster calculation and easier interpretation of results\n",
    "matrix_sample = tfidf_matrix[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Do clustering\n",
    "km = KMeans(n_clusters=30, random_state=123, verbose=0)\n",
    "km.fit(matrix_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import heapq, numpy as np\n",
    "\n",
    "# Custom function to print top keywords for each cluster\n",
    "def print_clusters(matrix, clusters, n_keywords=10):\n",
    "    for cluster in range(min(clusters), max(clusters)+1):\n",
    "        cluster_docs = [i for i, c in enumerate(clusters) if c == cluster]\n",
    "        print(\"Cluster: %d (%d docs)\" % (cluster, len(cluster_docs)))\n",
    "        \n",
    "        # Keep scores for top n terms\n",
    "        new_matrix = np.zeros((len(cluster_docs), matrix.shape[1]))\n",
    "        for cluster_i, doc_vec in enumerate(matrix[cluster_docs].toarray()):\n",
    "            for idx, score in heapq.nlargest(n_keywords, enumerate(doc_vec), key=lambda x:x[1]):\n",
    "                new_matrix[cluster_i][idx] = score\n",
    "\n",
    "        # Aggregate scores for kept top terms\n",
    "        keywords = heapq.nlargest(n_keywords, zip(new_matrix.sum(axis=0), features))\n",
    "        print(', '.join([w for s,w in keywords]))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print_clusters(matrix_sample, km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### Hierarchical clustering (alternative approach)\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "Z = linkage(matrix_sample.todense(), metric='cosine', method='complete')\n",
    "_ = dendrogram(Z, no_labels=True) # Plot dentrogram chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# View dendrogram for subset\n",
    "Z_ = linkage(matrix_sample.todense()[:250], metric='cosine', method='complete')\n",
    "_ = dendrogram(Z_, no_labels=True) # Plot dentrogram chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Get flat clusters from cluster hierarchy\n",
    "\n",
    "#clusters = fcluster(Z, 50, criterion='maxclust') # Create fix number of flat clusters\n",
    "clusters = fcluster(Z, 0.99, criterion='distance') # Create flat clusters by distance threshold\n",
    "\n",
    "print_clusters(matrix_sample, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probabilistic topic modeling\n",
    "\n",
    "- Various machine learning methods for *topic modeling* extend the idea of topic clustering\n",
    "- The most popular: [Latent Dirichlet Allocation (LDA) method](https://dl.acm.org/doi/pdf/10.1145/2133806.2133826) (see also [gensim library](https://radimrehurek.com/gensim/models/ldamodel.html))\n",
    "- LDA performs a type of fuzzy clustering into *k* topics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- \n",
    "    - A document can be assigned to *serveral topics*, according to a probability distribution\n",
    "        - E.g., 68% Topic 6, 21% Topic 5, 11% Topic 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-     - A topic is described by a *weighting of terms* (probability distribution)\n",
    "        - The terms reflect term frequencies in documents assigned to the topic\n",
    "        - The top-10 are often inspected, after some filtering/reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Topic modeling demo\n",
    "#!pip3 install gensim\n",
    "\n",
    "# Fast and simple tokenization\n",
    "new_vectorizer = TfidfVectorizer()\n",
    "word_tokenizer = new_vectorizer.build_tokenizer()\n",
    "tokenized_text = [word_tokenizer(doc) for doc in documents]\n",
    "\n",
    "# Train LDA model\n",
    "from gensim import corpora, models\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "lda_corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "lda_model = models.LdaModel(lda_corpus, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Inspect topics\n",
    "for i, topic in lda_model.show_topics(num_words=10, formatted=False):\n",
    "    print(\"Topic\", i)\n",
    "    for term, score in topic:\n",
    "        print(\"%.4f\\t%s\" % (score,term))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Inspect topics\n",
    "for i, topic in lda_model.show_topics(num_words=50, formatted=False):\n",
    "    print(\"Topic\", i)\n",
    "    printed_terms = 0\n",
    "    for term, score in topic:\n",
    "        if printed_terms >= 10:\n",
    "            break\n",
    "        elif term in \"the of and to for in or The is be may an a with at are on by as from can\".split():\n",
    "            continue\n",
    "        printed_terms += 1\n",
    "        print(\"%.4f\\t%s\" % (score,term))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part II: Transfer learning (modern NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word vectors\n",
    "\n",
    "- Recap: We used the term-document matrix to understand semantic similarities *between documents* based on overlapping keywords\n",
    "- Now, we study similarities *between words*, by comparing the close contexts in which they appear\n",
    "- The comparison is also done in a vector space, see illustration (with only 3 dimensions):\n",
    "\n",
    "![semantic_space](figs/semspace.svg \"Illustration of words embedded in a semantic vector space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Simple demo, document context:**\n",
    "- We can also inspect the term-document matrix vertically, by extracting *column vectors* for terms\n",
    "- Meaning by context: A term is described by the documents in which it occurs\n",
    "- The tf-idf scores of the term vector (or word vector) define coordinates in a high-dimensional space\n",
    "    - Distances can be measured, commonly by the cosine similarity function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"Similar terms to:\", features[3000])\n",
    "# Get most similar terms according to the cosine similarity of their vectors (columns in the term-document matrix)\n",
    "heapq.nlargest(10, zip(cosine_similarity(tfidf_matrix[:,3000].todense().T, tfidf_matrix.todense().T)[0], features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Better approach: Word co-occurrence matrix\n",
    "\n",
    "- Look at a more immediate context of word use, e.g., +/- 5 words\n",
    "- Count word co-occurrence statistics (--> word-word matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Analyzing word combinations (first-order co-occurrence):**\n",
    "- Strongly associated pairs of words can be identified using the *pointwise mutual information* (PMI) score:\n",
    "    - For words x and y: $$pmi(x,y) = \\log \\frac{p(x,y)}{p(x)p(y)} $$\n",
    "\n",
    "    - where p(x,y) is estimated as the co-occurrence frequency and p(x) and p(y) as the independent occurence frequencies\n",
    "- PMI can highlight common combinations of words and topics (e.g., \"poor\"-\"service\"; \"handsome\"-\"man\" / \"beautiful\"-\"woman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "More reading on: [PMI](https://en.wikipedia.org/wiki/Pointwise_mutual_information) and [other association measures](http://collocations.de/AM/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Analyzing word similarity (second-order co-occurrence):**\n",
    "\n",
    "- \"You shall know a word by the company it keeps\" (J.R. Firth)\n",
    "    - The meaning of a word can be understood by the distribution of words it tends to appear in (= distributional hypothesis)\n",
    "    - Words may be similar although they to not directly co-occur (but share context words) --> 2nd order co-occurrence\n",
    "- Cosine function between word vectors gives word pair similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Vectors are very sparse: \n",
    "    - Vocabulary (dimensionality) grows with corpus size, while context size is small and fixed\n",
    "    - Similarity can be estimated only when there is an overlap in terms\n",
    "    - Many rare terms / sparse vectors --> very big corpora needed for good models --> huge memory usage and scalability issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning dense vectors\n",
    "\n",
    "More useful vectors through:\n",
    "- Matrix decomposition approach:\n",
    "    - Reduce the number of dimensions os the semantic vector space, for instance, using Singular Value Decomposition (SVD)\n",
    "        - Dimensionality reduction, e.g., from 10k or 100k to 100\n",
    "    - Also used in Latent Semantic Analysis (LSA), applied to term-document matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Neural network approach:\n",
    "    - Learning *distributed representations* of concepts explored first by Hinton (1986)\n",
    "    - word2vec, from 2013, is an important milestone in NLP:\n",
    "        - Efficient estimation of dense word vectors, enabling fast training\n",
    "        - Provides high precision vectors when applied to large corpora\n",
    "\n",
    "- word2vec skip-gram model:\n",
    "<img src=\"figs/skipgram_model.svg\" alt=\"skip-gram model\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "More reading on [counting vs. predicting word vectors](https://www.aclweb.org/anthology/P14-1023.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### Train word vectors\n",
    "\n",
    "import gensim # Make sure you also have cython installed to accelerate computation!\n",
    "\n",
    "#import logging\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Train word2vec model\n",
    "vectors = gensim.models.Word2Vec(tokenized_text, size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Inspect a word vector\n",
    "vectors.wv['process']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Inspect words with vectors most similar to a given word\n",
    "print(\"Most similar to:\", 'process')\n",
    "print(vectors.wv.most_similar('process'))\n",
    "print()\n",
    "\n",
    "# ...or combination or words (average vector)\n",
    "print(\"Most similar to:\", ['process', 'payment'])\n",
    "print(vectors.wv.most_similar(['process', 'payment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Inspect cosine similarities between specific words\n",
    "print(vectors.wv.similarity('technology', 'infrastructure'))\n",
    "print(vectors.wv.similarity('technology', 'system'))\n",
    "print(vectors.wv.similarity('technology', 'consumer'))\n",
    "print(vectors.wv.similarity('technology', 'advanced'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "## Test for analogies and syntactic regularities (when trained on a large and suitable corpus)\n",
    "\n",
    "\"\"\"\n",
    "# Man is to King as Woman is to ...\n",
    "# vec(woman)+vec(king)-vec(man)\n",
    "print(vectors.most_similar(['woman','king'], negative=['man']))\n",
    "print()\n",
    "\n",
    "# Fall is to Fell as Watch is to ...\n",
    "print(vectors.most_similar(['wear','fell'], negative=['fall']))\n",
    "print()\n",
    "\n",
    "print(vectors.most_similar(['Moscow','France'], negative=['Paris']))\n",
    "print()\n",
    "\n",
    "print(vectors.most_similar(['London','France'], negative=['Paris']))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pre-trained word vectors\n",
    "- Word vectors trained on large corpora achieve more accurate results\n",
    "- For instance, word2vec vectors pre-trained on 100B words of Google News text can be [downloaded](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing) (1.5GB) and used in the same way as self-trained vectors\n",
    "- word2vec is also efficient for training your own vectors on mid-sized corpora (e.g., 1B words)\n",
    "    - Online demo [available here](http://bionlp-www.utu.fi/wv_demo/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vectors as features\n",
    "\n",
    "- Dense vectors are practical to use especially in neural network models for downstream tasks (e.g., text classification)\n",
    "- Word vectors (often _word embeddings_) estimated on large amounts of raw text:\n",
    "    - Embedding layer can serve as an abstraction of text input\n",
    "    - Reduces reliance on (limited) annotated data in supervised tasks\n",
    "    - Pre-trained word vectors can make NLP models more accurate and robust\n",
    "    - This transfer of knowledge about language is called *transfer learning*\n",
    "\n",
    "<img src=\"figs/simple_lstm.svg\" alt=\"Neural network with word embeddings\" width=\"400\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Limitations\n",
    "\n",
    "- Problem: Word vectors produced by word2vec are static with respect to the word\n",
    "    - I.e., a term always has the same vector representation regardless of how it is used\n",
    "    - Multiple senses of a word are not distinguished\n",
    "    - Difficult to capture meaning of sentences (unless fine-tuned on some task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ambiguous word vector examples\n",
    "\n",
    "print(\"Most similar to:\", 'data')\n",
    "print(vectors.wv.most_similar('data'))\n",
    "print()\n",
    "\n",
    "# Are the below average vectors less ambiguous?\n",
    "print(\"Most similar to:\", ['data','transfer'])\n",
    "print(vectors.wv.most_similar(['data','transfer']))\n",
    "print()\n",
    "\n",
    "print(\"Most similar to:\", ['data','storage'])\n",
    "print(vectors.wv.most_similar(['data','storage']))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contextualized word vectors with language modeling\n",
    "\n",
    "- Word meaning can be disambiguated from the context in an instance of use\n",
    "    - Compare:\n",
    "        - \"He wrote the **play** himself.\"\n",
    "        - \"He did **play** with the kids.\"\n",
    "        \n",
    "- Recent methods build on *language modeling* to provide *contextualized* word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Recap: A language model estimates the probability *P(x)* of seeing a word _x_ following a context, such as:\n",
    "\n",
    "$$P(\\ x\\ |\\ the\\ students\\ opened\\ their\\ )$$\n",
    "\n",
    "![language modeling (source: mc.ai)](figs/language_modeling.png \"Language modeling illustration\")\n",
    "The above suggestions would be deemed much more likely than, e.g., *P(x=keyboard)*, _P(x=follows)_ or *P(x=the)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Lanugage modeling recap: Consider a simple statistical model that estimates the probability of a word given a number of previous words in a text. For instance, given the words *the dog loved to*, the model would deem it much more likely that the next word is _play_ than any word that does not fit semantically or syntactically (e.g., *green*, _played_ or *code*). The probability _p(play | the dog loved to)_ is much higher than for most other words. Such a model can be implemented as a machine learning model, typically a neural network that can take into account a long context, and be trained over large amounts of text. Language models that have a large number of internal trainable parameters can learn complicated regularities of natural language, by learning to accurately predict the next word in a sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For instance, [ELMo](https://allennlp.org/elmo) (released early 2018) uses recurrent neural networks (LSTM) for learning contextualized word vectors through language modeling.\n",
    "\n",
    "<img src=\"figs/elmo-language-modeling.png\" width=\"400\" alt=\"Source: https://jalammar.github.io/illustrated-bert/\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- ELMo holds internal vector representations as it traverses the word sequence, which can be aggregated to:\n",
    "    - Contextualized word vectors\n",
    "    - A sentence vector, reflecting composite meaning\n",
    "\n",
    "<img src=\"figs/elmo-embedding.png\" width=\"550\" alt=\"Source: https://jalammar.github.io/illustrated-bert/\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#!pip3 install tensorflow_hub tensorflow\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load ELMo model (takes a little while)\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)\n",
    "\n",
    "def elmo_vectors(sents):\n",
    "    embeddings = elmo(sents, signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        return sess.run(embeddings)\n",
    "        #sess.run(tf.tables_initializer())\n",
    "        # return average of ELMo features as sentence vector\n",
    "        #return sess.run(tf.reduce_mean(embeddings,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Get contextualized vectors for target word in different sentences\n",
    "\n",
    "sents = \"\"\"He wrote the play himself .\n",
    "He did play with the kids .\n",
    "His excuse didn't play well .\n",
    "I saw the play with him .\n",
    "He can play it by ear .\"\"\".split('\\n')\n",
    "\n",
    "target = 'play'\n",
    "\n",
    "elmo_vecs = elmo_vectors(sents)\n",
    "word_vecs = []\n",
    "for i, sent in enumerate(sents):\n",
    "    word_vecs.append(elmo_vecs[i][sent.split().index(target)])\n",
    "    print(\"Sentence:\", sent)\n",
    "    print(\"Vector for '%s':\" % target, word_vecs[-1])\n",
    "    print()\n",
    "\n",
    "print(\"Word vector size:\", word_vecs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vec_size = word_vecs[0].shape[0]\n",
    "print(\"Similarities between '%s' vector in sentences:\" % target)\n",
    "for i in range(1, len(sents)):\n",
    "    print(\"Sent 0-%d:\" % i, cosine_similarity(word_vecs[0].reshape((1,vec_size)), \n",
    "                                              word_vecs[i].reshape((1,vec_size)))[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Even bigger pre-trained models: BERT etc.\n",
    "\n",
    "- In late 2018, a new, much larger pre-trained language model was released called *BERT* (Bidirectional Encoder Representations from Transformers)\n",
    "- Transformers can be better parallelized than recurrent network architectures\n",
    "    - --> Enables training of much larger models than before (on GPU/TPU)\n",
    "- BERT Large model: 340M parameters ~ 24 Transformer layers x 1024 hidden units x 16 attention heads\n",
    "    - Compared with ELMo (93.6M parameters), BERT Base (110M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Language modeling by learning to predict individual masked words\n",
    "    - Tricky task that benefits from sophisticated \"understanding\" of sentences\n",
    "    - Transformers are good at modeling long-range dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Computationally very demaning to train (compute cost ~$10k per run, trained on 3.3B words)\n",
    "- But, fine-tuning model for downstream task (e.g., classification) is quick\n",
    "- BERT was able to beat the best performing models in a range of NLP tasks at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Simple BERT model illustration. Each token position is connected to each position in the next layer.\n",
    "<img src=\"figs/bert.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Visualization of token-token activations for a sentence per layer and attention head.\n",
    "<img src=\"figs/bertvis.png\" source=\"Jesse Vig\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- BERT has been particularly successful at *natural language understanding (NLU)* tasks, e.g.:\n",
    "    - Sentence acceptability\n",
    "    - Sentiment\n",
    "    - Paraphrase identification\n",
    "    - Sentence similarity\n",
    "    - Textual inference\n",
    "    - Question answering\n",
    "    \n",
    "\n",
    "- Note, however, these models risk learning spurious statistical patterns, rather than real inference (cf. [[1]](https://www.technologyreview.com/s/615126/ai-common-sense-reads-human-language-ai2/)[[2]](https://www.aclweb.org/anthology/P19-1459.pdf))!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "BERT has inspired a never-ending stream of successors. See [GLUE (NLU tasks) leaderboard](https://gluebenchmark.com/leaderboard).\n",
    "<img alt=\"Map of the BERTology landscape\" src=\"figs/bertology_map.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Getting started with Transformer-based language models (GPU highly recommended): [huggingface transformers library](https://huggingface.co/transformers/pretrained_models.html)\n",
    "\n",
    "<code>import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "...</code>\n",
    "\n",
    "or with `bert-base-finnish-cased-v1` model from TurkuNLP!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can wget this notebook from https://raw.githubusercontent.com/TurkuNLP/intro-to-nlp/master/bow_classifier.ipynb\n",
    "\n",
    "# Text classification\n",
    "\n",
    "## Objectives of this lecture\n",
    "\n",
    "* Introduce text classification as a task\n",
    "* Introduce basic approaches to text classification, especially the bag-of-words model\n",
    "* Show practical implementation of the concepts in the previous two lectures (intro and machine learning primer)\n",
    "* Train a simple classifier on the IMDB dataset\n",
    "\n",
    "## Purporse of these materials\n",
    "\n",
    "* These materials support, but do not replace the lectures\n",
    "* Presence on lectures highly recommended\n",
    "* Make notes!\n",
    "\n",
    "## Text classification\n",
    "\n",
    "* Text in, class label out\n",
    "* Often thought especially in terms of \"Document classification\" --- document in, class label(s) out\n",
    "  * Movie review -> star rating\n",
    "  * Email -> ham or spam\n",
    "  * News article -> list of topics from a pre-defined set of topic categories\n",
    "  * Comment in online discussion -> abusive or not\n",
    "  * Customer email / call to customer service -> topic and relevant business process\n",
    "  * Customer feedback -> needs reaction or not\n",
    "  * ...\n",
    "  \n",
    " * Document classification can be seen as a supervised classification problem with pre-defined classes\n",
    " * A direct application of the standard approach:\n",
    "   1. prepare training/dev/test data\n",
    "   2. extract features\n",
    "   3. train your favorite classifier\n",
    "   4. test on held-out data\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words model\n",
    "\n",
    "* BoW is the simplest way to model documents for classification\n",
    "* The document is reduced to a **set** of features, in the simplest case the words\n",
    "\n",
    "\n",
    "* Feature vector: a vector with as many dimensions as we have unique features, and a non-zero value set for every feature present in our example\n",
    "* Values: 1/0 (present/absent) or oftentimes TF/IDF weights (why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB data\n",
    "\n",
    "* Movie review sentiment positive/negative\n",
    "* Some 25,000 examples, 50:50 split of classes (why is this number relevant?)\n",
    "* Current state-of-the-art is about 95% accuracy (what is accuracy?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class label: pos\n",
      "text: I saw this film with a special screening of the work of Owen Alik Shahadah. It is so interesting where did this guy come from. Now he is probably the key independent African filmmaker in the world. And I am not talking about black filmmakers I am talking about filmmakers who are rooted in culture. The idea if anything testifies to the diversity and range of African themes, with his film 500 Years Later it is a African issue. But the Idea doesn't fit that mold. Showing the artistic diversity. The film is an all African cast but the topic is a human topic which most of us could relate to. I just love the mild comedy in it. And the Kora of Tunde Jegede is just amazing, it is really a art-house gem.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "with open(\"Data/imdb_train.json\") as f:\n",
    "    data=json.load(f)\n",
    "random.shuffle(data) #play it safe! (why?)\n",
    "# Data given to you can be ordered and some classifiers might not work well\n",
    "print(\"class label:\", data[0][\"class\"])\n",
    "print(\"text:\",data[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words in practice\n",
    "\n",
    "* We will need to build a feature vector for every example\n",
    "* We will need to know the class for every example\n",
    "\n",
    "* Build a data matrix with dimensionality (number of examples, number of possible features), and a value for each feature, 0/1 for binary features, TF-IDF weights are also a typical choice\n",
    "\n",
    "It is quite useless to do all this ourselves, so we will use ready-made classes and functions mostly from scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This many texts 25000\n",
      "This many labels 25000\n",
      "\n",
      "pos I saw this film with a special screening of the wo...\n",
      "neg This film so NOT funny - such a waste of great sta...\n",
      "neg A Christmas Story Is A Holiday Classic And My Favo...\n",
      "neg I've seen all kinds of \\Hamlet\\\"s.   Kenneth Brana...\n",
      "neg This movie is a modest effort by Spike Lee. He is ...\n",
      "neg Surely one of the most ill-advised remakes of a cl...\n",
      "neg Okay. So there aren't really that many great movie...\n",
      "pos BEWARE SPOILERS. This movie was okay. Goldie Hawn ...\n",
      "pos I love occult Horror, and the great British Hammer...\n",
      "pos In Iran, the Islamic Revolution has shaped all par...\n",
      "pos I found this to be a so-so romance/drama that has ...\n",
      "neg Sorry did i miss something? did i walk out early? ...\n",
      "neg The movie had so much potential, but due to 70's t...\n",
      "neg *****probably minor spoilers******  I cant say i l...\n",
      "neg (aka: The Bloodsucker Leads the Dance)  Lots of na...\n",
      "pos Everyone has already commented on the cinematograp...\n",
      "pos \\What would you do?\\\" is a question that will stic...\n",
      "pos Louis Creed, a doctor from Chicago, moves to a lar...\n",
      "pos And this is a great rock'n'roll movie in itself. N...\n",
      "pos I was lucky to see \\Oliver!\\\" in 1968 on a big cin...\n"
     ]
    }
   ],
   "source": [
    "# We need to gather the texts and labels into separate lists\n",
    "texts=[one_example[\"text\"] for one_example in data]\n",
    "labels=[one_example[\"class\"] for one_example in data]\n",
    "print(\"This many texts\",len(texts))\n",
    "print(\"This many labels\",len(labels))\n",
    "print()\n",
    "for label,text in list(zip(labels,texts))[:20]:\n",
    "    print(label,text[:50]+\"...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn text vectorizers\n",
    "\n",
    "* Vectorizers take care of turning inputs into feature vectors\n",
    "* Also build the feature name to feature index mapping:\n",
    "    * `.fit()` learn the mapping\n",
    "    * `.fit_transform()` learn and apply the mapping\n",
    "    * `.transform()` apply the learned mapping\n",
    "    * (What is the difference?)\n",
    "\n",
    "Let's first try on a tiny example, then work our way to the real data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique features:\n",
      "['commodities', 'global', 'gold', 'has', 'is', 'markets', 'metal', 'more', 'of', 'on', 'palladium', 'precious', 'price', 'soared', 'soaring', 'than', 'the', 'why']\n",
      "\n",
      "Feature vectors (sparse format):\n",
      "  (0, 2)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 17)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 16)\t3\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer=CountVectorizer(ngram_range=(1,1))\n",
    "\n",
    "#just two short document\n",
    "toy_data=[\"More precious than gold: Why the metal palladium is soaring\",\"The price of the precious metal palladium has soared on the global commodities markets.\"]\n",
    "\n",
    "vectorizer.fit(toy_data)\n",
    "print(\"Unique features:\")\n",
    "print(vectorizer.get_feature_names())\n",
    "print()\n",
    "print(\"Feature vectors (sparse format):\")\n",
    "print(vectorizer.transform(toy_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1]\n",
      " [1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 3 0]]\n"
     ]
    }
   ],
   "source": [
    "#...and in a more understandable format\n",
    "#...these are the feature vectors of our two toy documents\n",
    "print(vectorizer.transform(toy_data).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# only features seen in the training data can be taken into account!\n",
    "print(vectorizer.transform([\"unseen words only\"]).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape= (25000, 74849)\n",
      "what did we get? -> <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer=CountVectorizer(max_features=100000,binary=True,ngram_range=(1,1))\n",
    "feature_matrix=vectorizer.fit_transform(texts)\n",
    "print(\"shape=\",feature_matrix.shape)\n",
    "print(\"what did we get? ->\", feature_matrix.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 18292)\t1\n",
      "  (0, 46050)\t1\n",
      "  (0, 38755)\t1\n",
      "  (0, 66339)\t1\n",
      "  (0, 32435)\t1\n",
      "  (0, 46680)\t1\n",
      "  (0, 24202)\t1\n",
      "  (0, 68640)\t1\n",
      "  (0, 4753)\t1\n",
      "  (0, 2662)\t1\n",
      "  (0, 60351)\t1\n",
      "  (0, 402)\t1\n",
      "  (0, 72365)\t1\n",
      "  (0, 36865)\t1\n",
      "  (0, 67117)\t1\n",
      "  (0, 67125)\t1\n",
      "  (0, 6334)\t1\n",
      "  (0, 25734)\t1\n",
      "  (0, 9304)\t1\n",
      "  (0, 73342)\t1\n",
      "  (0, 66367)\t1\n",
      "  (0, 65747)\t1\n",
      "  (0, 62338)\t1\n",
      "  (0, 3258)\t1\n",
      "  (0, 21827)\t1\n",
      "  :\t:\n",
      "  (24999, 3538)\t1\n",
      "  (24999, 57668)\t1\n",
      "  (24999, 49147)\t1\n",
      "  (24999, 66526)\t1\n",
      "  (24999, 66621)\t1\n",
      "  (24999, 31868)\t1\n",
      "  (24999, 13574)\t1\n",
      "  (24999, 31669)\t1\n",
      "  (24999, 68412)\t1\n",
      "  (24999, 45388)\t1\n",
      "  (24999, 66329)\t1\n",
      "  (24999, 45656)\t1\n",
      "  (24999, 50496)\t1\n",
      "  (24999, 72910)\t1\n",
      "  (24999, 33779)\t1\n",
      "  (24999, 18839)\t1\n",
      "  (24999, 72239)\t1\n",
      "  (24999, 17239)\t1\n",
      "  (24999, 29971)\t1\n",
      "  (24999, 70493)\t1\n",
      "  (24999, 12484)\t1\n",
      "  (24999, 51610)\t1\n",
      "  (24999, 15621)\t1\n",
      "  (24999, 67385)\t1\n",
      "  (24999, 25109)\t1\n"
     ]
    }
   ],
   "source": [
    "print(feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830', '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s', '01', '01pm', '02', '020410', '029', '03', '04', '041', '05', '050', '06', '06th', '07', '08', '087', '089', '08th', '09', '0f', '0ne', '0r', '0s', '10', '100', '1000', '1000000', '10000000000000', '1000lb', '1000s', '1001', '100b', '100k', '100m', '100min', '100mph', '100s', '100th', '100x', '100yards', '101', '101st', '102', '102nd', '103', '104', '1040', '1040a', '1040s', '105', '1050', '105lbs', '106', '106min', '107', '108', '109', '10am', '10lines', '10mil', '10min', '10minutes', '10p', '10pm', '10s', '10star', '10th', '10x', '10yr', '11', '110', '1100', '11001001', '1100ad', '111', '112', '1138', '114', '1146', '115', '116', '117', '11f', '11m', '11th', '12', '120', '1200', '1200f', '1201', '1202', '123', '12383499143743701', '125', '125m', '127', '128', '12a', '12hr', '12m', '12mm', '12s', '12th', '13', '130', '1300', '1300s', '131', '1318', '132', '134', '135', '135m', '136', '137', '138', '139', '13k', '13s', '13th', '14', '140', '1408', '140hp', '1415', '142', '145', '1454', '146', '147', '1473', '149', '1492', '14a', '14ieme', '14s', '14th', '14yr', '14ème', '15', '150', '1500', '1500s', '150_worst_cases_of_nepotism', '150k', '150m', '151', '152', '153', '1547', '155', '156', '1561', '157', '158', '1594', '15mins', '15minutes', '15s', '15th', '16', '160', '1600', '1600s', '160lbs', '161', '1610', '163', '164', '165', '166', '1660s', '168', '169', '1692', '16ieme', '16k', '16mm', '16s', '16th', '16x9', '16ème', '16éme', '17', '170', '1700', '1700s', '1701', '171', '175', '177', '1775', '1780s', '1790s', '1794', '1798', '17million', '17th', '18', '180', '1800', '1800mph', '1800s', '1801', '1805', '1809', '180d', '1812', '1813', '18137', '1814', '1816', '1820', '1824', '183', '1830', '1832', '1836', '1837', '1838', '1839', '1840', '1840s', '1844', '1846', '1847', '185', '1850', '1850ies', '1850s', '1852', '1853', '1854', '1855', '1859', '1860', '1860s', '1861', '1862', '1863', '1864', '1865', '1870', '1870s', '1871', '1873', '1874', '1875', '1876', '188', '1880', '1880s', '1881', '1886', '1887', '1888', '1889', '188o', '1890', '1890s', '1892', '1893', '1894', '1895', '1896', '1897', '1898', '1899', '18a', '18s', '18th', '18year', '19', '190', '1900', '1900s', '1901', '1902', '1903', '1904', '1905', '1906', '1907', '1908', '1909', '1910', '1910s', '1911', '1912', '1913', '1914', '1915', '1916', '1917', '1918', '1919', '192', '1920', '1920ies', '1920s', '1921', '1922', '1923', '1924', '1925', '1926', '1927', '1928', '1929', '1930', '1930ies', '1930s', '1931', '1932', '1933', '1934', '1935', '1936', '1937', '1938', '1939', '193o', '194', '1940', '1940s', '1941', '1942', '1943', '1944', '1945', '1946', '1947', '1948', '1949', '1949er', '195', '1950', '1950s', '1951', '1952', '1953', '1954', '1955', '1956', '1957', '1958', '1959', '1960', '1960s', '1961', '1961s', '1962', '1963', '1964', '1965', '1966', '1967', '1968', '1969', '197', '1970', '1970ies', '1970s', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '19796', '197o', '1980', '1980ies', '1980s', '1981', '1982', '1982s', '1983', '1983s', '1984', '1984ish', '1985', '1986', '1987', '1988', '1989', '1990', '1990s', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '19k', '19th', '19thc', '1am', '1and', '1d', '1h', '1h30', '1h40', '1h40m', '1h53', '1hour', '1hr', '1million', '1min', '1mln', '1o', '1s', '1st', '1ton', '1tv', '1½', '1ç', '20', '200', '2000', '20000', '20001', '2000ad', '2000s', '2001', '2002', '2003', '2004', '2004s', '2005', '2006', '2007', '2008', '2009', '200ft', '200th', '201', '2010', '2012', '2013', '2015', '2017', '2019', '2020', '2022', '2023', '2030', '2031', '2033', '2035', '2036', '2038', '204', '2040', '2044', '2046', '2047', '2050', '2053', '2054', '206', '2060', '2070', '2080', '209', '2090', '20c', '20ft', '20k', '20m', '20mins', '20minutes', '20mn', '20p', '20perr', '20s', '20th', '20ties', '20widow', '20x', '20year', '20yrs', '21', '210', '2100', '214', '215', '2151', '216', '21699', '21849889', '21849890', '21849907', '21st', '22', '220', '2200', '221', '2210', '22101', '222', '223', '225', '2257', '225mins', '227', '22d', '22h45', '22nd', '23', '230lbs', '230mph', '231', '232', '233', '236', '237', '23d', '23rd', '24', '240', '2400', '241', '242', '248', '2480', '249', '24m30s', '24th', '24years', '25', '250', '2500', '250000', '25million', '25mins', '25s', '25th', '25yo', '25yrs', '26', '260', '2600', '261k', '262', '2642', '269', '26th', '27', '270', '272', '273', '274', '275', '2772', '278', '27th', '27x41', '28', '280', '285', '28th', '29', '29th', '2am', '2d', '2fast', '2furious', '2h', '2h30', '2hour', '2hours', '2hr', '2hrs', '2in', '2inch', '2k', '2more', '2nd', '2oo4', '2oo5', '2pac', '2point4', '2s', '2x4', '30', '300', '3000', '300ad', '300c', '300lbs', '300mln', '3012', '303', '305', '30am', '30ish', '30k', '30lbs', '30min', '30mins', '30pm', '30s', '30something', '30th', '30ties', '31', '3199', '31st', '32', '3200', '320x180', '32lb', '32nd', '33', '330am', '330mins', '332960073452', '336th', '33m', '34', '345', '3462', '34th', '35', '350', '3500', '3516', '356', '357', '35c', '35mins', '35mm', '35pm', '35th', '35yr', '36', '360', '365', '36th', '37', '370', '372', '378', '38', '38k', '38th', '39', '395', '39th', '3am', '3bs', '3d', '3dvd', '3k', '3lbs', '3m', '3mins', '3p', '3p0', '3pm', '3po', '3rd', '3rds', '3th', '3who', '3x5', '3yrs', '40', '400', '4000', '401k', '405', '409', '40am', '40min', '40mins', '40mph', '40s', '40th', '41', '42', '420', '425', '428', '42nd', '43', '430', '44', '440', '442nd', '44c', '44yrs', '45', '450', '4500', '451', '454', '45am', '45min', '45mins', '45s', '46', '465', '469', '47', '475', '477', '47s', '48', '480m', '480p', '48hrs', '49', '498', '49th', '4am', '4cylinder', '4d', '4eva', '4ever', '4f', '4h', '4hrs', '4k', '4kids', '4m', '4o', '4pm', '4th', '4w', '4ward', '4x', '4x4', '50', '500', '5000', '500000', '500ad', '500db', '500lbs', '502', '50c', '50ft', '50ies', '50ish', '50k', '50min', '50mins', '50s', '50th', '50usd', '51', '51b', '51st', '52', '5200', '5250', '529', '52s', '53', '53m', '54', '5400', '540i', '54th', '55', '5539', '555', '55th', '56', '57', '571', '576', '578', '57d', '58', '58th', '59', '598947', '59th', '5hrs', '5ive', '5kph', '5million', '5min', '5mins', '5s', '5seconds', '5th', '5x', '5x5', '5years', '5yo', '5yrs', '60', '600', '6000', '607', '608', '60ies', '60ish', '60mph', '60s', '60th', '60ties', '61', '618', '62', '6200', '62229249', '63', '637', '63rd', '64', '65', '65m', '66', '660', '666', '66er', '67', '6723', '67th', '68', '68th', '69', '69th', '6am', '6b', '6ft', '6hours', '6k', '6million', '6pm', '6th', '6wks', '6yo', '70', '700', '701', '707', '70ies', '70m', '70mm', '70s', '70th', '71', '713', '72', '72nd', '73', '7300', '735', '737', '74', '740', '740il', '747', '747s', '74th', '75', '750', '75054', '75c', '75m', '76', '7600', '77', '78', '788', '78rpm', '79', '79th', '7days', '7even', '7eventy', '7ft', '7ish', '7mm', '7th', '7½th', '80', '800', '8000', '80ies', '80ish', '80min', '80s', '80yr', '81', '817', '819', '82', '820', '8217', '8230', '83', '84', '849', '84f', '84s', '85', '850', '850pm', '86', '86s', '87', '8700', '8763', '878', '87minutes', '88', '88min', '89', '89or', '89s', '8bit', '8ftdf', '8k', '8mm', '8o', '8p', '8pm', '8star', '8th', '8u', '8½', '90', '900', '9000', '90210', '905', '90c', '90ish', '90min', '90mins', '90s', '91', '911', '914', '917', '92', '921', '92fs', '92nd', '93', '937', '94', '9484', '94s', '94th', '95', '950', '95th', '96', '97', '970', '974th', '978', '98', '987', '98minutes', '99', '998', '999', '9999', '99cents', '99p', '99½', '9_', '9am', '9as', '9do', '9ers', '9is', '9lbs', '9mm']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names()[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the feature matrix done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split\n",
    "\n",
    "* Train data - all training based on it (this includes the vectorizer!)\n",
    "* Development data - set the parameters\n",
    "* Test data - used for nothing during training, produce final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, dev_texts, train_labels, dev_labels = train_test_split(texts,labels,test_size=0.2)\n",
    "# bad habits\n",
    "vectorizer=CountVectorizer(max_features=100000,binary=True,ngram_range=(1,1))\n",
    "feature_matrix_train=vectorizer.fit_transform(train_texts)\n",
    "feature_matrix_dev=vectorizer.transform(dev_texts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 68545)\n",
      "(5000, 68545)\n"
     ]
    }
   ],
   "source": [
    "print(feature_matrix_train.shape)\n",
    "print(feature_matrix_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier train\n",
    "\n",
    "* Let us try the venerable, if a bit outdated SVM\n",
    "* Linear SVM for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.007, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=1)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.svm\n",
    "# Always try your way forward with the C value\n",
    "classifier=sklearn.svm.LinearSVC(C=0.007,verbose=1)\n",
    "classifier.fit(feature_matrix_train, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "* For a quick test we can use the .score() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV 0.886\n",
      "TRAIN 0.965\n"
     ]
    }
   ],
   "source": [
    "print(\"DEV\",classifier.score(feature_matrix_dev, dev_labels))\n",
    "print(\"TRAIN\",classifier.score(feature_matrix_train, train_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try varying the C value and observe the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg' 'neg' 'neg' ... 'pos' 'pos' 'pos']\n",
      "[[2189  289]\n",
      " [ 281 2241]]\n",
      "0.886\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "predictions_dev=classifier.predict(feature_matrix_dev)\n",
    "print(predictions_dev)\n",
    "print(sklearn.metrics.confusion_matrix(dev_labels,predictions_dev))\n",
    "print(sklearn.metrics.accuracy_score(dev_labels,predictions_dev))\n",
    "# Negatives can have a higher cost if they're wrongly predicted compared to \n",
    "# positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained classifier save and load\n",
    "\n",
    "* We fitted two things: the vectorizer and the classifier\n",
    "* If we want to reuse them later, we need to save them\n",
    "* You can use `pickle` in most cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"saved_model.pickle\",\"wb\") as f:\n",
    "    pickle.dump((classifier,vectorizer),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* let's try to load and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV - loaded (should match the score above) 0.8654\n"
     ]
    }
   ],
   "source": [
    "with open(\"saved_model.pickle\",\"rb\") as f:\n",
    "    classifier_loaded,vectorizer_loaded=pickle.load(f)\n",
    "\n",
    "feature_matrix_dev_loaded=vectorizer_loaded.transform(dev_texts)\n",
    "print(\"DEV - loaded (should match the score above)\",classifier_loaded.score(feature_matrix_dev, dev_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
